{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:09:48.089280Z",
     "iopub.status.busy": "2023-06-14T19:09:48.088853Z",
     "iopub.status.idle": "2023-06-14T19:10:05.036898Z",
     "shell.execute_reply": "2023-06-14T19:10:05.035724Z",
     "shell.execute_reply.started": "2023-06-14T19:09:48.089237Z"
    },
    "executionInfo": {
     "elapsed": 72270,
     "status": "ok",
     "timestamp": 1605574122422,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "gxKQc7hvK0PO",
    "outputId": "262330b9-5b65-4906-d82a-44593c513050"
   },
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:05.039833Z",
     "iopub.status.busy": "2023-06-14T19:10:05.039444Z",
     "iopub.status.idle": "2023-06-14T19:10:07.414510Z",
     "shell.execute_reply": "2023-06-14T19:10:07.413307Z",
     "shell.execute_reply.started": "2023-06-14T19:10:05.039791Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/cable/* /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:07.416614Z",
     "iopub.status.busy": "2023-06-14T19:10:07.416210Z",
     "iopub.status.idle": "2023-06-14T19:10:07.745094Z",
     "shell.execute_reply": "2023-06-14T19:10:07.744198Z",
     "shell.execute_reply.started": "2023-06-14T19:10:07.416572Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import uuid, os\n",
    "from datetime import date\n",
    "\n",
    "current_date = date.today()\n",
    "date_name = current_date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "ACCESS_KEY = os.environ.get('AWS_ACCESS_KEY_ID')\n",
    "SECRET_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "BUCKET_NAME = os.environ.get('BUCKET_NAME')\n",
    "DATASET_NAME = \"cycle-gan/cable\"\n",
    "CABLE_NAME = \"weights.zip\"\n",
    "CABLE_PATH = f\"{DATASET_NAME}/cable_weights-{date_name}.zip\"\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=ACCESS_KEY, \n",
    "    aws_secret_access_key=SECRET_KEY\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvVG92WsKeYm"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, we will attempt to greate a Cycle Generative Adversarial Network (cycleGAN) in order to manipulate defective industrial parts. We will attempt to augment images data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYM61P4RKeYo"
   },
   "source": [
    "Our first step is to load the neccessary libraries we will be using for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:07.748842Z",
     "iopub.status.busy": "2023-06-14T19:10:07.748521Z",
     "iopub.status.idle": "2023-06-14T19:10:11.344267Z",
     "shell.execute_reply": "2023-06-14T19:10:11.343055Z",
     "shell.execute_reply.started": "2023-06-14T19:10:07.748817Z"
    },
    "id": "2z7zHHmcKeYq"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os , itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TLPoFq6S14N"
   },
   "source": [
    "Since we are using two platforms, we will be assigning a directory to the location of the data for each platform. This will make it easier to access the data, depending the platform we will be using.\n",
    "\n",
    "The two platforms will be:\n",
    "1. A local installation of  Jupyter Notebook - work on local codes. GPU is limited\n",
    "2. A Google Colaboratory Notebook - Use of available resources and GPU to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:13:58.917523Z",
     "iopub.status.busy": "2023-06-14T19:13:58.917062Z",
     "iopub.status.idle": "2023-06-14T19:13:58.923062Z",
     "shell.execute_reply": "2023-06-14T19:13:58.922003Z",
     "shell.execute_reply.started": "2023-06-14T19:13:58.917485Z"
    },
    "id": "AijKd1STKkUM"
   },
   "outputs": [],
   "source": [
    "    data_dir = 'cable'\n",
    "    train_dir = os.path.join(data_dir,'train')\n",
    "    test_dir = os.path.join(data_dir,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqld3FCnL4Tc"
   },
   "source": [
    "## Parameters\n",
    "\n",
    "In this block, we will create global parameters which we can use at different areas of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.362657Z",
     "iopub.status.busy": "2023-06-14T19:10:11.359999Z",
     "iopub.status.idle": "2023-06-14T19:10:11.376650Z",
     "shell.execute_reply": "2023-06-14T19:10:11.374078Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.362614Z"
    },
    "id": "otQKmH-brXxc"
   },
   "outputs": [],
   "source": [
    "# Creating the Parameters\n",
    "\n",
    "parameters = {\n",
    "    'batch_size': 1,\n",
    "    'input_size': 256,\n",
    "    'resize_scale': 286,\n",
    "    'crop_size': 256,\n",
    "    'fliplr':True,\n",
    "    \n",
    "    # Model parameters\n",
    "    'num_epochs': 100,\n",
    "    'decay_epoch': 100,\n",
    "    'ngf': 32,   # Number of generator filters\n",
    "    'ndf': 64,   # Number of discriminator filters\n",
    "    'num_resnet': 6, # Number of resnet blocks\n",
    "    'lrG': 0.0002,    # Learning rate for generator\n",
    "    'lrD': 0.0002,    # Learning rate for discriminator\n",
    "    'beta1': 0.5 ,    # Beta1 for Adam optimizer\n",
    "    'beta2': 0.999 ,  # Beta2 for Adam optimizer\n",
    "    'lambdaA': 10 ,   # LambdaA for cycle loss\n",
    "    'lambdaB': 10  ,  # LambdaB for cycle loss\n",
    "}\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4gcJSynYgR9"
   },
   "source": [
    "## Creating functions\n",
    "\n",
    "In the next set, we will create several functions that will be used throughout. The first function will convert our data into Numpy, which we can call at a later point.\n",
    "\n",
    "We will also set a variable so we can use a GPU for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.380069Z",
     "iopub.status.busy": "2023-06-14T19:10:11.378444Z",
     "iopub.status.idle": "2023-06-14T19:10:11.416215Z",
     "shell.execute_reply": "2023-06-14T19:10:11.415354Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.380032Z"
    },
    "id": "iM56xhMDyBIu"
   },
   "outputs": [],
   "source": [
    "# Convert to Numpy \n",
    "def to_numpy(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T20:02:05.958258Z",
     "iopub.status.busy": "2023-06-14T20:02:05.957812Z",
     "iopub.status.idle": "2023-06-14T20:02:06.901309Z",
     "shell.execute_reply": "2023-06-14T20:02:06.899987Z",
     "shell.execute_reply.started": "2023-06-14T20:02:05.958228Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir training_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in6MLzoYZBRb"
   },
   "source": [
    "### Transformer\n",
    "\n",
    "Since we will be using a __transformer__ at a later stage, we will set a transformer variable and assign it procedures which transforms an image size accordingly. This transformer will also convert the image into a Tensor and Normalize it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.418020Z",
     "iopub.status.busy": "2023-06-14T19:10:11.417615Z",
     "iopub.status.idle": "2023-06-14T19:10:11.425983Z",
     "shell.execute_reply": "2023-06-14T19:10:11.425080Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.417988Z"
    },
    "id": "uDjZWE75jLwO"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=params['input_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kt-ZRdnWZvZL"
   },
   "source": [
    "Defining a function which will __plot__ the result from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_result(real_image, gen_image, recon_image, epoch, save=False,  show=False, fig_size=(15, 15)):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=fig_size)\n",
    "    imgs = [to_np(real_image[0]), to_np(gen_image[0]), to_np(recon_image[0]),\n",
    "            to_np(real_image[1]), to_np(gen_image[1]), to_np(recon_image[1])]\n",
    "    for ax, img in zip(axes.flatten(), imgs):\n",
    "        ax.axis('off')\n",
    "        #ax.set_adjustable('box-forced')\n",
    "        # Scale to 0-255\n",
    "        img = img.squeeze()\n",
    "        img = (((img - img.min()) * 255) / (img.max() - img.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "        ax.imshow(img, cmap=None, aspect='equal')\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    title = 'Epoch {0}'.format(epoch + 1)\n",
    "    fig.text(0.5, 0.04, title, ha='center')\n",
    "\n",
    "    # save figure\n",
    "    if save:\n",
    "        save_fn = f'Result_epoch_{epoch+1}.png'\n",
    "        plt.savefig(save_fn)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_qDIcq5Z3KJ"
   },
   "source": [
    "Next, we will define an __ImagePool__ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.445038Z",
     "iopub.status.busy": "2023-06-14T19:10:11.444783Z",
     "iopub.status.idle": "2023-06-14T19:10:11.455801Z",
     "shell.execute_reply": "2023-06-14T19:10:11.454900Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.445016Z"
    },
    "id": "VM0YNJl5y58U"
   },
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images.data:\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size-1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "        return_images = Variable(torch.cat(return_images, 0))\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWviChqSaQoP"
   },
   "source": [
    "### Creating the Dataloader\n",
    "\n",
    "In this section, we begin by creating the __dataloader__ which we will use to retrieve the data in parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.457456Z",
     "iopub.status.busy": "2023-06-14T19:10:11.457112Z",
     "iopub.status.idle": "2023-06-14T19:10:11.470308Z",
     "shell.execute_reply": "2023-06-14T19:10:11.469428Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.457415Z"
    },
    "id": "IYnaXef7zAFj"
   },
   "outputs": [],
   "source": [
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, subfolder='train', transform=None, resize_scale=None, crop_size=None, fliplr=False):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.input_path = os.path.join(image_dir, subfolder)\n",
    "        self.image_filenames = [x for x in sorted(os.listdir(self.input_path))]\n",
    "        self.transform = transform\n",
    "\n",
    "        self.resize_scale = resize_scale\n",
    "        self.crop_size = crop_size\n",
    "        self.fliplr = fliplr\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        img_fn = os.path.join(self.input_path, self.image_filenames[index])\n",
    "        img = Image.open(img_fn).convert('RGB')\n",
    "\n",
    "        # preprocessing\n",
    "        if self.resize_scale:\n",
    "            img = img.resize((self.resize_scale, self.resize_scale), Image.BILINEAR)\n",
    "\n",
    "        if self.crop_size:\n",
    "            x = random.randint(0, self.resize_scale - self.crop_size + 1)\n",
    "            y = random.randint(0, self.resize_scale - self.crop_size + 1)\n",
    "            img = img.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
    "        if self.fliplr:\n",
    "            if random.random() < 0.5:\n",
    "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Es9nw6Xaecu"
   },
   "source": [
    "### Creating the Convolutional Block\n",
    "\n",
    "The __convolution__ block will consist of two things.\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "2. Forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.472145Z",
     "iopub.status.busy": "2023-06-14T19:10:11.471502Z",
     "iopub.status.idle": "2023-06-14T19:10:11.484241Z",
     "shell.execute_reply": "2023-06-14T19:10:11.483296Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.472115Z"
    },
    "id": "KLJ6rMcTpZRv"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self,input_size,output_size,kernel_size=3,stride=2,padding=1,activation='relu',batch_norm=True):\n",
    "        super(ConvBlock,self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(input_size,output_size,kernel_size,stride,padding)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.InstanceNorm2d(output_size)\n",
    "        self.activation = activation\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.lrelu = torch.nn.LeakyReLU(0.2,True)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    def forward(self,x):\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.conv(x))\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(out)\n",
    "        elif self.activation == 'lrelu':\n",
    "            return self.lrelu(out)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.tanh(out)\n",
    "        elif self.activation == 'no_act':\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XBNtHuHbZp9"
   },
   "source": [
    "### De-Convolution Function\n",
    "\n",
    "In this class we will create, we will need to bring back the convolution from the results previously processed. There will be two functions inside this class.\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "2. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.485886Z",
     "iopub.status.busy": "2023-06-14T19:10:11.485466Z",
     "iopub.status.idle": "2023-06-14T19:10:11.497789Z",
     "shell.execute_reply": "2023-06-14T19:10:11.496654Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.485855Z"
    },
    "id": "YF4rQT_VpiDU"
   },
   "outputs": [],
   "source": [
    "class DeconvBlock(torch.nn.Module):\n",
    "    def __init__(self,input_size,output_size,kernel_size=3,stride=2,padding=1,output_padding=1,activation='relu',batch_norm=True):\n",
    "        super(DeconvBlock,self).__init__()\n",
    "        self.deconv = torch.nn.ConvTranspose2d(input_size,output_size,kernel_size,stride,padding,output_padding)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.InstanceNorm2d(output_size)\n",
    "        self.activation = activation\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "    def forward(self,x):\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.deconv(x))\n",
    "        else:\n",
    "            out = self.deconv(x)\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(out)\n",
    "        elif self.activation == 'lrelu':\n",
    "            return self.lrelu(out)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.tanh(out)\n",
    "        elif self.activation == 'no_act':\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMTWZK_QcFI_"
   },
   "source": [
    "### Residual Learning Block\n",
    "\n",
    "In this secton, we will create a __residual learning__ block or __Resnet__. This class will also contain two functions.\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "2. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.499623Z",
     "iopub.status.busy": "2023-06-14T19:10:11.499175Z",
     "iopub.status.idle": "2023-06-14T19:10:11.511126Z",
     "shell.execute_reply": "2023-06-14T19:10:11.510233Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.499593Z"
    },
    "id": "mr_C4hnKpkyw"
   },
   "outputs": [],
   "source": [
    "class ResnetBlock(torch.nn.Module):\n",
    "    def __init__(self,num_filter,kernel_size=3,stride=1,padding=0):\n",
    "        super(ResnetBlock,self).__init__()\n",
    "        conv1 = torch.nn.Conv2d(num_filter,num_filter,kernel_size,stride,padding)\n",
    "        conv2 = torch.nn.Conv2d(num_filter,num_filter,kernel_size,stride,padding)\n",
    "        bn = torch.nn.InstanceNorm2d(num_filter)\n",
    "        relu = torch.nn.ReLU(True)\n",
    "        pad = torch.nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.resnet_block = torch.nn.Sequential(pad, conv1, bn, relu, pad, conv2, bn)\n",
    "    def forward(self,x):\n",
    "        out = self.resnet_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL_Aa8m0g5OB"
   },
   "source": [
    "## Creating the Generator\n",
    "\n",
    "We will now create the __Generator__. The Generator will consist of the following:\n",
    "1. Encoder\n",
    "2. Transformer\n",
    "3. Decoder\n",
    "\n",
    "In addition, we will also be adding __weights__.\n",
    "\n",
    "After creating the class, we will create the actuall Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.513178Z",
     "iopub.status.busy": "2023-06-14T19:10:11.512632Z",
     "iopub.status.idle": "2023-06-14T19:10:11.527271Z",
     "shell.execute_reply": "2023-06-14T19:10:11.526433Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.513094Z"
    },
    "id": "sO6cRW__hB-E"
   },
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self,input_dim,num_filter,output_dim,num_resnet):\n",
    "        super(Generator,self).__init__()\n",
    "\n",
    "        # Reflection padding\n",
    "        self.pad = torch.nn.ReflectionPad2d(3)\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(input_dim,num_filter,kernel_size=7,stride=1,padding=0)\n",
    "        self.conv2 = ConvBlock(num_filter,num_filter*2)\n",
    "        self.conv3 = ConvBlock(num_filter*2,num_filter*4)\n",
    "        # Resnet blocks\n",
    "        self.resnet_blocks = []\n",
    "        for i in range(num_resnet):\n",
    "            self.resnet_blocks.append(ResnetBlock(num_filter*4))\n",
    "        self.resnet_blocks = torch.nn.Sequential(*self.resnet_blocks)\n",
    "        #Decoder\n",
    "        self.deconv1 = DeconvBlock(num_filter*4,num_filter*2)\n",
    "        self.deconv2 = DeconvBlock(num_filter*2,num_filter)\n",
    "        self.deconv3 = ConvBlock(num_filter,output_dim,kernel_size=7,stride=1,padding=0,activation='tanh',batch_norm=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Encoder\n",
    "        enc1 = self.conv1(self.pad(x))\n",
    "        enc2 = self.conv2(enc1)\n",
    "        enc3 = self.conv3(enc2)\n",
    "        # Resnet blocks\n",
    "        res = self.resnet_blocks(enc3)\n",
    "        # Decoder\n",
    "        dec1 = self.deconv1(res)\n",
    "        dec2 = self.deconv2(dec1)\n",
    "        out = self.deconv3(self.pad(dec2))\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self,mean=0.0,std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m,ConvBlock):\n",
    "                torch.nn.init.normal_(m.conv.weight,mean,std)\n",
    "            if isinstance(m,DeconvBlock):\n",
    "                torch.nn.init.normal_(m.deconv.weight,mean,std)\n",
    "            if isinstance(m,ResnetBlock):\n",
    "                torch.nn.init.normal_(m.conv.weight,mean,std)\n",
    "                torch.nn.init.constant_(m.conv.bias,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_l3oAV-dZwA"
   },
   "source": [
    "### Creating the Generator and Testing the Neural Network\n",
    "\n",
    "Since we will be using a Cycle GAN, which requires 2 Neural Networks competing with each other, we will create 2 Generators as its requirement. We will also test it by calling it and view the results created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIJRhsDF9FBt"
   },
   "source": [
    "#### Generator A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:11.529082Z",
     "iopub.status.busy": "2023-06-14T19:10:11.528499Z",
     "iopub.status.idle": "2023-06-14T19:10:14.241150Z",
     "shell.execute_reply": "2023-06-14T19:10:14.240272Z",
     "shell.execute_reply.started": "2023-06-14T19:10:11.529051Z"
    },
    "executionInfo": {
     "elapsed": 11322,
     "status": "ok",
     "timestamp": 1605203530649,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "GWqNSHCIdbFI",
    "outputId": "21b7b67c-6e64-4d5d-d901-3bc0aea22943"
   },
   "outputs": [],
   "source": [
    "G_A = Generator(3, parameters['ngf'], 3, parameters['num_resnet']).cuda() # input_dim, num_filter, output_dim, num_resnet\n",
    "G_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "G_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MADq2MXY9JtS"
   },
   "source": [
    "#### Generator B\n",
    "\n",
    "Repeating the previous step, but for Generator B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:14.243410Z",
     "iopub.status.busy": "2023-06-14T19:10:14.242767Z",
     "iopub.status.idle": "2023-06-14T19:10:14.284924Z",
     "shell.execute_reply": "2023-06-14T19:10:14.283928Z",
     "shell.execute_reply.started": "2023-06-14T19:10:14.243359Z"
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1605203532238,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "h4mRA2cZeJ4T",
    "outputId": "c45a76b1-c60c-445f-be69-cb2d27cf13c9"
   },
   "outputs": [],
   "source": [
    "G_B = Generator(3, parameters['ngf'], 3, parameters['num_resnet']).cuda()\n",
    "G_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "G_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWhXRqM4lS6v"
   },
   "source": [
    "## Creating the Discriminator\n",
    "\n",
    "The Discriminator will consist of the following\n",
    "1. Initialization\n",
    "2. Forward Pass\n",
    "3. Weights\n",
    "\n",
    "After creating the class, we will create the actuall Discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:14.286674Z",
     "iopub.status.busy": "2023-06-14T19:10:14.286325Z",
     "iopub.status.idle": "2023-06-14T19:10:14.346776Z",
     "shell.execute_reply": "2023-06-14T19:10:14.345796Z",
     "shell.execute_reply.started": "2023-06-14T19:10:14.286643Z"
    },
    "id": "eJsYjXHYg8AC"
   },
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self,input_dim,num_filter,output_dim):\n",
    "        super(Discriminator,self).__init__()\n",
    "        conv1 = ConvBlock(input_dim,num_filter,kernel_size=4,stride=2,padding=1,activation='lrelu',batch_norm=False)\n",
    "        conv2 = ConvBlock(num_filter,num_filter*2,kernel_size=4,stride=2,padding=1,activation='lrelu')\n",
    "        conv3 = ConvBlock(num_filter*2,num_filter*4,kernel_size=4,stride=2,padding=1,activation='lrelu')\n",
    "        conv4 = ConvBlock(num_filter*4,num_filter*8,kernel_size=4,stride=1,padding=1,activation='lrelu')\n",
    "        conv5 = ConvBlock(num_filter*8,output_dim,kernel_size=4,stride=1,padding=1,activation='no_act',batch_norm=False)\n",
    "        self.conv_blocks = torch.nn.Sequential(conv1, conv2, conv3, conv4, conv5)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.conv_blocks(x)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self,mean=0.0,std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m,ConvBlock):\n",
    "                torch.nn.init.normal_(m.conv.weight.data,mean,std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36IywNAtd-xY"
   },
   "source": [
    "### Creating the Discriminator and Testing the Neural Network\n",
    "\n",
    "Similar with the Generator, which requires 2 Neural Networks. A cycleGAN also uses 2 Discriminators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YYXIKrz81I8"
   },
   "source": [
    "#### Discriminator A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:14.348524Z",
     "iopub.status.busy": "2023-06-14T19:10:14.348035Z",
     "iopub.status.idle": "2023-06-14T19:10:14.386706Z",
     "shell.execute_reply": "2023-06-14T19:10:14.385733Z",
     "shell.execute_reply.started": "2023-06-14T19:10:14.348493Z"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1605219271350,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "1vcEqAoNeCKR",
    "outputId": "1e06bff6-048a-4eae-a6ba-ef81c0309f36"
   },
   "outputs": [],
   "source": [
    "# Creating the first Discriminator\n",
    "D_A = Discriminator(3, parameters['ndf'], 1).cuda() # input_dim, num_filter, output_dim\n",
    "D_A.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJhcGbtZ88Nh"
   },
   "source": [
    "#### Discriminator B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:10:14.388542Z",
     "iopub.status.busy": "2023-06-14T19:10:14.388141Z",
     "iopub.status.idle": "2023-06-14T19:10:14.421182Z",
     "shell.execute_reply": "2023-06-14T19:10:14.420290Z",
     "shell.execute_reply.started": "2023-06-14T19:10:14.388511Z"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1605203538689,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "RHLJKzoMemlT",
    "outputId": "6f572964-b3a6-4330-e888-02129845ce4b"
   },
   "outputs": [],
   "source": [
    "# Creating the second Discriminator\n",
    "D_B = Discriminator(3, parameters['ndf'], 1).cuda()\n",
    "D_B.normal_weight_init(mean=0.0, std=0.02)\n",
    "D_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM0cpsAcHpON"
   },
   "source": [
    "## Training Block / DataLoaders\n",
    "\n",
    "In this block, we will create the actual training block we will be using to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:18:48.679232Z",
     "iopub.status.busy": "2023-06-14T19:18:48.678867Z",
     "iopub.status.idle": "2023-06-14T19:18:48.689527Z",
     "shell.execute_reply": "2023-06-14T19:18:48.688656Z",
     "shell.execute_reply.started": "2023-06-14T19:18:48.679204Z"
    },
    "id": "hl_9r6z8q84A"
   },
   "outputs": [],
   "source": [
    "train_data_A = DatasetFromFolder(train_dir, subfolder='good', transform=transform,resize_scale=parameters['resize_scale'], crop_size=parameters['crop_size'], fliplr=parameters['fliplr'])\n",
    "train_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A,batch_size=parameters['batch_size'], shuffle=True)\n",
    "train_data_B = DatasetFromFolder(data_dir, subfolder='bad', transform=transform, resize_scale=parameters['resize_scale'], crop_size=parameters['crop_size'], fliplr=parameters['fliplr'])\n",
    "train_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B, batch_size=parameters['batch_size'], shuffle=True)\n",
    "\n",
    "#Load test data\n",
    "test_data_A = DatasetFromFolder(test_dir, subfolder='good', transform=transform)\n",
    "test_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A, batch_size=parameters['batch_size'], shuffle=False)\n",
    "test_data_B = DatasetFromFolder(test_dir, subfolder='bad', transform=transform)\n",
    "test_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B, batch_size=parameters['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gZoWnJchfFq"
   },
   "source": [
    "## Creating the Optimizer\n",
    "\n",
    "In this block, we need to create the learning optimizers. We will have one optimizer for the Generator and two optimizers for the Discriminator. The Discriminator has two optimizers as this function checks the data generated by the Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:19:07.371269Z",
     "iopub.status.busy": "2023-06-14T19:19:07.370919Z",
     "iopub.status.idle": "2023-06-14T19:19:07.379438Z",
     "shell.execute_reply": "2023-06-14T19:19:07.378362Z",
     "shell.execute_reply.started": "2023-06-14T19:19:07.371242Z"
    },
    "id": "aULDtm48JVS4"
   },
   "outputs": [],
   "source": [
    "G_optimizer = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()), lr=parameters['lrG'], betas=(parameters['beta1'], parameters['beta2']))\n",
    "D_A_optimizer = torch.optim.Adam(D_A.parameters(), lr=parameters['lrD'], betas=(parameters['beta1'], parameters['beta2']))\n",
    "D_B_optimizer = torch.optim.Adam(D_B.parameters(), lr=parameters['lrD'], betas=(parameters['beta1'], parameters['beta2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_S7jO44wNvk"
   },
   "source": [
    "## Defining Losses\n",
    "\n",
    "In order to make sure that there is learning, we will create cycle losses, which we will pass on at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:19:09.519866Z",
     "iopub.status.busy": "2023-06-14T19:19:09.519465Z",
     "iopub.status.idle": "2023-06-14T19:19:09.525779Z",
     "shell.execute_reply": "2023-06-14T19:19:09.524607Z",
     "shell.execute_reply.started": "2023-06-14T19:19:09.519835Z"
    },
    "id": "WGS3-iixJao8"
   },
   "outputs": [],
   "source": [
    "MSE_Loss = torch.nn.MSELoss().cuda()\n",
    "L1_Loss = torch.nn.L1Loss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:19:09.745916Z",
     "iopub.status.busy": "2023-06-14T19:19:09.745610Z",
     "iopub.status.idle": "2023-06-14T19:19:09.752723Z",
     "shell.execute_reply": "2023-06-14T19:19:09.751449Z",
     "shell.execute_reply.started": "2023-06-14T19:19:09.745889Z"
    },
    "id": "lLmXrBTMRB21"
   },
   "outputs": [],
   "source": [
    "D_A_avg_losses = []\n",
    "D_B_avg_losses = []\n",
    "G_A_avg_losses = []\n",
    "G_B_avg_losses = []\n",
    "cycle_A_avg_losses = []\n",
    "cycle_B_avg_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZ__inadh3-i"
   },
   "source": [
    "## Training the model and retreiving Images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR_L6bUV_NLa"
   },
   "source": [
    "Using the image pool created earliler, we will create a pool of variables which we will be using for training at the next cell block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-15T09:09:58.090451Z",
     "iopub.status.busy": "2023-06-15T09:09:58.090090Z",
     "iopub.status.idle": "2023-06-15T09:09:58.520311Z",
     "shell.execute_reply": "2023-06-15T09:09:58.519031Z",
     "shell.execute_reply.started": "2023-06-15T09:09:58.090422Z"
    },
    "id": "TSD7HSK6_ZUM"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImagePool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 2\u001b[0m fake_A_pool \u001b[38;5;241m=\u001b[39m \u001b[43mImagePool\u001b[49m(num_pool)\n\u001b[1;32m      3\u001b[0m fake_B_pool \u001b[38;5;241m=\u001b[39m ImagePool(num_pool)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get specific test images\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImagePool' is not defined"
     ]
    }
   ],
   "source": [
    "num_pool = 50\n",
    "fake_A_pool = ImagePool(num_pool)\n",
    "fake_B_pool = ImagePool(num_pool)\n",
    "\n",
    "# Get specific test images\n",
    "test_real_A_data = train_data_A.__getitem__(8).unsqueeze(0)\n",
    "test_real_B_data = train_data_B.__getitem__(6).unsqueeze(0)\n",
    "\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9sRRpj5_ho3"
   },
   "source": [
    "We will now run the Training sequence in the block. It is also in these block that learning begins and the time to process the results can take several hours. \n",
    "\n",
    "There will be two results displayed below.\n",
    "1. The epoch number where the learning takes place. This will also include the loss.\n",
    "2. The image results on how well it did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-14T19:19:11.497269Z",
     "iopub.status.busy": "2023-06-14T19:19:11.496927Z",
     "iopub.status.idle": "2023-06-14T19:19:32.735817Z",
     "shell.execute_reply": "2023-06-14T19:19:32.734305Z",
     "shell.execute_reply.started": "2023-06-14T19:19:11.497239Z"
    },
    "id": "9Oze_Sg2IB3T"
   },
   "outputs": [],
   "source": [
    "for epoch in range(parameters['num_epochs']):\n",
    "    D_A_losses = []\n",
    "    D_B_losses = []\n",
    "    G_A_losses = []\n",
    "    G_B_losses = []\n",
    "    cycle_A_losses = []\n",
    "    cycle_B_losses = []\n",
    "\n",
    "    # Learing rate decay\n",
    "    if(epoch + 1) > parameters['decay_epoch']:\n",
    "        D_A_optimizer.param_groups[0]['lr'] -= parameters['lrD'] / (parameters['num_epochs'] - parameters['decay_epoch'])\n",
    "        D_B_optimizer.param_groups[0]['lr'] -= parameters['lrD'] / (parameters['num_epochs'] - parameters['decay_epoch'])\n",
    "        G_optimizer.param_groups[0]['lr'] -= parameters['lrG'] / (parameters['num_epochs'] - parameters['decay_epoch'])\n",
    "\n",
    "\n",
    "    # training\n",
    "    for i, (real_A, real_B) in enumerate(zip(train_data_loader_A, train_data_loader_B)):\n",
    "\n",
    "        # input image data\n",
    "        real_A = real_A.to(device)\n",
    "        real_B = real_B.to(device)\n",
    "\n",
    "        # -------------------------- train generator G --------------------------\n",
    "        # A --> B\n",
    "        fake_B = G_A(real_A)\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        G_A_loss = MSE_Loss(D_B_fake_decision, Variable(torch.ones(D_B_fake_decision.size()).cuda()))\n",
    "\n",
    "        # forward cycle loss\n",
    "        recon_A = G_B(fake_B)\n",
    "        cycle_A_loss = L1_Loss(recon_A, real_A) * parameters['lambdaA']\n",
    "\n",
    "        # B --> A\n",
    "        fake_A = G_B(real_B)\n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        G_B_loss = MSE_Loss(D_A_fake_decision, Variable(torch.ones(D_A_fake_decision.size()).cuda()))\n",
    "\n",
    "        # backward cycle loss\n",
    "        recon_B = G_A(fake_A)\n",
    "        cycle_B_loss = L1_Loss(recon_B, real_B) * parameters['lambdaB']\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_A_loss + G_B_loss + cycle_A_loss + cycle_B_loss\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "\n",
    "        # -------------------------- train discriminator D_A --------------------------\n",
    "        D_A_real_decision = D_A(real_A)\n",
    "        D_A_real_loss = MSE_Loss(D_A_real_decision, Variable(torch.ones(D_A_real_decision.size()).cuda()))\n",
    "\n",
    "        fake_A = fake_A_pool.query(fake_A)\n",
    "\n",
    "        D_A_fake_decision = D_A(fake_A)\n",
    "        D_A_fake_loss = MSE_Loss(D_A_fake_decision, Variable(torch.zeros(D_A_fake_decision.size()).cuda()))\n",
    "\n",
    "        # Back propagation\n",
    "        D_A_loss = (D_A_real_loss + D_A_fake_loss) * 0.5\n",
    "        D_A_optimizer.zero_grad()\n",
    "        D_A_loss.backward()\n",
    "        D_A_optimizer.step()\n",
    "\n",
    "        # -------------------------- train discriminator D_B --------------------------\n",
    "        D_B_real_decision = D_B(real_B)\n",
    "        D_B_real_loss = MSE_Loss(D_B_real_decision, Variable(torch.ones(D_B_fake_decision.size()).cuda()))\n",
    "\n",
    "        fake_B = fake_B_pool.query(fake_B)\n",
    "\n",
    "        D_B_fake_decision = D_B(fake_B)\n",
    "        D_B_fake_loss = MSE_Loss(D_B_fake_decision, Variable(torch.zeros(D_B_fake_decision.size()).cuda()))\n",
    "\n",
    "        # Back propagation\n",
    "        D_B_loss = (D_B_real_loss + D_B_fake_loss) * 0.5\n",
    "        D_B_optimizer.zero_grad()\n",
    "        D_B_loss.backward()\n",
    "        D_B_optimizer.step()\n",
    "\n",
    "        # ------------------------ Print -----------------------------\n",
    "        # loss values\n",
    "        D_A_losses.append(D_A_loss.item())\n",
    "        D_B_losses.append(D_B_loss.item())\n",
    "        G_A_losses.append(G_A_loss.item())\n",
    "        G_B_losses.append(G_B_loss.item())\n",
    "        cycle_A_losses.append(cycle_A_loss.item())\n",
    "        cycle_B_losses.append(cycle_B_loss.item())\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], D_A_loss: %.4f, D_B_loss: %.4f, G_A_loss: %.4f, G_B_loss: %.4f'\n",
    "                  % (epoch+1, parameters['num_epochs'], i+1, len(train_data_loader_A), D_A_loss.item(), D_B_loss.item(), G_A_loss.item(), G_B_loss.item()))\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    D_A_avg_loss = torch.mean(torch.FloatTensor(D_A_losses))\n",
    "    D_B_avg_loss = torch.mean(torch.FloatTensor(D_B_losses))\n",
    "    G_A_avg_loss = torch.mean(torch.FloatTensor(G_A_losses))\n",
    "    G_B_avg_loss = torch.mean(torch.FloatTensor(G_B_losses))\n",
    "    cycle_A_avg_loss = torch.mean(torch.FloatTensor(cycle_A_losses))\n",
    "    cycle_B_avg_loss = torch.mean(torch.FloatTensor(cycle_B_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_A_avg_losses.append(D_A_avg_loss.item())\n",
    "    D_B_avg_losses.append(D_B_avg_loss.item())\n",
    "    G_A_avg_losses.append(G_A_avg_loss.item())\n",
    "    G_B_avg_losses.append(G_B_avg_loss.item())\n",
    "    cycle_A_avg_losses.append(cycle_A_avg_loss.item())\n",
    "    cycle_B_avg_losses.append(cycle_B_avg_loss.item())\n",
    "\n",
    "    # Show result for test image\n",
    "    test_real_A = test_real_A_data.cuda()\n",
    "    test_fake_B = G_A(test_real_A)\n",
    "    test_recon_A = G_B(test_fake_B)\n",
    "\n",
    "    test_real_B = test_real_B_data.cuda()\n",
    "    test_fake_A = G_B(test_real_B)\n",
    "    test_recon_B = G_A(test_fake_A)\n",
    "\n",
    "    plot_train_result([test_real_A, test_real_B], [test_fake_B, test_fake_A], [test_recon_A, test_recon_B], epoch, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8R8Dm-_PbdYZ"
   },
   "source": [
    "## Retreiving the Loss results\n",
    "\n",
    "We will not retreive the losses generated after learning and save them to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1605214810494,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "HD1iaAfVKBwE",
    "outputId": "163174cf-a752-426c-d2f7-e3aaf4accc67"
   },
   "outputs": [],
   "source": [
    "all_losses = pd.DataFrame()\n",
    "all_losses['D_A_avg_losses'] = D_A_avg_losses\n",
    "all_losses['D_B_avg_losses'] = D_B_avg_losses\n",
    "all_losses['G_A_avg_losses'] = G_A_avg_losses\n",
    "all_losses['G_B_avg_losses'] = G_B_avg_losses\n",
    "all_losses['cycle_A_avg_losses'] = cycle_A_avg_losses\n",
    "all_losses['cycle_B_avg_losses'] = cycle_B_avg_losses\n",
    "all_losses.to_csv('avg_losses',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRnk-OKcVnCS"
   },
   "outputs": [],
   "source": [
    "#save to csv\n",
    "all_losses.to_csv('all_losses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbAPQwnHZ0Lf"
   },
   "source": [
    "## Plotting all the Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 973,
     "status": "ok",
     "timestamp": 1605419257874,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "Pg9zV1VcZ4-Z",
    "outputId": "ebb85af7-1b5f-406c-a3b2-1ea88c1bef9a"
   },
   "outputs": [],
   "source": [
    "losses = pd.read_csv('all_losses.csv')\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "losses.plot()\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSl-hEnyj0ry"
   },
   "source": [
    "## Saving the Models to a file\n",
    "\n",
    "In order to reproduce, use the same model at a later point. We will save the models state_dict created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14284,
     "status": "ok",
     "timestamp": 1605216045984,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "3eLrZK55SwDB",
    "outputId": "6da7bf48-85fd-4b17-afbe-fb7461a773dd"
   },
   "outputs": [],
   "source": [
    "# Save Generators\n",
    "torch.save(Generator_A.state_dict(), 'Generator_A.pth')\n",
    "torch.save(Generator_B.state_dict(), 'Generator_B.pth')\n",
    "\n",
    "# Save Descriminators\n",
    "torch.save(Discriminator_A.state_dict(), 'Discriminator_A.pth')\n",
    "torch.save(Discriminator_B.state_dict(), 'Discriminator_B.pth')\n",
    "\n",
    "# Save Optimizers\n",
    "torch.save(Generator_optimizer.state_dict(), 'Generator_optimizer.pth')\n",
    "torch.save(Discriminator_A_optimizer.state_dict(), 'Discriminator_A_optimizer.pth')\n",
    "torch.save(Discriminator_B_optimizer.state_dict(), 'Discriminator_B_optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2J3cZy1DVJRa"
   },
   "outputs": [],
   "source": [
    "zip -r weights.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bucket.upload_file(Key=CABLE_PATH, Filename=CABLE_NAME)\n",
    "    print(f'File {CABLE_NAME} uploaded to S3 bucket {BUCKET_NAME}')\n",
    "except:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
