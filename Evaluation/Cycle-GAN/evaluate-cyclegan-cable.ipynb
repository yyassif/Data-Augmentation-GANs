{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:51:08.292604Z",
     "iopub.status.busy": "2023-06-17T20:51:08.292250Z",
     "iopub.status.idle": "2023-06-17T20:51:10.757892Z",
     "shell.execute_reply": "2023-06-17T20:51:10.756696Z",
     "shell.execute_reply.started": "2023-06-17T20:51:08.292575Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/cable-weights/* /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:51:14.911980Z",
     "iopub.status.busy": "2023-06-17T20:51:14.911534Z",
     "iopub.status.idle": "2023-06-17T20:51:18.288769Z",
     "shell.execute_reply": "2023-06-17T20:51:18.287534Z",
     "shell.execute_reply.started": "2023-06-17T20:51:14.911939Z"
    }
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/cable/* /kaggle/working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvVG92WsKeYm"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, we will attempt to greate a Cycle Generative Adversarial Network (cycleGAN) in order to manipulate defective industrial parts. We will attempt to augment images data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYM61P4RKeYo"
   },
   "source": [
    "Our first step is to load the neccessary libraries we will be using for the entire process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:32:29.945621Z",
     "iopub.status.busy": "2023-06-17T21:32:29.944798Z",
     "iopub.status.idle": "2023-06-17T21:32:30.469442Z",
     "shell.execute_reply": "2023-06-17T21:32:30.468469Z",
     "shell.execute_reply.started": "2023-06-17T21:32:29.945586Z"
    },
    "id": "2z7zHHmcKeYq"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os , itertools, cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TLPoFq6S14N"
   },
   "source": [
    "Since we are using two platforms, we will be assigning a directory to the location of the data for each platform. This will make it easier to access the data, depending the platform we will be using.\n",
    "\n",
    "The two platforms will be:\n",
    "1. A local installation of  Jupyter Notebook - work on local codes. GPU is limited\n",
    "2. A Google Colaboratory Notebook - Use of available resources and GPU to run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:51:33.752142Z",
     "iopub.status.busy": "2023-06-17T20:51:33.751567Z",
     "iopub.status.idle": "2023-06-17T20:51:33.757129Z",
     "shell.execute_reply": "2023-06-17T20:51:33.756048Z",
     "shell.execute_reply.started": "2023-06-17T20:51:33.752108Z"
    },
    "id": "AijKd1STKkUM"
   },
   "outputs": [],
   "source": [
    "data_dir = 'cable'\n",
    "train_dir = os.path.join(data_dir,'train')\n",
    "test_dir = os.path.join(data_dir,'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sqld3FCnL4Tc"
   },
   "source": [
    "## Parameters\n",
    "\n",
    "In this block, we will create global parameters which we can use at different areas of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:51:46.367259Z",
     "iopub.status.busy": "2023-06-17T20:51:46.366900Z",
     "iopub.status.idle": "2023-06-17T20:51:46.373297Z",
     "shell.execute_reply": "2023-06-17T20:51:46.372321Z",
     "shell.execute_reply.started": "2023-06-17T20:51:46.367230Z"
    },
    "id": "otQKmH-brXxc"
   },
   "outputs": [],
   "source": [
    "# Creating the Parameters\n",
    "\n",
    "parameters = {\n",
    "    'batch_size': 1,\n",
    "    'input_size': 256,\n",
    "    'resize_scale': 286,\n",
    "    'crop_size': 256,\n",
    "    'fliplr':True,\n",
    "    \n",
    "    # Model parameters\n",
    "    'num_epochs': 100,\n",
    "    'decay_epoch': 100,\n",
    "    'ngf': 32,   # Number of generator filters\n",
    "    'ndf': 64,   # Number of discriminator filters\n",
    "    'num_resnet': 6, # Number of resnet blocks\n",
    "    'lrG': 0.0002,    # Learning rate for generator\n",
    "    'lrD': 0.0002,    # Learning rate for discriminator\n",
    "    'beta1': 0.5 ,    # Beta1 for Adam optimizer\n",
    "    'beta2': 0.999 ,  # Beta2 for Adam optimizer\n",
    "    'lambdaA': 10 ,   # LambdaA for cycle loss\n",
    "    'lambdaB': 10  ,  # LambdaB for cycle loss\n",
    "}\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4gcJSynYgR9"
   },
   "source": [
    "## Creating functions\n",
    "\n",
    "In the next set, we will create several functions that will be used throughout. The first function will convert our data into Numpy, which we can call at a later point.\n",
    "\n",
    "We will also set a variable so we can use a GPU for faster processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:00:01.699578Z",
     "iopub.status.busy": "2023-06-17T21:00:01.699168Z",
     "iopub.status.idle": "2023-06-17T21:00:01.708479Z",
     "shell.execute_reply": "2023-06-17T21:00:01.707414Z",
     "shell.execute_reply.started": "2023-06-17T21:00:01.699548Z"
    },
    "id": "iM56xhMDyBIu"
   },
   "outputs": [],
   "source": [
    "# Convert to Numpy \n",
    "def to_numpy(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "def load_weights(model, optimizer, filename):\n",
    "    if os.path.isfile(filename):\n",
    "        ckpt = torch.load(filename)\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "    else:\n",
    "        print(f'No checkpoint found at {filename}')\n",
    "        \n",
    "def save_image(image, save_path, i):\n",
    "    img = to_numpy(image[0])\n",
    "    img = img.squeeze()\n",
    "    img = (((img - img.min()) * 255) / (img.max() - img.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "    img_path = os.path.join(save_path, f\"image_{i}.jpg\")\n",
    "    cv2.imwrite(img_path, img)\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in6MLzoYZBRb"
   },
   "source": [
    "### Transformer\n",
    "\n",
    "Since we will be using a __transformer__ at a later stage, we will set a transformer variable and assign it procedures which transforms an image size accordingly. This transformer will also convert the image into a Tensor and Normalize it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:52:15.254402Z",
     "iopub.status.busy": "2023-06-17T20:52:15.254044Z",
     "iopub.status.idle": "2023-06-17T20:52:15.260334Z",
     "shell.execute_reply": "2023-06-17T20:52:15.259010Z",
     "shell.execute_reply.started": "2023-06-17T20:52:15.254374Z"
    },
    "id": "uDjZWE75jLwO"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=parameters['input_size']),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_qDIcq5Z3KJ"
   },
   "source": [
    "Next, we will define an __ImagePool__ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:53:29.917862Z",
     "iopub.status.busy": "2023-06-17T20:53:29.917482Z",
     "iopub.status.idle": "2023-06-17T20:53:29.927134Z",
     "shell.execute_reply": "2023-06-17T20:53:29.925905Z",
     "shell.execute_reply.started": "2023-06-17T20:53:29.917809Z"
    },
    "id": "VM0YNJl5y58U"
   },
   "outputs": [],
   "source": [
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        for image in images.data:\n",
    "            image = torch.unsqueeze(image, 0)\n",
    "            if self.num_imgs < self.pool_size:\n",
    "                self.num_imgs = self.num_imgs + 1\n",
    "                self.images.append(image)\n",
    "                return_images.append(image)\n",
    "            else:\n",
    "                p = random.uniform(0, 1)\n",
    "                if p > 0.5:\n",
    "                    random_id = random.randint(0, self.pool_size-1)\n",
    "                    tmp = self.images[random_id].clone()\n",
    "                    self.images[random_id] = image\n",
    "                    return_images.append(tmp)\n",
    "                else:\n",
    "                    return_images.append(image)\n",
    "        return_images = Variable(torch.cat(return_images, 0))\n",
    "        return return_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWviChqSaQoP"
   },
   "source": [
    "### Creating the Dataloader\n",
    "\n",
    "In this section, we begin by creating the __dataloader__ which we will use to retrieve the data in parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:55:37.304032Z",
     "iopub.status.busy": "2023-06-17T20:55:37.303658Z",
     "iopub.status.idle": "2023-06-17T20:55:37.314804Z",
     "shell.execute_reply": "2023-06-17T20:55:37.313930Z",
     "shell.execute_reply.started": "2023-06-17T20:55:37.304002Z"
    }
   },
   "outputs": [],
   "source": [
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, image_dir, subfolder='train', transform=None, resize_scale=None, crop_size=None, fliplr=False):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.input_path = os.path.join(image_dir, subfolder)\n",
    "        self.image_filenames = [x for x in sorted(os.listdir(self.input_path))]\n",
    "        self.transform = transform\n",
    "\n",
    "        self.resize_scale = resize_scale\n",
    "        self.crop_size = crop_size\n",
    "        self.fliplr = fliplr\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        img_fn = os.path.join(self.input_path, self.image_filenames[index])\n",
    "        img = Image.open(img_fn).convert('RGB')\n",
    "\n",
    "        # preprocessing\n",
    "        if self.resize_scale:\n",
    "            img = img.resize((self.resize_scale, self.resize_scale), Image.BILINEAR)\n",
    "\n",
    "        if self.crop_size:\n",
    "            x = random.randint(0, self.resize_scale - self.crop_size + 1)\n",
    "            y = random.randint(0, self.resize_scale - self.crop_size + 1)\n",
    "            img = img.crop((x, y, x + self.crop_size, y + self.crop_size))\n",
    "        if self.fliplr:\n",
    "            if random.random() < 0.5:\n",
    "                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Es9nw6Xaecu"
   },
   "source": [
    "### Creating the Convolutional Block\n",
    "\n",
    "The __convolution__ block will consist of two things.\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "2. Forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:53:32.682969Z",
     "iopub.status.busy": "2023-06-17T20:53:32.682499Z",
     "iopub.status.idle": "2023-06-17T20:53:32.692301Z",
     "shell.execute_reply": "2023-06-17T20:53:32.691249Z",
     "shell.execute_reply.started": "2023-06-17T20:53:32.682938Z"
    },
    "id": "KLJ6rMcTpZRv"
   },
   "outputs": [],
   "source": [
    "class ConvBlock(torch.nn.Module):\n",
    "    def __init__(self,input_size,output_size,kernel_size=3,stride=2,padding=1,activation='relu',batch_norm=True):\n",
    "        super(ConvBlock,self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(input_size,output_size,kernel_size,stride,padding)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.InstanceNorm2d(output_size)\n",
    "        self.activation = activation\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "        self.lrelu = torch.nn.LeakyReLU(0.2,True)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    def forward(self,x):\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.conv(x))\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(out)\n",
    "        elif self.activation == 'lrelu':\n",
    "            return self.lrelu(out)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.tanh(out)\n",
    "        elif self.activation == 'no_act':\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XBNtHuHbZp9"
   },
   "source": [
    "### De-Convolution Function\n",
    "\n",
    "In this class we will create, we will need to bring back the convolution from the results previously processed. There will be two functions inside this class.\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "2. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:53:33.644963Z",
     "iopub.status.busy": "2023-06-17T20:53:33.644283Z",
     "iopub.status.idle": "2023-06-17T20:53:33.653523Z",
     "shell.execute_reply": "2023-06-17T20:53:33.652258Z",
     "shell.execute_reply.started": "2023-06-17T20:53:33.644928Z"
    },
    "id": "YF4rQT_VpiDU"
   },
   "outputs": [],
   "source": [
    "class DeconvBlock(torch.nn.Module):\n",
    "    def __init__(self,input_size,output_size,kernel_size=3,stride=2,padding=1,output_padding=1,activation='relu',batch_norm=True):\n",
    "        super(DeconvBlock,self).__init__()\n",
    "        self.deconv = torch.nn.ConvTranspose2d(input_size,output_size,kernel_size,stride,padding,output_padding)\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn = torch.nn.InstanceNorm2d(output_size)\n",
    "        self.activation = activation\n",
    "        self.relu = torch.nn.ReLU(True)\n",
    "    def forward(self,x):\n",
    "        if self.batch_norm:\n",
    "            out = self.bn(self.deconv(x))\n",
    "        else:\n",
    "            out = self.deconv(x)\n",
    "        if self.activation == 'relu':\n",
    "            return self.relu(out)\n",
    "        elif self.activation == 'lrelu':\n",
    "            return self.lrelu(out)\n",
    "        elif self.activation == 'tanh':\n",
    "            return self.tanh(out)\n",
    "        elif self.activation == 'no_act':\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMTWZK_QcFI_"
   },
   "source": [
    "### Residual Learning Block\n",
    "\n",
    "In this secton, we will create a __residual learning__ block or __Resnet__. This class will also contain two functions.\n",
    "\n",
    "1. Initialization\n",
    "\n",
    "2. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:53:35.220053Z",
     "iopub.status.busy": "2023-06-17T20:53:35.219369Z",
     "iopub.status.idle": "2023-06-17T20:53:35.227969Z",
     "shell.execute_reply": "2023-06-17T20:53:35.226900Z",
     "shell.execute_reply.started": "2023-06-17T20:53:35.220015Z"
    },
    "id": "mr_C4hnKpkyw"
   },
   "outputs": [],
   "source": [
    "class ResnetBlock(torch.nn.Module):\n",
    "    def __init__(self,num_filter,kernel_size=3,stride=1,padding=0):\n",
    "        super(ResnetBlock,self).__init__()\n",
    "        conv1 = torch.nn.Conv2d(num_filter,num_filter,kernel_size,stride,padding)\n",
    "        conv2 = torch.nn.Conv2d(num_filter,num_filter,kernel_size,stride,padding)\n",
    "        bn = torch.nn.InstanceNorm2d(num_filter)\n",
    "        relu = torch.nn.ReLU(True)\n",
    "        pad = torch.nn.ReflectionPad2d(1)\n",
    "\n",
    "        self.resnet_block = torch.nn.Sequential(pad, conv1, bn, relu, pad, conv2, bn)\n",
    "    def forward(self,x):\n",
    "        out = self.resnet_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL_Aa8m0g5OB"
   },
   "source": [
    "## Creating the Generator\n",
    "\n",
    "We will now create the __Generator__. The Generator will consist of the following:\n",
    "1. Encoder\n",
    "2. Transformer\n",
    "3. Decoder\n",
    "\n",
    "In addition, we will also be adding __weights__.\n",
    "\n",
    "After creating the class, we will create the actuall Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:53:36.443812Z",
     "iopub.status.busy": "2023-06-17T20:53:36.443458Z",
     "iopub.status.idle": "2023-06-17T20:53:36.457503Z",
     "shell.execute_reply": "2023-06-17T20:53:36.456435Z",
     "shell.execute_reply.started": "2023-06-17T20:53:36.443782Z"
    },
    "id": "sO6cRW__hB-E"
   },
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self,input_dim,num_filter,output_dim,num_resnet):\n",
    "        super(Generator,self).__init__()\n",
    "\n",
    "        # Reflection padding\n",
    "        self.pad = torch.nn.ReflectionPad2d(3)\n",
    "        # Encoder\n",
    "        self.conv1 = ConvBlock(input_dim,num_filter,kernel_size=7,stride=1,padding=0)\n",
    "        self.conv2 = ConvBlock(num_filter,num_filter*2)\n",
    "        self.conv3 = ConvBlock(num_filter*2,num_filter*4)\n",
    "        # Resnet blocks\n",
    "        self.resnet_blocks = []\n",
    "        for i in range(num_resnet):\n",
    "            self.resnet_blocks.append(ResnetBlock(num_filter*4))\n",
    "        self.resnet_blocks = torch.nn.Sequential(*self.resnet_blocks)\n",
    "        #Decoder\n",
    "        self.deconv1 = DeconvBlock(num_filter*4,num_filter*2)\n",
    "        self.deconv2 = DeconvBlock(num_filter*2,num_filter)\n",
    "        self.deconv3 = ConvBlock(num_filter,output_dim,kernel_size=7,stride=1,padding=0,activation='tanh',batch_norm=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Encoder\n",
    "        enc1 = self.conv1(self.pad(x))\n",
    "        enc2 = self.conv2(enc1)\n",
    "        enc3 = self.conv3(enc2)\n",
    "        # Resnet blocks\n",
    "        res = self.resnet_blocks(enc3)\n",
    "        # Decoder\n",
    "        dec1 = self.deconv1(res)\n",
    "        dec2 = self.deconv2(dec1)\n",
    "        out = self.deconv3(self.pad(dec2))\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self,mean=0.0,std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m,ConvBlock):\n",
    "                torch.nn.init.normal_(m.conv.weight,mean,std)\n",
    "            if isinstance(m,DeconvBlock):\n",
    "                torch.nn.init.normal_(m.deconv.weight,mean,std)\n",
    "            if isinstance(m,ResnetBlock):\n",
    "                torch.nn.init.normal_(m.conv.weight,mean,std)\n",
    "                torch.nn.init.constant_(m.conv.bias,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_l3oAV-dZwA"
   },
   "source": [
    "### Creating the Generator and Testing the Neural Network\n",
    "\n",
    "Since we will be using a Cycle GAN, which requires 2 Neural Networks competing with each other, we will create 2 Generators as its requirement. We will also test it by calling it and view the results created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIJRhsDF9FBt"
   },
   "source": [
    "#### Generator A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:54:09.271918Z",
     "iopub.status.busy": "2023-06-17T20:54:09.271521Z",
     "iopub.status.idle": "2023-06-17T20:54:12.333645Z",
     "shell.execute_reply": "2023-06-17T20:54:12.332724Z",
     "shell.execute_reply.started": "2023-06-17T20:54:09.271885Z"
    },
    "executionInfo": {
     "elapsed": 11322,
     "status": "ok",
     "timestamp": 1605203530649,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "GWqNSHCIdbFI",
    "outputId": "21b7b67c-6e64-4d5d-d901-3bc0aea22943"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (conv3): ConvBlock(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (resnet_blocks): Sequential(\n",
       "    (0): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (1): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (2): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (3): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (4): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (5): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (deconv1): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv2): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv3): ConvBlock(\n",
       "    (conv): Conv2d(32, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (bn): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Generators\n",
    "G_A = Generator(3, parameters['ngf'], 3, parameters['num_resnet']).cuda() # input_dim, num_filter, output_dim, num_resnet\n",
    "G_A.load_state_dict(torch.load('Generator_A.pth'))\n",
    "G_A.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MADq2MXY9JtS"
   },
   "source": [
    "#### Generator B\n",
    "\n",
    "Repeating the previous step, but for Generator B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:54:19.327420Z",
     "iopub.status.busy": "2023-06-17T20:54:19.327068Z",
     "iopub.status.idle": "2023-06-17T20:54:19.378331Z",
     "shell.execute_reply": "2023-06-17T20:54:19.377299Z",
     "shell.execute_reply.started": "2023-06-17T20:54:19.327391Z"
    },
    "executionInfo": {
     "elapsed": 488,
     "status": "ok",
     "timestamp": 1605203532238,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "h4mRA2cZeJ4T",
    "outputId": "c45a76b1-c60c-445f-be69-cb2d27cf13c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (pad): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (conv1): ConvBlock(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (conv2): ConvBlock(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (conv3): ConvBlock(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (resnet_blocks): Sequential(\n",
       "    (0): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (1): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (2): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (3): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (4): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (5): ResnetBlock(\n",
       "      (resnet_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (deconv1): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv2): DeconvBlock(\n",
       "    (deconv): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (bn): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (deconv3): ConvBlock(\n",
       "    (conv): Conv2d(32, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (bn): InstanceNorm2d(3, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_B = Generator(3, parameters['ngf'], 3, parameters['num_resnet']).cuda()\n",
    "G_B.load_state_dict(torch.load('Generator_B.pth'))\n",
    "G_B.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWhXRqM4lS6v"
   },
   "source": [
    "## Creating the Discriminator\n",
    "\n",
    "The Discriminator will consist of the following\n",
    "1. Initialization\n",
    "2. Forward Pass\n",
    "3. Weights\n",
    "\n",
    "After creating the class, we will create the actuall Discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:54:25.308467Z",
     "iopub.status.busy": "2023-06-17T20:54:25.308104Z",
     "iopub.status.idle": "2023-06-17T20:54:25.319524Z",
     "shell.execute_reply": "2023-06-17T20:54:25.318420Z",
     "shell.execute_reply.started": "2023-06-17T20:54:25.308437Z"
    },
    "id": "eJsYjXHYg8AC"
   },
   "outputs": [],
   "source": [
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self,input_dim,num_filter,output_dim):\n",
    "        super(Discriminator,self).__init__()\n",
    "        conv1 = ConvBlock(input_dim,num_filter,kernel_size=4,stride=2,padding=1,activation='lrelu',batch_norm=False)\n",
    "        conv2 = ConvBlock(num_filter,num_filter*2,kernel_size=4,stride=2,padding=1,activation='lrelu')\n",
    "        conv3 = ConvBlock(num_filter*2,num_filter*4,kernel_size=4,stride=2,padding=1,activation='lrelu')\n",
    "        conv4 = ConvBlock(num_filter*4,num_filter*8,kernel_size=4,stride=1,padding=1,activation='lrelu')\n",
    "        conv5 = ConvBlock(num_filter*8,output_dim,kernel_size=4,stride=1,padding=1,activation='no_act',batch_norm=False)\n",
    "        self.conv_blocks = torch.nn.Sequential(conv1, conv2, conv3, conv4, conv5)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.conv_blocks(x)\n",
    "        return out\n",
    "\n",
    "    def normal_weight_init(self,mean=0.0,std=0.02):\n",
    "        for m in self.children():\n",
    "            if isinstance(m,ConvBlock):\n",
    "                torch.nn.init.normal_(m.conv.weight.data,mean,std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36IywNAtd-xY"
   },
   "source": [
    "### Creating the Discriminator and Testing the Neural Network\n",
    "\n",
    "Similar with the Generator, which requires 2 Neural Networks. A cycleGAN also uses 2 Discriminators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YYXIKrz81I8"
   },
   "source": [
    "#### Discriminator A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:54:54.504758Z",
     "iopub.status.busy": "2023-06-17T20:54:54.504374Z",
     "iopub.status.idle": "2023-06-17T20:54:54.551962Z",
     "shell.execute_reply": "2023-06-17T20:54:54.550897Z",
     "shell.execute_reply.started": "2023-06-17T20:54:54.504727Z"
    },
    "executionInfo": {
     "elapsed": 813,
     "status": "ok",
     "timestamp": 1605219271350,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "1vcEqAoNeCKR",
    "outputId": "1e06bff6-048a-4eae-a6ba-ef81c0309f36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (4): ConvBlock(\n",
       "      (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Discriminators\n",
    "D_A = Discriminator(3, parameters['ndf'], 1).cuda() # input_dim, num_filter, output_dim\n",
    "D_A.load_state_dict(torch.load('Discriminator_A.pth'))\n",
    "D_A.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJhcGbtZ88Nh"
   },
   "source": [
    "#### Discriminator B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:55:00.612231Z",
     "iopub.status.busy": "2023-06-17T20:55:00.611874Z",
     "iopub.status.idle": "2023-06-17T20:55:00.658272Z",
     "shell.execute_reply": "2023-06-17T20:55:00.657074Z",
     "shell.execute_reply.started": "2023-06-17T20:55:00.612203Z"
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1605203538689,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "RHLJKzoMemlT",
    "outputId": "6f572964-b3a6-4330-e888-02129845ce4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (3): ConvBlock(\n",
       "      (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "    (4): ConvBlock(\n",
       "      (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "      (bn): InstanceNorm2d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "      (tanh): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_B = Discriminator(3, parameters['ndf'], 1).cuda()\n",
    "D_B.load_state_dict(torch.load('Discriminator_B.pth'))\n",
    "D_B.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM0cpsAcHpON"
   },
   "source": [
    "## Training Block / DataLoaders\n",
    "\n",
    "In this block, we will create the actual training block we will be using to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:56:18.121443Z",
     "iopub.status.busy": "2023-06-17T20:56:18.120742Z",
     "iopub.status.idle": "2023-06-17T20:56:18.130771Z",
     "shell.execute_reply": "2023-06-17T20:56:18.129904Z",
     "shell.execute_reply.started": "2023-06-17T20:56:18.121408Z"
    },
    "id": "hl_9r6z8q84A"
   },
   "outputs": [],
   "source": [
    "train_data_A = DatasetFromFolder(train_dir, subfolder='good', transform=transform,resize_scale=parameters['resize_scale'], crop_size=parameters['crop_size'], fliplr=parameters['fliplr'])\n",
    "train_data_loader_A = torch.utils.data.DataLoader(dataset=train_data_A,batch_size=parameters['batch_size'], shuffle=True)\n",
    "train_data_B = DatasetFromFolder(train_dir, subfolder='bad', transform=transform, resize_scale=parameters['resize_scale'], crop_size=parameters['crop_size'], fliplr=parameters['fliplr'])\n",
    "train_data_loader_B = torch.utils.data.DataLoader(dataset=train_data_B, batch_size=parameters['batch_size'], shuffle=True)\n",
    "\n",
    "#Load test data\n",
    "test_data_A = DatasetFromFolder(test_dir, subfolder='good', transform=transform)\n",
    "test_data_loader_A = torch.utils.data.DataLoader(dataset=test_data_A, batch_size=parameters['batch_size'], shuffle=False)\n",
    "test_data_B = DatasetFromFolder(test_dir, subfolder='bad', transform=transform)\n",
    "test_data_loader_B = torch.utils.data.DataLoader(dataset=test_data_B, batch_size=parameters['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gZoWnJchfFq"
   },
   "source": [
    "## Creating the Optimizer\n",
    "\n",
    "In this block, we need to create the learning optimizers. We will have one optimizer for the Generator and two optimizers for the Discriminator. The Discriminator has two optimizers as this function checks the data generated by the Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:56:21.231126Z",
     "iopub.status.busy": "2023-06-17T20:56:21.230738Z",
     "iopub.status.idle": "2023-06-17T20:56:21.348473Z",
     "shell.execute_reply": "2023-06-17T20:56:21.347539Z",
     "shell.execute_reply.started": "2023-06-17T20:56:21.231096Z"
    },
    "id": "aULDtm48JVS4"
   },
   "outputs": [],
   "source": [
    "G_optimizer = torch.optim.Adam(itertools.chain(G_A.parameters(), G_B.parameters()), lr=parameters['lrG'], betas=(parameters['beta1'], parameters['beta2']))\n",
    "G_optimizer.load_state_dict(torch.load('Generator_optimizer.pth'))\n",
    "\n",
    "D_A_optimizer = torch.optim.Adam(D_A.parameters(), lr=parameters['lrD'], betas=(parameters['beta1'], parameters['beta2']))\n",
    "D_A_optimizer.load_state_dict(torch.load('Discriminator_A_optimizer.pth'))\n",
    "\n",
    "D_B_optimizer = torch.optim.Adam(D_B.parameters(), lr=parameters['lrD'], betas=(parameters['beta1'], parameters['beta2']))\n",
    "D_B_optimizer.load_state_dict(torch.load('Discriminator_B_optimizer.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_S7jO44wNvk"
   },
   "source": [
    "## Defining Losses\n",
    "\n",
    "In order to make sure that there is learning, we will create cycle losses, which we will pass on at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:56:22.436540Z",
     "iopub.status.busy": "2023-06-17T20:56:22.436194Z",
     "iopub.status.idle": "2023-06-17T20:56:22.441688Z",
     "shell.execute_reply": "2023-06-17T20:56:22.440423Z",
     "shell.execute_reply.started": "2023-06-17T20:56:22.436513Z"
    },
    "id": "WGS3-iixJao8"
   },
   "outputs": [],
   "source": [
    "MSE_Loss = torch.nn.MSELoss().cuda()\n",
    "L1_Loss = torch.nn.L1Loss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:56:23.033855Z",
     "iopub.status.busy": "2023-06-17T20:56:23.033436Z",
     "iopub.status.idle": "2023-06-17T20:56:23.040533Z",
     "shell.execute_reply": "2023-06-17T20:56:23.039360Z",
     "shell.execute_reply.started": "2023-06-17T20:56:23.033804Z"
    },
    "executionInfo": {
     "elapsed": 14284,
     "status": "ok",
     "timestamp": 1605216045984,
     "user": {
      "displayName": "Lawrence Quesada",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgDfi5ZMUHec5oABIqu-Gh8jDLncpZUMsEadkSem7Y=s64",
      "userId": "14131790862529985455"
     },
     "user_tz": 480
    },
    "id": "3eLrZK55SwDB",
    "outputId": "6da7bf48-85fd-4b17-afbe-fb7461a773dd"
   },
   "outputs": [],
   "source": [
    "good_save_path = 'good_generated_images'\n",
    "bad_save_path = 'bad_generated_images'\n",
    "os.makedirs(good_save_path, exist_ok=True)\n",
    "os.makedirs(bad_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:02:21.502786Z",
     "iopub.status.busy": "2023-06-17T21:02:21.502404Z",
     "iopub.status.idle": "2023-06-17T21:02:28.931478Z",
     "shell.execute_reply": "2023-06-17T21:02:28.930422Z",
     "shell.execute_reply.started": "2023-06-17T21:02:21.502753Z"
    }
   },
   "outputs": [],
   "source": [
    "test_real_A_data = train_data_A.__getitem__(8).unsqueeze(0)\n",
    "test_real_B_data = train_data_B.__getitem__(6).unsqueeze(0)\n",
    "i = 0\n",
    "for epoch in range(200):\n",
    "    # Show result for test image\n",
    "    test_real_A = test_real_A_data.cuda()\n",
    "    test_fake_B = G_A(test_real_A)\n",
    "    test_recon_A = G_B(test_fake_B)\n",
    "\n",
    "    test_real_B = test_real_B_data.cuda()\n",
    "    test_fake_A = G_B(test_real_B)\n",
    "    test_recon_B = G_A(test_fake_A)\n",
    "    (bad_one, good_one), (good_two, bad_two) = [test_fake_B, test_fake_A], [test_recon_A, test_recon_B]\n",
    "    \n",
    "    save_image(good_one, good_save_path, i)\n",
    "    i +=1\n",
    "    save_image(good_two, good_save_path, i)\n",
    "    i +=1\n",
    "    save_image(bad_one, bad_save_path, i)\n",
    "    i +=1\n",
    "    save_image(bad_two, bad_save_path, i)\n",
    "    i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r cable-generated-images.zip good_generated_images/* bad_generated_images/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/working/good_generated_images/* /kaggle/working/cable/train/good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/working/good_generated_images/* /kaggle/working/cable/test/good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/working/bad_generated_images/* /kaggle/working/cable/train/bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r /kaggle/working/bad_generated_images/* /kaggle/working/cable/test/bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:18:17.337949Z",
     "iopub.status.busy": "2023-06-17T21:18:17.337518Z",
     "iopub.status.idle": "2023-06-17T21:18:17.343663Z",
     "shell.execute_reply": "2023-06-17T21:18:17.342611Z",
     "shell.execute_reply.started": "2023-06-17T21:18:17.337903Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = 'cable'\n",
    "train_dir = os.path.join(data_dir,'train')\n",
    "test_dir = os.path.join(data_dir,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:18:17.671449Z",
     "iopub.status.busy": "2023-06-17T21:18:17.670763Z",
     "iopub.status.idle": "2023-06-17T21:18:17.681048Z",
     "shell.execute_reply": "2023-06-17T21:18:17.680120Z",
     "shell.execute_reply.started": "2023-06-17T21:18:17.671413Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(data.Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        super(ImageDataset, self).__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "\n",
    "        # Iterate over the data directory and load images\n",
    "        for root, dirs, files in os.walk(data_dir):\n",
    "            for file in files:\n",
    "                image_path = os.path.join(root, file)\n",
    "                image = Image.open(image_path)\n",
    "                self.data.append(image)\n",
    "\n",
    "                # Determine the class label based on the directory name\n",
    "                class_label = os.path.basename(root)\n",
    "                if class_label == \"good\":\n",
    "                    self.targets.append(1)  # Assign class 1 for \"good\"\n",
    "                elif class_label == \"bad\":\n",
    "                    self.targets.append(0)  # Assign class 0 for \"bad\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:18:18.295059Z",
     "iopub.status.busy": "2023-06-17T21:18:18.294687Z",
     "iopub.status.idle": "2023-06-17T21:18:18.304712Z",
     "shell.execute_reply": "2023-06-17T21:18:18.303814Z",
     "shell.execute_reply.started": "2023-06-17T21:18:18.295031Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 16 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:21:08.737384Z",
     "iopub.status.busy": "2023-06-17T21:21:08.736692Z",
     "iopub.status.idle": "2023-06-17T21:21:08.783901Z",
     "shell.execute_reply": "2023-06-17T21:21:08.783004Z",
     "shell.execute_reply.started": "2023-06-17T21:21:08.737350Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(50),  # Rotate randomly between -50 and 50 degrees\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),  # Random scale and crop to 256x256\n",
    "    transforms.RandomHorizontalFlip(),  # Flip horizontally with a 50% chance\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = ImageDataset(train_dir, transform=transform)\n",
    "test_dataset = ImageDataset(test_dir, transform=transform)\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Set device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the CNN model\n",
    "model = CNN().to(device)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:21:13.318686Z",
     "iopub.status.busy": "2023-06-17T21:21:13.318326Z",
     "iopub.status.idle": "2023-06-17T21:30:12.997121Z",
     "shell.execute_reply": "2023-06-17T21:30:12.996055Z",
     "shell.execute_reply.started": "2023-06-17T21:21:13.318655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.6564\n",
      "Epoch [1/500], Loss: 0.0916\n",
      "Epoch [1/500], Loss: 7.2464\n",
      "Epoch [1/500], Loss: 0.0011\n",
      "Epoch [1/500], Loss: 0.3414\n",
      "Epoch [1/500], Loss: 0.2940\n",
      "Epoch [1/500], Loss: 0.2985\n",
      "Epoch [1/500], Loss: 0.2617\n",
      "Epoch [2/500], Loss: 0.2017\n",
      "Epoch [2/500], Loss: 0.2402\n",
      "Epoch [2/500], Loss: 0.4276\n",
      "Epoch [2/500], Loss: 0.0111\n",
      "Epoch [2/500], Loss: 0.1430\n",
      "Epoch [2/500], Loss: 0.2937\n",
      "Epoch [2/500], Loss: 0.1484\n",
      "Epoch [2/500], Loss: 0.0401\n",
      "Epoch [3/500], Loss: 0.0524\n",
      "Epoch [3/500], Loss: 0.2312\n",
      "Epoch [3/500], Loss: 0.4185\n",
      "Epoch [3/500], Loss: 0.1493\n",
      "Epoch [3/500], Loss: 0.3166\n",
      "Epoch [3/500], Loss: 0.0948\n",
      "Epoch [3/500], Loss: 0.0860\n",
      "Epoch [3/500], Loss: 0.0567\n",
      "Epoch [4/500], Loss: 0.3559\n",
      "Epoch [4/500], Loss: 0.0217\n",
      "Epoch [4/500], Loss: 0.1435\n",
      "Epoch [4/500], Loss: 0.0108\n",
      "Epoch [4/500], Loss: 0.4646\n",
      "Epoch [4/500], Loss: 0.1483\n",
      "Epoch [4/500], Loss: 0.1377\n",
      "Epoch [4/500], Loss: 0.3500\n",
      "Epoch [5/500], Loss: 0.3144\n",
      "Epoch [5/500], Loss: 0.1136\n",
      "Epoch [5/500], Loss: 0.1316\n",
      "Epoch [5/500], Loss: 0.3204\n",
      "Epoch [5/500], Loss: 0.1172\n",
      "Epoch [5/500], Loss: 0.1621\n",
      "Epoch [5/500], Loss: 0.2501\n",
      "Epoch [5/500], Loss: 0.3560\n",
      "Epoch [6/500], Loss: 0.0422\n",
      "Epoch [6/500], Loss: 0.0361\n",
      "Epoch [6/500], Loss: 0.3633\n",
      "Epoch [6/500], Loss: 0.3892\n",
      "Epoch [6/500], Loss: 0.2461\n",
      "Epoch [6/500], Loss: 0.1374\n",
      "Epoch [6/500], Loss: 0.1414\n",
      "Epoch [6/500], Loss: 0.0629\n",
      "Epoch [7/500], Loss: 0.0589\n",
      "Epoch [7/500], Loss: 0.1433\n",
      "Epoch [7/500], Loss: 0.3346\n",
      "Epoch [7/500], Loss: 0.0357\n",
      "Epoch [7/500], Loss: 0.2544\n",
      "Epoch [7/500], Loss: 0.0292\n",
      "Epoch [7/500], Loss: 0.4778\n",
      "Epoch [7/500], Loss: 0.0338\n",
      "Epoch [8/500], Loss: 0.0396\n",
      "Epoch [8/500], Loss: 0.0379\n",
      "Epoch [8/500], Loss: 0.1383\n",
      "Epoch [8/500], Loss: 0.1365\n",
      "Epoch [8/500], Loss: 0.2535\n",
      "Epoch [8/500], Loss: 0.5796\n",
      "Epoch [8/500], Loss: 0.1413\n",
      "Epoch [8/500], Loss: 0.0659\n",
      "Epoch [9/500], Loss: 0.1588\n",
      "Epoch [9/500], Loss: 0.0807\n",
      "Epoch [9/500], Loss: 0.2308\n",
      "Epoch [9/500], Loss: 0.2296\n",
      "Epoch [9/500], Loss: 0.2364\n",
      "Epoch [9/500], Loss: 0.1482\n",
      "Epoch [9/500], Loss: 0.1417\n",
      "Epoch [9/500], Loss: 0.3370\n",
      "Epoch [10/500], Loss: 0.1420\n",
      "Epoch [10/500], Loss: 0.3364\n",
      "Epoch [10/500], Loss: 0.1378\n",
      "Epoch [10/500], Loss: 0.1445\n",
      "Epoch [10/500], Loss: 0.1414\n",
      "Epoch [10/500], Loss: 0.1437\n",
      "Epoch [10/500], Loss: 0.1431\n",
      "Epoch [10/500], Loss: 0.3318\n",
      "Epoch [11/500], Loss: 0.1341\n",
      "Epoch [11/500], Loss: 0.1440\n",
      "Epoch [11/500], Loss: 0.3520\n",
      "Epoch [11/500], Loss: 0.1479\n",
      "Epoch [11/500], Loss: 0.2380\n",
      "Epoch [11/500], Loss: 0.1395\n",
      "Epoch [11/500], Loss: 0.1423\n",
      "Epoch [11/500], Loss: 0.0547\n",
      "Epoch [12/500], Loss: 0.1370\n",
      "Epoch [12/500], Loss: 0.0397\n",
      "Epoch [12/500], Loss: 0.0287\n",
      "Epoch [12/500], Loss: 0.1420\n",
      "Epoch [12/500], Loss: 0.4046\n",
      "Epoch [12/500], Loss: 0.4011\n",
      "Epoch [12/500], Loss: 0.2414\n",
      "Epoch [12/500], Loss: 0.0332\n",
      "Epoch [13/500], Loss: 0.1412\n",
      "Epoch [13/500], Loss: 0.0590\n",
      "Epoch [13/500], Loss: 0.0607\n",
      "Epoch [13/500], Loss: 0.2353\n",
      "Epoch [13/500], Loss: 0.1513\n",
      "Epoch [13/500], Loss: 0.3357\n",
      "Epoch [13/500], Loss: 0.2554\n",
      "Epoch [13/500], Loss: 0.3579\n",
      "Epoch [14/500], Loss: 0.2126\n",
      "Epoch [14/500], Loss: 0.1528\n",
      "Epoch [14/500], Loss: 0.0816\n",
      "Epoch [14/500], Loss: 0.2218\n",
      "Epoch [14/500], Loss: 0.1407\n",
      "Epoch [14/500], Loss: 0.2229\n",
      "Epoch [14/500], Loss: 0.1299\n",
      "Epoch [14/500], Loss: 0.3700\n",
      "Epoch [15/500], Loss: 0.2402\n",
      "Epoch [15/500], Loss: 0.2382\n",
      "Epoch [15/500], Loss: 0.0483\n",
      "Epoch [15/500], Loss: 0.3298\n",
      "Epoch [15/500], Loss: 0.2357\n",
      "Epoch [15/500], Loss: 0.0574\n",
      "Epoch [15/500], Loss: 0.1408\n",
      "Epoch [15/500], Loss: 0.0516\n",
      "Epoch [16/500], Loss: 0.2393\n",
      "Epoch [16/500], Loss: 0.1426\n",
      "Epoch [16/500], Loss: 0.0379\n",
      "Epoch [16/500], Loss: 0.2642\n",
      "Epoch [16/500], Loss: 0.0270\n",
      "Epoch [16/500], Loss: 0.1469\n",
      "Epoch [16/500], Loss: 0.4840\n",
      "Epoch [16/500], Loss: 0.0306\n",
      "Epoch [17/500], Loss: 0.0360\n",
      "Epoch [17/500], Loss: 0.2482\n",
      "Epoch [17/500], Loss: 0.1383\n",
      "Epoch [17/500], Loss: 0.1427\n",
      "Epoch [17/500], Loss: 0.2381\n",
      "Epoch [17/500], Loss: 0.1378\n",
      "Epoch [17/500], Loss: 0.2330\n",
      "Epoch [17/500], Loss: 0.3492\n",
      "Epoch [18/500], Loss: 0.0778\n",
      "Epoch [18/500], Loss: 0.1439\n",
      "Epoch [18/500], Loss: 0.2352\n",
      "Epoch [18/500], Loss: 0.0449\n",
      "Epoch [18/500], Loss: 0.2339\n",
      "Epoch [18/500], Loss: 0.3306\n",
      "Epoch [18/500], Loss: 0.2397\n",
      "Epoch [18/500], Loss: 0.0459\n",
      "Epoch [19/500], Loss: 0.2388\n",
      "Epoch [19/500], Loss: 0.2362\n",
      "Epoch [19/500], Loss: 0.4025\n",
      "Epoch [19/500], Loss: 0.0651\n",
      "Epoch [19/500], Loss: 0.0718\n",
      "Epoch [19/500], Loss: 0.2262\n",
      "Epoch [19/500], Loss: 0.0625\n",
      "Epoch [19/500], Loss: 0.0553\n",
      "Epoch [20/500], Loss: 0.1190\n",
      "Epoch [20/500], Loss: 0.0275\n",
      "Epoch [20/500], Loss: 0.0185\n",
      "Epoch [20/500], Loss: 0.2873\n",
      "Epoch [20/500], Loss: 0.4171\n",
      "Epoch [20/500], Loss: 0.1483\n",
      "Epoch [20/500], Loss: 0.3935\n",
      "Epoch [20/500], Loss: 0.0398\n",
      "Epoch [21/500], Loss: 0.0474\n",
      "Epoch [21/500], Loss: 0.0530\n",
      "Epoch [21/500], Loss: 0.2225\n",
      "Epoch [21/500], Loss: 0.2132\n",
      "Epoch [21/500], Loss: 0.2365\n",
      "Epoch [21/500], Loss: 0.2208\n",
      "Epoch [21/500], Loss: 0.1159\n",
      "Epoch [21/500], Loss: 0.3156\n",
      "Epoch [22/500], Loss: 0.2229\n",
      "Epoch [22/500], Loss: 0.2225\n",
      "Epoch [22/500], Loss: 0.1301\n",
      "Epoch [22/500], Loss: 0.0669\n",
      "Epoch [22/500], Loss: 0.2769\n",
      "Epoch [22/500], Loss: 0.1388\n",
      "Epoch [22/500], Loss: 0.1344\n",
      "Epoch [22/500], Loss: 0.0250\n",
      "Epoch [23/500], Loss: 0.0265\n",
      "Epoch [23/500], Loss: 0.0904\n",
      "Epoch [23/500], Loss: 0.3515\n",
      "Epoch [23/500], Loss: 0.1157\n",
      "Epoch [23/500], Loss: 0.1453\n",
      "Epoch [23/500], Loss: 0.2774\n",
      "Epoch [23/500], Loss: 0.1990\n",
      "Epoch [23/500], Loss: 0.0432\n",
      "Epoch [24/500], Loss: 0.0578\n",
      "Epoch [24/500], Loss: 0.1845\n",
      "Epoch [24/500], Loss: 0.0550\n",
      "Epoch [24/500], Loss: 0.1238\n",
      "Epoch [24/500], Loss: 0.2515\n",
      "Epoch [24/500], Loss: 0.2131\n",
      "Epoch [24/500], Loss: 0.0592\n",
      "Epoch [24/500], Loss: 0.3937\n",
      "Epoch [25/500], Loss: 0.0776\n",
      "Epoch [25/500], Loss: 0.2422\n",
      "Epoch [25/500], Loss: 0.2317\n",
      "Epoch [25/500], Loss: 0.0900\n",
      "Epoch [25/500], Loss: 0.0773\n",
      "Epoch [25/500], Loss: 0.1549\n",
      "Epoch [25/500], Loss: 0.2995\n",
      "Epoch [25/500], Loss: 0.0157\n",
      "Epoch [26/500], Loss: 0.1265\n",
      "Epoch [26/500], Loss: 0.0135\n",
      "Epoch [26/500], Loss: 0.1540\n",
      "Epoch [26/500], Loss: 0.1413\n",
      "Epoch [26/500], Loss: 0.0694\n",
      "Epoch [26/500], Loss: 0.1543\n",
      "Epoch [26/500], Loss: 0.3547\n",
      "Epoch [26/500], Loss: 0.3439\n",
      "Epoch [27/500], Loss: 0.2921\n",
      "Epoch [27/500], Loss: 0.2328\n",
      "Epoch [27/500], Loss: 0.1324\n",
      "Epoch [27/500], Loss: 0.1815\n",
      "Epoch [27/500], Loss: 0.1189\n",
      "Epoch [27/500], Loss: 0.0843\n",
      "Epoch [27/500], Loss: 0.1933\n",
      "Epoch [27/500], Loss: 0.0364\n",
      "Epoch [28/500], Loss: 0.2762\n",
      "Epoch [28/500], Loss: 0.0182\n",
      "Epoch [28/500], Loss: 0.1627\n",
      "Epoch [28/500], Loss: 0.1509\n",
      "Epoch [28/500], Loss: 0.1451\n",
      "Epoch [28/500], Loss: 0.2317\n",
      "Epoch [28/500], Loss: 0.2763\n",
      "Epoch [28/500], Loss: 0.0181\n",
      "Epoch [29/500], Loss: 0.2498\n",
      "Epoch [29/500], Loss: 0.0440\n",
      "Epoch [29/500], Loss: 0.1858\n",
      "Epoch [29/500], Loss: 0.0807\n",
      "Epoch [29/500], Loss: 0.1602\n",
      "Epoch [29/500], Loss: 0.2409\n",
      "Epoch [29/500], Loss: 0.0953\n",
      "Epoch [29/500], Loss: 0.4163\n",
      "Epoch [30/500], Loss: 0.1097\n",
      "Epoch [30/500], Loss: 0.1271\n",
      "Epoch [30/500], Loss: 0.2581\n",
      "Epoch [30/500], Loss: 0.1384\n",
      "Epoch [30/500], Loss: 0.0649\n",
      "Epoch [30/500], Loss: 0.1333\n",
      "Epoch [30/500], Loss: 0.2335\n",
      "Epoch [30/500], Loss: 0.0518\n",
      "Epoch [31/500], Loss: 0.2437\n",
      "Epoch [31/500], Loss: 0.0410\n",
      "Epoch [31/500], Loss: 0.1378\n",
      "Epoch [31/500], Loss: 0.1059\n",
      "Epoch [31/500], Loss: 0.1366\n",
      "Epoch [31/500], Loss: 0.2423\n",
      "Epoch [31/500], Loss: 0.0364\n",
      "Epoch [31/500], Loss: 0.3728\n",
      "Epoch [32/500], Loss: 0.1399\n",
      "Epoch [32/500], Loss: 0.4708\n",
      "Epoch [32/500], Loss: 0.2511\n",
      "Epoch [32/500], Loss: 0.1411\n",
      "Epoch [32/500], Loss: 0.1210\n",
      "Epoch [32/500], Loss: 0.1044\n",
      "Epoch [32/500], Loss: 0.0563\n",
      "Epoch [32/500], Loss: 0.0295\n",
      "Epoch [33/500], Loss: 0.0188\n",
      "Epoch [33/500], Loss: 0.3957\n",
      "Epoch [33/500], Loss: 0.0091\n",
      "Epoch [33/500], Loss: 0.1482\n",
      "Epoch [33/500], Loss: 0.1425\n",
      "Epoch [33/500], Loss: 0.1654\n",
      "Epoch [33/500], Loss: 0.3232\n",
      "Epoch [33/500], Loss: 0.0117\n",
      "Epoch [34/500], Loss: 0.1629\n",
      "Epoch [34/500], Loss: 0.2739\n",
      "Epoch [34/500], Loss: 0.1179\n",
      "Epoch [34/500], Loss: 0.0358\n",
      "Epoch [34/500], Loss: 0.0306\n",
      "Epoch [34/500], Loss: 0.3144\n",
      "Epoch [34/500], Loss: 0.1303\n",
      "Epoch [34/500], Loss: 0.0440\n",
      "Epoch [35/500], Loss: 0.0593\n",
      "Epoch [35/500], Loss: 0.2056\n",
      "Epoch [35/500], Loss: 0.0594\n",
      "Epoch [35/500], Loss: 0.1395\n",
      "Epoch [35/500], Loss: 0.2010\n",
      "Epoch [35/500], Loss: 0.2356\n",
      "Epoch [35/500], Loss: 0.0453\n",
      "Epoch [35/500], Loss: 0.1715\n",
      "Epoch [36/500], Loss: 0.0909\n",
      "Epoch [36/500], Loss: 0.2790\n",
      "Epoch [36/500], Loss: 0.0765\n",
      "Epoch [36/500], Loss: 0.0425\n",
      "Epoch [36/500], Loss: 0.0654\n",
      "Epoch [36/500], Loss: 0.2151\n",
      "Epoch [36/500], Loss: 0.2850\n",
      "Epoch [36/500], Loss: 0.0155\n",
      "Epoch [37/500], Loss: 0.0879\n",
      "Epoch [37/500], Loss: 0.1310\n",
      "Epoch [37/500], Loss: 0.1865\n",
      "Epoch [37/500], Loss: 0.1720\n",
      "Epoch [37/500], Loss: 0.2041\n",
      "Epoch [37/500], Loss: 0.0261\n",
      "Epoch [37/500], Loss: 0.1470\n",
      "Epoch [37/500], Loss: 0.2392\n",
      "Epoch [38/500], Loss: 0.1428\n",
      "Epoch [38/500], Loss: 0.1054\n",
      "Epoch [38/500], Loss: 0.1534\n",
      "Epoch [38/500], Loss: 0.0525\n",
      "Epoch [38/500], Loss: 0.1984\n",
      "Epoch [38/500], Loss: 0.0320\n",
      "Epoch [38/500], Loss: 0.2991\n",
      "Epoch [38/500], Loss: 0.0193\n",
      "Epoch [39/500], Loss: 0.1367\n",
      "Epoch [39/500], Loss: 0.1502\n",
      "Epoch [39/500], Loss: 0.0162\n",
      "Epoch [39/500], Loss: 0.1471\n",
      "Epoch [39/500], Loss: 0.2688\n",
      "Epoch [39/500], Loss: 0.0344\n",
      "Epoch [39/500], Loss: 0.1419\n",
      "Epoch [39/500], Loss: 0.2260\n",
      "Epoch [40/500], Loss: 0.2335\n",
      "Epoch [40/500], Loss: 0.2469\n",
      "Epoch [40/500], Loss: 0.0941\n",
      "Epoch [40/500], Loss: 0.1463\n",
      "Epoch [40/500], Loss: 0.0849\n",
      "Epoch [40/500], Loss: 0.0689\n",
      "Epoch [40/500], Loss: 0.0480\n",
      "Epoch [40/500], Loss: 0.1062\n",
      "Epoch [41/500], Loss: 0.0282\n",
      "Epoch [41/500], Loss: 0.0162\n",
      "Epoch [41/500], Loss: 0.3281\n",
      "Epoch [41/500], Loss: 0.0373\n",
      "Epoch [41/500], Loss: 0.1848\n",
      "Epoch [41/500], Loss: 0.2396\n",
      "Epoch [41/500], Loss: 0.1544\n",
      "Epoch [41/500], Loss: 0.0186\n",
      "Epoch [42/500], Loss: 0.0282\n",
      "Epoch [42/500], Loss: 0.0153\n",
      "Epoch [42/500], Loss: 0.5927\n",
      "Epoch [42/500], Loss: 0.1556\n",
      "Epoch [42/500], Loss: 0.1288\n",
      "Epoch [42/500], Loss: 0.1842\n",
      "Epoch [42/500], Loss: 0.0922\n",
      "Epoch [42/500], Loss: 0.3707\n",
      "Epoch [43/500], Loss: 0.0376\n",
      "Epoch [43/500], Loss: 0.2463\n",
      "Epoch [43/500], Loss: 0.2504\n",
      "Epoch [43/500], Loss: 0.1543\n",
      "Epoch [43/500], Loss: 0.0285\n",
      "Epoch [43/500], Loss: 0.1471\n",
      "Epoch [43/500], Loss: 0.0271\n",
      "Epoch [43/500], Loss: 0.5137\n",
      "Epoch [44/500], Loss: 0.1392\n",
      "Epoch [44/500], Loss: 0.1445\n",
      "Epoch [44/500], Loss: 0.1008\n",
      "Epoch [44/500], Loss: 0.0481\n",
      "Epoch [44/500], Loss: 0.2326\n",
      "Epoch [44/500], Loss: 0.1422\n",
      "Epoch [44/500], Loss: 0.1542\n",
      "Epoch [44/500], Loss: 0.0462\n",
      "Epoch [45/500], Loss: 0.0511\n",
      "Epoch [45/500], Loss: 0.0387\n",
      "Epoch [45/500], Loss: 0.0408\n",
      "Epoch [45/500], Loss: 0.1494\n",
      "Epoch [45/500], Loss: 0.2317\n",
      "Epoch [45/500], Loss: 0.2718\n",
      "Epoch [45/500], Loss: 0.0878\n",
      "Epoch [45/500], Loss: 0.0244\n",
      "Epoch [46/500], Loss: 0.0312\n",
      "Epoch [46/500], Loss: 0.0992\n",
      "Epoch [46/500], Loss: 0.2878\n",
      "Epoch [46/500], Loss: 0.0540\n",
      "Epoch [46/500], Loss: 0.1963\n",
      "Epoch [46/500], Loss: 0.1281\n",
      "Epoch [46/500], Loss: 0.0608\n",
      "Epoch [46/500], Loss: 0.0551\n",
      "Epoch [47/500], Loss: 0.1025\n",
      "Epoch [47/500], Loss: 0.3539\n",
      "Epoch [47/500], Loss: 0.0175\n",
      "Epoch [47/500], Loss: 0.1411\n",
      "Epoch [47/500], Loss: 0.0238\n",
      "Epoch [47/500], Loss: 0.1541\n",
      "Epoch [47/500], Loss: 0.0333\n",
      "Epoch [47/500], Loss: 0.0462\n",
      "Epoch [48/500], Loss: 0.1952\n",
      "Epoch [48/500], Loss: 0.0263\n",
      "Epoch [48/500], Loss: 0.0293\n",
      "Epoch [48/500], Loss: 0.0600\n",
      "Epoch [48/500], Loss: 0.0274\n",
      "Epoch [48/500], Loss: 0.4195\n",
      "Epoch [48/500], Loss: 0.0186\n",
      "Epoch [48/500], Loss: 0.0185\n",
      "Epoch [49/500], Loss: 0.0329\n",
      "Epoch [49/500], Loss: 0.1483\n",
      "Epoch [49/500], Loss: 0.0238\n",
      "Epoch [49/500], Loss: 0.1200\n",
      "Epoch [49/500], Loss: 0.1517\n",
      "Epoch [49/500], Loss: 0.1624\n",
      "Epoch [49/500], Loss: 0.0977\n",
      "Epoch [49/500], Loss: 0.3752\n",
      "Epoch [50/500], Loss: 0.2536\n",
      "Epoch [50/500], Loss: 0.1599\n",
      "Epoch [50/500], Loss: 0.1058\n",
      "Epoch [50/500], Loss: 0.0564\n",
      "Epoch [50/500], Loss: 0.1886\n",
      "Epoch [50/500], Loss: 0.0569\n",
      "Epoch [50/500], Loss: 0.0479\n",
      "Epoch [50/500], Loss: 0.0432\n",
      "Epoch [51/500], Loss: 0.1860\n",
      "Epoch [51/500], Loss: 0.0311\n",
      "Epoch [51/500], Loss: 0.1436\n",
      "Epoch [51/500], Loss: 0.0177\n",
      "Epoch [51/500], Loss: 0.1440\n",
      "Epoch [51/500], Loss: 0.1472\n",
      "Epoch [51/500], Loss: 0.1446\n",
      "Epoch [51/500], Loss: 0.0128\n",
      "Epoch [52/500], Loss: 0.2859\n",
      "Epoch [52/500], Loss: 0.1421\n",
      "Epoch [52/500], Loss: 0.0135\n",
      "Epoch [52/500], Loss: 0.1451\n",
      "Epoch [52/500], Loss: 0.0167\n",
      "Epoch [52/500], Loss: 0.0235\n",
      "Epoch [52/500], Loss: 0.0181\n",
      "Epoch [52/500], Loss: 0.7861\n",
      "Epoch [53/500], Loss: 0.1407\n",
      "Epoch [53/500], Loss: 0.1866\n",
      "Epoch [53/500], Loss: 0.0504\n",
      "Epoch [53/500], Loss: 0.2339\n",
      "Epoch [53/500], Loss: 0.1097\n",
      "Epoch [53/500], Loss: 0.1127\n",
      "Epoch [53/500], Loss: 0.0733\n",
      "Epoch [53/500], Loss: 0.0568\n",
      "Epoch [54/500], Loss: 0.0501\n",
      "Epoch [54/500], Loss: 0.1056\n",
      "Epoch [54/500], Loss: 0.2569\n",
      "Epoch [54/500], Loss: 0.0213\n",
      "Epoch [54/500], Loss: 0.0145\n",
      "Epoch [54/500], Loss: 0.0208\n",
      "Epoch [54/500], Loss: 0.1567\n",
      "Epoch [54/500], Loss: 0.4547\n",
      "Epoch [55/500], Loss: 0.0062\n",
      "Epoch [55/500], Loss: 0.3130\n",
      "Epoch [55/500], Loss: 0.1424\n",
      "Epoch [55/500], Loss: 0.2050\n",
      "Epoch [55/500], Loss: 0.6570\n",
      "Epoch [55/500], Loss: 0.0348\n",
      "Epoch [55/500], Loss: 0.0059\n",
      "Epoch [55/500], Loss: 0.0060\n",
      "Epoch [56/500], Loss: 0.0039\n",
      "Epoch [56/500], Loss: 0.0918\n",
      "Epoch [56/500], Loss: 0.0168\n",
      "Epoch [56/500], Loss: 0.3238\n",
      "Epoch [56/500], Loss: 0.0162\n",
      "Epoch [56/500], Loss: 0.2398\n",
      "Epoch [56/500], Loss: 0.0198\n",
      "Epoch [56/500], Loss: 1.0824\n",
      "Epoch [57/500], Loss: 0.0234\n",
      "Epoch [57/500], Loss: 0.2034\n",
      "Epoch [57/500], Loss: 0.2637\n",
      "Epoch [57/500], Loss: 0.1540\n",
      "Epoch [57/500], Loss: 0.1675\n",
      "Epoch [57/500], Loss: 0.0532\n",
      "Epoch [57/500], Loss: 0.0382\n",
      "Epoch [57/500], Loss: 0.0421\n",
      "Epoch [58/500], Loss: 0.0468\n",
      "Epoch [58/500], Loss: 0.0258\n",
      "Epoch [58/500], Loss: 0.1693\n",
      "Epoch [58/500], Loss: 0.2510\n",
      "Epoch [58/500], Loss: 0.0227\n",
      "Epoch [58/500], Loss: 0.1382\n",
      "Epoch [58/500], Loss: 0.2623\n",
      "Epoch [58/500], Loss: 0.0298\n",
      "Epoch [59/500], Loss: 0.0627\n",
      "Epoch [59/500], Loss: 0.2249\n",
      "Epoch [59/500], Loss: 0.2420\n",
      "Epoch [59/500], Loss: 0.1378\n",
      "Epoch [59/500], Loss: 0.1411\n",
      "Epoch [59/500], Loss: 0.0453\n",
      "Epoch [59/500], Loss: 0.0461\n",
      "Epoch [59/500], Loss: 0.2109\n",
      "Epoch [60/500], Loss: 0.0430\n",
      "Epoch [60/500], Loss: 0.0381\n",
      "Epoch [60/500], Loss: 0.0261\n",
      "Epoch [60/500], Loss: 0.1398\n",
      "Epoch [60/500], Loss: 0.2516\n",
      "Epoch [60/500], Loss: 0.2421\n",
      "Epoch [60/500], Loss: 0.1787\n",
      "Epoch [60/500], Loss: 0.0460\n",
      "Epoch [61/500], Loss: 0.1597\n",
      "Epoch [61/500], Loss: 0.0699\n",
      "Epoch [61/500], Loss: 0.2744\n",
      "Epoch [61/500], Loss: 0.0503\n",
      "Epoch [61/500], Loss: 0.0296\n",
      "Epoch [61/500], Loss: 0.1512\n",
      "Epoch [61/500], Loss: 0.1482\n",
      "Epoch [61/500], Loss: 0.0176\n",
      "Epoch [62/500], Loss: 0.0764\n",
      "Epoch [62/500], Loss: 0.1799\n",
      "Epoch [62/500], Loss: 0.1546\n",
      "Epoch [62/500], Loss: 0.1492\n",
      "Epoch [62/500], Loss: 0.0623\n",
      "Epoch [62/500], Loss: 0.1579\n",
      "Epoch [62/500], Loss: 0.0114\n",
      "Epoch [62/500], Loss: 0.4425\n",
      "Epoch [63/500], Loss: 0.2483\n",
      "Epoch [63/500], Loss: 0.0821\n",
      "Epoch [63/500], Loss: 0.0462\n",
      "Epoch [63/500], Loss: 0.0383\n",
      "Epoch [63/500], Loss: 0.0463\n",
      "Epoch [63/500], Loss: 0.3595\n",
      "Epoch [63/500], Loss: 0.0478\n",
      "Epoch [63/500], Loss: 0.0472\n",
      "Epoch [64/500], Loss: 0.1695\n",
      "Epoch [64/500], Loss: 0.0561\n",
      "Epoch [64/500], Loss: 0.0365\n",
      "Epoch [64/500], Loss: 0.2009\n",
      "Epoch [64/500], Loss: 0.1486\n",
      "Epoch [64/500], Loss: 0.0227\n",
      "Epoch [64/500], Loss: 0.1606\n",
      "Epoch [64/500], Loss: 0.2151\n",
      "Epoch [65/500], Loss: 0.0311\n",
      "Epoch [65/500], Loss: 0.0265\n",
      "Epoch [65/500], Loss: 0.2067\n",
      "Epoch [65/500], Loss: 0.1750\n",
      "Epoch [65/500], Loss: 0.0474\n",
      "Epoch [65/500], Loss: 0.2972\n",
      "Epoch [65/500], Loss: 0.0618\n",
      "Epoch [65/500], Loss: 0.0171\n",
      "Epoch [66/500], Loss: 0.2940\n",
      "Epoch [66/500], Loss: 0.0762\n",
      "Epoch [66/500], Loss: 0.0144\n",
      "Epoch [66/500], Loss: 0.0128\n",
      "Epoch [66/500], Loss: 0.0103\n",
      "Epoch [66/500], Loss: 0.2827\n",
      "Epoch [66/500], Loss: 0.1012\n",
      "Epoch [66/500], Loss: 0.0327\n",
      "Epoch [67/500], Loss: 0.1674\n",
      "Epoch [67/500], Loss: 0.2991\n",
      "Epoch [67/500], Loss: 0.0243\n",
      "Epoch [67/500], Loss: 0.0208\n",
      "Epoch [67/500], Loss: 0.0631\n",
      "Epoch [67/500], Loss: 0.1287\n",
      "Epoch [67/500], Loss: 0.0651\n",
      "Epoch [67/500], Loss: 0.0258\n",
      "Epoch [68/500], Loss: 0.1532\n",
      "Epoch [68/500], Loss: 0.1230\n",
      "Epoch [68/500], Loss: 0.0481\n",
      "Epoch [68/500], Loss: 0.0227\n",
      "Epoch [68/500], Loss: 0.1896\n",
      "Epoch [68/500], Loss: 0.0470\n",
      "Epoch [68/500], Loss: 0.1449\n",
      "Epoch [68/500], Loss: 0.0994\n",
      "Epoch [69/500], Loss: 0.0501\n",
      "Epoch [69/500], Loss: 0.1598\n",
      "Epoch [69/500], Loss: 0.0101\n",
      "Epoch [69/500], Loss: 0.4403\n",
      "Epoch [69/500], Loss: 0.0102\n",
      "Epoch [69/500], Loss: 0.0104\n",
      "Epoch [69/500], Loss: 0.1598\n",
      "Epoch [69/500], Loss: 0.0100\n",
      "Epoch [70/500], Loss: 0.0129\n",
      "Epoch [70/500], Loss: 0.1468\n",
      "Epoch [70/500], Loss: 0.0844\n",
      "Epoch [70/500], Loss: 0.1419\n",
      "Epoch [70/500], Loss: 0.1429\n",
      "Epoch [70/500], Loss: 0.0174\n",
      "Epoch [70/500], Loss: 0.2356\n",
      "Epoch [70/500], Loss: 0.3900\n",
      "Epoch [71/500], Loss: 0.1415\n",
      "Epoch [71/500], Loss: 0.0375\n",
      "Epoch [71/500], Loss: 0.3061\n",
      "Epoch [71/500], Loss: 0.1203\n",
      "Epoch [71/500], Loss: 0.0626\n",
      "Epoch [71/500], Loss: 0.0676\n",
      "Epoch [71/500], Loss: 0.0740\n",
      "Epoch [71/500], Loss: 0.3086\n",
      "Epoch [72/500], Loss: 0.0621\n",
      "Epoch [72/500], Loss: 0.0696\n",
      "Epoch [72/500], Loss: 0.0595\n",
      "Epoch [72/500], Loss: 0.2367\n",
      "Epoch [72/500], Loss: 0.0801\n",
      "Epoch [72/500], Loss: 0.2603\n",
      "Epoch [72/500], Loss: 0.0274\n",
      "Epoch [72/500], Loss: 0.0751\n",
      "Epoch [73/500], Loss: 0.0201\n",
      "Epoch [73/500], Loss: 0.1451\n",
      "Epoch [73/500], Loss: 0.2147\n",
      "Epoch [73/500], Loss: 0.1877\n",
      "Epoch [73/500], Loss: 0.1354\n",
      "Epoch [73/500], Loss: 0.0127\n",
      "Epoch [73/500], Loss: 0.1486\n",
      "Epoch [73/500], Loss: 0.0346\n",
      "Epoch [74/500], Loss: 0.0997\n",
      "Epoch [74/500], Loss: 0.1466\n",
      "Epoch [74/500], Loss: 0.1456\n",
      "Epoch [74/500], Loss: 0.1432\n",
      "Epoch [74/500], Loss: 0.1038\n",
      "Epoch [74/500], Loss: 0.0212\n",
      "Epoch [74/500], Loss: 0.1273\n",
      "Epoch [74/500], Loss: 0.0186\n",
      "Epoch [75/500], Loss: 0.0344\n",
      "Epoch [75/500], Loss: 0.0700\n",
      "Epoch [75/500], Loss: 0.0311\n",
      "Epoch [75/500], Loss: 0.0696\n",
      "Epoch [75/500], Loss: 0.2785\n",
      "Epoch [75/500], Loss: 0.0321\n",
      "Epoch [75/500], Loss: 0.2642\n",
      "Epoch [75/500], Loss: 0.0100\n",
      "Epoch [76/500], Loss: 0.0983\n",
      "Epoch [76/500], Loss: 0.0084\n",
      "Epoch [76/500], Loss: 0.1446\n",
      "Epoch [76/500], Loss: 0.0145\n",
      "Epoch [76/500], Loss: 0.1584\n",
      "Epoch [76/500], Loss: 0.1482\n",
      "Epoch [76/500], Loss: 0.1496\n",
      "Epoch [76/500], Loss: 0.0756\n",
      "Epoch [77/500], Loss: 0.0182\n",
      "Epoch [77/500], Loss: 0.1761\n",
      "Epoch [77/500], Loss: 0.1318\n",
      "Epoch [77/500], Loss: 0.1301\n",
      "Epoch [77/500], Loss: 0.1314\n",
      "Epoch [77/500], Loss: 0.0374\n",
      "Epoch [77/500], Loss: 0.1425\n",
      "Epoch [77/500], Loss: 0.0410\n",
      "Epoch [78/500], Loss: 0.2450\n",
      "Epoch [78/500], Loss: 0.0202\n",
      "Epoch [78/500], Loss: 0.0521\n",
      "Epoch [78/500], Loss: 0.0195\n",
      "Epoch [78/500], Loss: 0.1471\n",
      "Epoch [78/500], Loss: 0.0137\n",
      "Epoch [78/500], Loss: 0.1965\n",
      "Epoch [78/500], Loss: 0.2909\n",
      "Epoch [79/500], Loss: 0.0179\n",
      "Epoch [79/500], Loss: 0.0177\n",
      "Epoch [79/500], Loss: 0.0409\n",
      "Epoch [79/500], Loss: 0.1470\n",
      "Epoch [79/500], Loss: 0.1720\n",
      "Epoch [79/500], Loss: 0.1562\n",
      "Epoch [79/500], Loss: 0.2230\n",
      "Epoch [79/500], Loss: 0.0128\n",
      "Epoch [80/500], Loss: 0.1556\n",
      "Epoch [80/500], Loss: 0.1043\n",
      "Epoch [80/500], Loss: 0.2825\n",
      "Epoch [80/500], Loss: 0.0160\n",
      "Epoch [80/500], Loss: 0.0123\n",
      "Epoch [80/500], Loss: 0.0990\n",
      "Epoch [80/500], Loss: 0.1282\n",
      "Epoch [80/500], Loss: 0.0126\n",
      "Epoch [81/500], Loss: 0.1265\n",
      "Epoch [81/500], Loss: 0.1140\n",
      "Epoch [81/500], Loss: 0.1438\n",
      "Epoch [81/500], Loss: 0.1431\n",
      "Epoch [81/500], Loss: 0.0213\n",
      "Epoch [81/500], Loss: 0.1324\n",
      "Epoch [81/500], Loss: 0.1326\n",
      "Epoch [81/500], Loss: 0.0249\n",
      "Epoch [82/500], Loss: 0.0954\n",
      "Epoch [82/500], Loss: 0.0335\n",
      "Epoch [82/500], Loss: 0.3797\n",
      "Epoch [82/500], Loss: 0.0290\n",
      "Epoch [82/500], Loss: 0.0299\n",
      "Epoch [82/500], Loss: 0.1482\n",
      "Epoch [82/500], Loss: 0.0409\n",
      "Epoch [82/500], Loss: 0.0300\n",
      "Epoch [83/500], Loss: 0.1446\n",
      "Epoch [83/500], Loss: 0.1739\n",
      "Epoch [83/500], Loss: 0.0307\n",
      "Epoch [83/500], Loss: 0.1582\n",
      "Epoch [83/500], Loss: 0.0232\n",
      "Epoch [83/500], Loss: 0.0889\n",
      "Epoch [83/500], Loss: 0.1350\n",
      "Epoch [83/500], Loss: 0.0154\n",
      "Epoch [84/500], Loss: 0.0116\n",
      "Epoch [84/500], Loss: 0.0092\n",
      "Epoch [84/500], Loss: 0.0615\n",
      "Epoch [84/500], Loss: 0.0086\n",
      "Epoch [84/500], Loss: 0.2982\n",
      "Epoch [84/500], Loss: 0.4374\n",
      "Epoch [84/500], Loss: 0.0209\n",
      "Epoch [84/500], Loss: 0.0257\n",
      "Epoch [85/500], Loss: 0.0352\n",
      "Epoch [85/500], Loss: 0.2578\n",
      "Epoch [85/500], Loss: 0.1488\n",
      "Epoch [85/500], Loss: 0.0637\n",
      "Epoch [85/500], Loss: 0.0467\n",
      "Epoch [85/500], Loss: 0.1410\n",
      "Epoch [85/500], Loss: 0.0731\n",
      "Epoch [85/500], Loss: 0.0268\n",
      "Epoch [86/500], Loss: 0.0603\n",
      "Epoch [86/500], Loss: 0.0272\n",
      "Epoch [86/500], Loss: 0.1166\n",
      "Epoch [86/500], Loss: 0.0167\n",
      "Epoch [86/500], Loss: 0.2849\n",
      "Epoch [86/500], Loss: 0.0137\n",
      "Epoch [86/500], Loss: 0.1609\n",
      "Epoch [86/500], Loss: 0.0100\n",
      "Epoch [87/500], Loss: 0.0110\n",
      "Epoch [87/500], Loss: 0.0112\n",
      "Epoch [87/500], Loss: 0.2249\n",
      "Epoch [87/500], Loss: 0.1668\n",
      "Epoch [87/500], Loss: 0.0339\n",
      "Epoch [87/500], Loss: 0.0146\n",
      "Epoch [87/500], Loss: 0.1732\n",
      "Epoch [87/500], Loss: 0.5029\n",
      "Epoch [88/500], Loss: 0.0423\n",
      "Epoch [88/500], Loss: 0.2180\n",
      "Epoch [88/500], Loss: 0.0308\n",
      "Epoch [88/500], Loss: 0.0438\n",
      "Epoch [88/500], Loss: 0.0199\n",
      "Epoch [88/500], Loss: 0.2606\n",
      "Epoch [88/500], Loss: 0.1378\n",
      "Epoch [88/500], Loss: 0.0236\n",
      "Epoch [89/500], Loss: 0.1381\n",
      "Epoch [89/500], Loss: 0.3409\n",
      "Epoch [89/500], Loss: 0.1333\n",
      "Epoch [89/500], Loss: 0.0342\n",
      "Epoch [89/500], Loss: 0.0319\n",
      "Epoch [89/500], Loss: 0.0380\n",
      "Epoch [89/500], Loss: 0.0317\n",
      "Epoch [89/500], Loss: 0.3457\n",
      "Epoch [90/500], Loss: 0.0281\n",
      "Epoch [90/500], Loss: 0.0356\n",
      "Epoch [90/500], Loss: 0.2461\n",
      "Epoch [90/500], Loss: 0.2107\n",
      "Epoch [90/500], Loss: 0.0306\n",
      "Epoch [90/500], Loss: 0.1785\n",
      "Epoch [90/500], Loss: 0.0390\n",
      "Epoch [90/500], Loss: 0.0435\n",
      "Epoch [91/500], Loss: 0.0380\n",
      "Epoch [91/500], Loss: 0.1285\n",
      "Epoch [91/500], Loss: 0.1879\n",
      "Epoch [91/500], Loss: 0.0415\n",
      "Epoch [91/500], Loss: 0.0190\n",
      "Epoch [91/500], Loss: 0.0263\n",
      "Epoch [91/500], Loss: 0.2869\n",
      "Epoch [91/500], Loss: 0.0208\n",
      "Epoch [92/500], Loss: 0.0112\n",
      "Epoch [92/500], Loss: 0.0418\n",
      "Epoch [92/500], Loss: 0.0097\n",
      "Epoch [92/500], Loss: 0.4420\n",
      "Epoch [92/500], Loss: 0.1573\n",
      "Epoch [92/500], Loss: 0.0106\n",
      "Epoch [92/500], Loss: 0.0113\n",
      "Epoch [92/500], Loss: 0.2153\n",
      "Epoch [93/500], Loss: 0.0320\n",
      "Epoch [93/500], Loss: 0.2062\n",
      "Epoch [93/500], Loss: 0.0538\n",
      "Epoch [93/500], Loss: 0.2803\n",
      "Epoch [93/500], Loss: 0.0996\n",
      "Epoch [93/500], Loss: 0.1273\n",
      "Epoch [93/500], Loss: 0.1628\n",
      "Epoch [93/500], Loss: 0.0080\n",
      "Epoch [94/500], Loss: 0.2805\n",
      "Epoch [94/500], Loss: 0.1588\n",
      "Epoch [94/500], Loss: 0.0111\n",
      "Epoch [94/500], Loss: 0.0160\n",
      "Epoch [94/500], Loss: 0.1482\n",
      "Epoch [94/500], Loss: 0.1462\n",
      "Epoch [94/500], Loss: 0.0419\n",
      "Epoch [94/500], Loss: 0.0145\n",
      "Epoch [95/500], Loss: 0.2534\n",
      "Epoch [95/500], Loss: 0.1414\n",
      "Epoch [95/500], Loss: 0.1393\n",
      "Epoch [95/500], Loss: 0.1352\n",
      "Epoch [95/500], Loss: 0.0278\n",
      "Epoch [95/500], Loss: 0.0375\n",
      "Epoch [95/500], Loss: 0.0906\n",
      "Epoch [95/500], Loss: 0.0262\n",
      "Epoch [96/500], Loss: 0.0283\n",
      "Epoch [96/500], Loss: 0.1276\n",
      "Epoch [96/500], Loss: 0.0223\n",
      "Epoch [96/500], Loss: 0.0171\n",
      "Epoch [96/500], Loss: 0.2746\n",
      "Epoch [96/500], Loss: 0.0193\n",
      "Epoch [96/500], Loss: 0.3174\n",
      "Epoch [96/500], Loss: 0.0308\n",
      "Epoch [97/500], Loss: 0.1362\n",
      "Epoch [97/500], Loss: 0.0324\n",
      "Epoch [97/500], Loss: 0.0352\n",
      "Epoch [97/500], Loss: 0.0570\n",
      "Epoch [97/500], Loss: 0.1634\n",
      "Epoch [97/500], Loss: 0.0489\n",
      "Epoch [97/500], Loss: 0.2717\n",
      "Epoch [97/500], Loss: 0.0697\n",
      "Epoch [98/500], Loss: 0.1422\n",
      "Epoch [98/500], Loss: 0.1515\n",
      "Epoch [98/500], Loss: 0.1301\n",
      "Epoch [98/500], Loss: 0.2171\n",
      "Epoch [98/500], Loss: 0.0223\n",
      "Epoch [98/500], Loss: 0.0151\n",
      "Epoch [98/500], Loss: 0.0760\n",
      "Epoch [98/500], Loss: 0.0175\n",
      "Epoch [99/500], Loss: 0.1444\n",
      "Epoch [99/500], Loss: 0.1000\n",
      "Epoch [99/500], Loss: 0.0195\n",
      "Epoch [99/500], Loss: 0.1440\n",
      "Epoch [99/500], Loss: 0.0795\n",
      "Epoch [99/500], Loss: 0.1428\n",
      "Epoch [99/500], Loss: 0.1219\n",
      "Epoch [99/500], Loss: 0.0230\n",
      "Epoch [100/500], Loss: 0.0414\n",
      "Epoch [100/500], Loss: 0.0231\n",
      "Epoch [100/500], Loss: 0.0173\n",
      "Epoch [100/500], Loss: 0.3854\n",
      "Epoch [100/500], Loss: 0.2535\n",
      "Epoch [100/500], Loss: 0.0190\n",
      "Epoch [100/500], Loss: 0.0243\n",
      "Epoch [100/500], Loss: 0.0155\n",
      "Epoch [101/500], Loss: 0.0323\n",
      "Epoch [101/500], Loss: 0.0279\n",
      "Epoch [101/500], Loss: 0.2872\n",
      "Epoch [101/500], Loss: 0.1589\n",
      "Epoch [101/500], Loss: 0.1366\n",
      "Epoch [101/500], Loss: 0.0236\n",
      "Epoch [101/500], Loss: 0.0549\n",
      "Epoch [101/500], Loss: 0.0463\n",
      "Epoch [102/500], Loss: 0.0225\n",
      "Epoch [102/500], Loss: 0.1831\n",
      "Epoch [102/500], Loss: 0.0181\n",
      "Epoch [102/500], Loss: 0.0142\n",
      "Epoch [102/500], Loss: 0.1242\n",
      "Epoch [102/500], Loss: 0.2946\n",
      "Epoch [102/500], Loss: 0.1208\n",
      "Epoch [102/500], Loss: 0.0119\n",
      "Epoch [103/500], Loss: 0.1423\n",
      "Epoch [103/500], Loss: 0.0168\n",
      "Epoch [103/500], Loss: 0.0950\n",
      "Epoch [103/500], Loss: 0.1276\n",
      "Epoch [103/500], Loss: 0.0430\n",
      "Epoch [103/500], Loss: 0.1425\n",
      "Epoch [103/500], Loss: 0.1559\n",
      "Epoch [103/500], Loss: 0.0313\n",
      "Epoch [104/500], Loss: 0.0441\n",
      "Epoch [104/500], Loss: 0.0417\n",
      "Epoch [104/500], Loss: 0.2418\n",
      "Epoch [104/500], Loss: 0.0279\n",
      "Epoch [104/500], Loss: 0.0307\n",
      "Epoch [104/500], Loss: 0.0168\n",
      "Epoch [104/500], Loss: 0.2888\n",
      "Epoch [104/500], Loss: 0.0180\n",
      "Epoch [105/500], Loss: 0.0256\n",
      "Epoch [105/500], Loss: 0.2777\n",
      "Epoch [105/500], Loss: 0.0963\n",
      "Epoch [105/500], Loss: 0.0186\n",
      "Epoch [105/500], Loss: 0.0213\n",
      "Epoch [105/500], Loss: 0.2826\n",
      "Epoch [105/500], Loss: 0.0165\n",
      "Epoch [105/500], Loss: 0.0149\n",
      "Epoch [106/500], Loss: 0.1481\n",
      "Epoch [106/500], Loss: 0.0307\n",
      "Epoch [106/500], Loss: 0.0201\n",
      "Epoch [106/500], Loss: 0.0160\n",
      "Epoch [106/500], Loss: 0.1477\n",
      "Epoch [106/500], Loss: 0.2795\n",
      "Epoch [106/500], Loss: 0.0185\n",
      "Epoch [106/500], Loss: 0.1754\n",
      "Epoch [107/500], Loss: 0.1733\n",
      "Epoch [107/500], Loss: 0.1535\n",
      "Epoch [107/500], Loss: 0.0616\n",
      "Epoch [107/500], Loss: 0.1211\n",
      "Epoch [107/500], Loss: 0.2369\n",
      "Epoch [107/500], Loss: 0.0260\n",
      "Epoch [107/500], Loss: 0.0420\n",
      "Epoch [107/500], Loss: 0.0138\n",
      "Epoch [108/500], Loss: 0.2188\n",
      "Epoch [108/500], Loss: 0.0108\n",
      "Epoch [108/500], Loss: 0.0121\n",
      "Epoch [108/500], Loss: 0.0078\n",
      "Epoch [108/500], Loss: 0.1353\n",
      "Epoch [108/500], Loss: 0.1631\n",
      "Epoch [108/500], Loss: 0.3084\n",
      "Epoch [108/500], Loss: 0.0085\n",
      "Epoch [109/500], Loss: 0.1627\n",
      "Epoch [109/500], Loss: 0.1515\n",
      "Epoch [109/500], Loss: 0.0125\n",
      "Epoch [109/500], Loss: 0.1393\n",
      "Epoch [109/500], Loss: 0.0927\n",
      "Epoch [109/500], Loss: 0.2587\n",
      "Epoch [109/500], Loss: 0.0204\n",
      "Epoch [109/500], Loss: 0.0024\n",
      "Epoch [110/500], Loss: 0.0001\n",
      "Epoch [110/500], Loss: 0.3663\n",
      "Epoch [110/500], Loss: 0.0002\n",
      "Epoch [110/500], Loss: 0.2089\n",
      "Epoch [110/500], Loss: 0.1812\n",
      "Epoch [110/500], Loss: 0.1687\n",
      "Epoch [110/500], Loss: 0.2108\n",
      "Epoch [110/500], Loss: 0.1348\n",
      "Epoch [111/500], Loss: 0.0509\n",
      "Epoch [111/500], Loss: 0.0425\n",
      "Epoch [111/500], Loss: 0.1418\n",
      "Epoch [111/500], Loss: 0.1397\n",
      "Epoch [111/500], Loss: 0.2459\n",
      "Epoch [111/500], Loss: 0.1345\n",
      "Epoch [111/500], Loss: 0.1202\n",
      "Epoch [111/500], Loss: 0.0279\n",
      "Epoch [112/500], Loss: 0.0935\n",
      "Epoch [112/500], Loss: 0.2552\n",
      "Epoch [112/500], Loss: 0.0152\n",
      "Epoch [112/500], Loss: 0.1417\n",
      "Epoch [112/500], Loss: 0.1466\n",
      "Epoch [112/500], Loss: 0.0183\n",
      "Epoch [112/500], Loss: 0.1308\n",
      "Epoch [112/500], Loss: 0.0163\n",
      "Epoch [113/500], Loss: 0.1344\n",
      "Epoch [113/500], Loss: 0.0167\n",
      "Epoch [113/500], Loss: 0.0544\n",
      "Epoch [113/500], Loss: 0.2634\n",
      "Epoch [113/500], Loss: 0.0346\n",
      "Epoch [113/500], Loss: 0.1363\n",
      "Epoch [113/500], Loss: 0.0309\n",
      "Epoch [113/500], Loss: 0.0354\n",
      "Epoch [114/500], Loss: 0.2794\n",
      "Epoch [114/500], Loss: 0.0254\n",
      "Epoch [114/500], Loss: 0.0346\n",
      "Epoch [114/500], Loss: 0.0373\n",
      "Epoch [114/500], Loss: 0.2722\n",
      "Epoch [114/500], Loss: 0.0192\n",
      "Epoch [114/500], Loss: 0.0679\n",
      "Epoch [114/500], Loss: 0.0159\n",
      "Epoch [115/500], Loss: 0.0119\n",
      "Epoch [115/500], Loss: 0.1469\n",
      "Epoch [115/500], Loss: 0.1309\n",
      "Epoch [115/500], Loss: 0.1806\n",
      "Epoch [115/500], Loss: 0.1447\n",
      "Epoch [115/500], Loss: 0.0180\n",
      "Epoch [115/500], Loss: 0.0147\n",
      "Epoch [115/500], Loss: 0.0114\n",
      "Epoch [116/500], Loss: 0.0222\n",
      "Epoch [116/500], Loss: 0.2706\n",
      "Epoch [116/500], Loss: 0.0262\n",
      "Epoch [116/500], Loss: 0.0322\n",
      "Epoch [116/500], Loss: 0.1505\n",
      "Epoch [116/500], Loss: 0.0188\n",
      "Epoch [116/500], Loss: 0.1860\n",
      "Epoch [116/500], Loss: 0.0274\n",
      "Epoch [117/500], Loss: 0.2724\n",
      "Epoch [117/500], Loss: 0.0209\n",
      "Epoch [117/500], Loss: 0.0244\n",
      "Epoch [117/500], Loss: 0.1551\n",
      "Epoch [117/500], Loss: 0.0245\n",
      "Epoch [117/500], Loss: 0.0811\n",
      "Epoch [117/500], Loss: 0.1464\n",
      "Epoch [117/500], Loss: 0.0165\n",
      "Epoch [118/500], Loss: 0.0208\n",
      "Epoch [118/500], Loss: 0.0185\n",
      "Epoch [118/500], Loss: 0.0125\n",
      "Epoch [118/500], Loss: 0.3993\n",
      "Epoch [118/500], Loss: 0.0151\n",
      "Epoch [118/500], Loss: 0.1449\n",
      "Epoch [118/500], Loss: 0.0500\n",
      "Epoch [118/500], Loss: 0.0903\n",
      "Epoch [119/500], Loss: 0.0177\n",
      "Epoch [119/500], Loss: 0.2531\n",
      "Epoch [119/500], Loss: 0.1437\n",
      "Epoch [119/500], Loss: 0.0398\n",
      "Epoch [119/500], Loss: 0.1565\n",
      "Epoch [119/500], Loss: 0.0288\n",
      "Epoch [119/500], Loss: 0.1695\n",
      "Epoch [119/500], Loss: 0.0161\n",
      "Epoch [120/500], Loss: 0.1448\n",
      "Epoch [120/500], Loss: 0.0463\n",
      "Epoch [120/500], Loss: 0.1372\n",
      "Epoch [120/500], Loss: 0.0345\n",
      "Epoch [120/500], Loss: 0.1456\n",
      "Epoch [120/500], Loss: 0.1415\n",
      "Epoch [120/500], Loss: 0.0183\n",
      "Epoch [120/500], Loss: 0.0136\n",
      "Epoch [121/500], Loss: 0.0172\n",
      "Epoch [121/500], Loss: 0.1446\n",
      "Epoch [121/500], Loss: 0.1848\n",
      "Epoch [121/500], Loss: 0.0151\n",
      "Epoch [121/500], Loss: 0.1365\n",
      "Epoch [121/500], Loss: 0.1601\n",
      "Epoch [121/500], Loss: 0.0161\n",
      "Epoch [121/500], Loss: 0.0268\n",
      "Epoch [122/500], Loss: 0.1519\n",
      "Epoch [122/500], Loss: 0.0509\n",
      "Epoch [122/500], Loss: 0.0285\n",
      "Epoch [122/500], Loss: 0.0220\n",
      "Epoch [122/500], Loss: 0.2797\n",
      "Epoch [122/500], Loss: 0.1471\n",
      "Epoch [122/500], Loss: 0.0236\n",
      "Epoch [122/500], Loss: 0.0228\n",
      "Epoch [123/500], Loss: 0.0181\n",
      "Epoch [123/500], Loss: 0.0486\n",
      "Epoch [123/500], Loss: 0.0128\n",
      "Epoch [123/500], Loss: 0.0111\n",
      "Epoch [123/500], Loss: 0.6107\n",
      "Epoch [123/500], Loss: 0.0153\n",
      "Epoch [123/500], Loss: 0.0358\n",
      "Epoch [123/500], Loss: 0.0240\n",
      "Epoch [124/500], Loss: 0.1226\n",
      "Epoch [124/500], Loss: 0.0275\n",
      "Epoch [124/500], Loss: 0.2571\n",
      "Epoch [124/500], Loss: 0.1317\n",
      "Epoch [124/500], Loss: 0.0329\n",
      "Epoch [124/500], Loss: 0.0578\n",
      "Epoch [124/500], Loss: 0.0393\n",
      "Epoch [124/500], Loss: 0.0298\n",
      "Epoch [125/500], Loss: 0.1651\n",
      "Epoch [125/500], Loss: 0.0250\n",
      "Epoch [125/500], Loss: 0.1463\n",
      "Epoch [125/500], Loss: 0.0216\n",
      "Epoch [125/500], Loss: 0.1404\n",
      "Epoch [125/500], Loss: 0.1433\n",
      "Epoch [125/500], Loss: 0.0143\n",
      "Epoch [125/500], Loss: 0.0151\n",
      "Epoch [126/500], Loss: 0.2768\n",
      "Epoch [126/500], Loss: 0.1518\n",
      "Epoch [126/500], Loss: 0.0196\n",
      "Epoch [126/500], Loss: 0.0277\n",
      "Epoch [126/500], Loss: 0.0152\n",
      "Epoch [126/500], Loss: 0.1362\n",
      "Epoch [126/500], Loss: 0.0149\n",
      "Epoch [126/500], Loss: 0.0224\n",
      "Epoch [127/500], Loss: 0.0214\n",
      "Epoch [127/500], Loss: 0.0114\n",
      "Epoch [127/500], Loss: 0.3056\n",
      "Epoch [127/500], Loss: 0.0151\n",
      "Epoch [127/500], Loss: 0.0117\n",
      "Epoch [127/500], Loss: 0.1974\n",
      "Epoch [127/500], Loss: 0.1566\n",
      "Epoch [127/500], Loss: 0.0125\n",
      "Epoch [128/500], Loss: 0.0197\n",
      "Epoch [128/500], Loss: 0.1546\n",
      "Epoch [128/500], Loss: 0.1285\n",
      "Epoch [128/500], Loss: 0.0414\n",
      "Epoch [128/500], Loss: 0.2258\n",
      "Epoch [128/500], Loss: 0.0314\n",
      "Epoch [128/500], Loss: 0.0276\n",
      "Epoch [128/500], Loss: 0.0165\n",
      "Epoch [129/500], Loss: 0.0181\n",
      "Epoch [129/500], Loss: 0.0187\n",
      "Epoch [129/500], Loss: 0.2263\n",
      "Epoch [129/500], Loss: 0.1437\n",
      "Epoch [129/500], Loss: 0.1542\n",
      "Epoch [129/500], Loss: 0.1482\n",
      "Epoch [129/500], Loss: 0.0167\n",
      "Epoch [129/500], Loss: 0.0296\n",
      "Epoch [130/500], Loss: 0.1376\n",
      "Epoch [130/500], Loss: 0.0260\n",
      "Epoch [130/500], Loss: 0.0183\n",
      "Epoch [130/500], Loss: 0.2574\n",
      "Epoch [130/500], Loss: 0.0671\n",
      "Epoch [130/500], Loss: 0.1327\n",
      "Epoch [130/500], Loss: 0.0340\n",
      "Epoch [130/500], Loss: 0.0250\n",
      "Epoch [131/500], Loss: 0.2878\n",
      "Epoch [131/500], Loss: 0.1323\n",
      "Epoch [131/500], Loss: 0.0404\n",
      "Epoch [131/500], Loss: 0.1582\n",
      "Epoch [131/500], Loss: 0.0198\n",
      "Epoch [131/500], Loss: 0.0664\n",
      "Epoch [131/500], Loss: 0.0155\n",
      "Epoch [131/500], Loss: 0.0163\n",
      "Epoch [132/500], Loss: 0.1396\n",
      "Epoch [132/500], Loss: 0.1444\n",
      "Epoch [132/500], Loss: 0.0101\n",
      "Epoch [132/500], Loss: 0.1477\n",
      "Epoch [132/500], Loss: 0.1131\n",
      "Epoch [132/500], Loss: 0.0094\n",
      "Epoch [132/500], Loss: 0.1751\n",
      "Epoch [132/500], Loss: 0.0116\n",
      "Epoch [133/500], Loss: 0.0099\n",
      "Epoch [133/500], Loss: 0.1837\n",
      "Epoch [133/500], Loss: 0.0111\n",
      "Epoch [133/500], Loss: 0.0131\n",
      "Epoch [133/500], Loss: 0.2777\n",
      "Epoch [133/500], Loss: 0.0252\n",
      "Epoch [133/500], Loss: 0.1195\n",
      "Epoch [133/500], Loss: 0.0209\n",
      "Epoch [134/500], Loss: 0.1465\n",
      "Epoch [134/500], Loss: 0.0459\n",
      "Epoch [134/500], Loss: 0.0630\n",
      "Epoch [134/500], Loss: 0.0131\n",
      "Epoch [134/500], Loss: 0.1449\n",
      "Epoch [134/500], Loss: 0.2367\n",
      "Epoch [134/500], Loss: 0.1502\n",
      "Epoch [134/500], Loss: 0.0133\n",
      "Epoch [135/500], Loss: 0.2615\n",
      "Epoch [135/500], Loss: 0.0212\n",
      "Epoch [135/500], Loss: 0.0246\n",
      "Epoch [135/500], Loss: 0.0262\n",
      "Epoch [135/500], Loss: 0.0516\n",
      "Epoch [135/500], Loss: 0.2058\n",
      "Epoch [135/500], Loss: 0.1342\n",
      "Epoch [135/500], Loss: 0.0251\n",
      "Epoch [136/500], Loss: 0.1875\n",
      "Epoch [136/500], Loss: 0.0278\n",
      "Epoch [136/500], Loss: 0.0327\n",
      "Epoch [136/500], Loss: 0.2745\n",
      "Epoch [136/500], Loss: 0.0332\n",
      "Epoch [136/500], Loss: 0.1471\n",
      "Epoch [136/500], Loss: 0.0213\n",
      "Epoch [136/500], Loss: 0.0407\n",
      "Epoch [137/500], Loss: 0.1439\n",
      "Epoch [137/500], Loss: 0.1665\n",
      "Epoch [137/500], Loss: 0.0193\n",
      "Epoch [137/500], Loss: 0.0205\n",
      "Epoch [137/500], Loss: 0.0239\n",
      "Epoch [137/500], Loss: 0.2812\n",
      "Epoch [137/500], Loss: 0.0116\n",
      "Epoch [137/500], Loss: 0.0237\n",
      "Epoch [138/500], Loss: 0.1508\n",
      "Epoch [138/500], Loss: 0.1395\n",
      "Epoch [138/500], Loss: 0.0104\n",
      "Epoch [138/500], Loss: 0.1538\n",
      "Epoch [138/500], Loss: 0.1131\n",
      "Epoch [138/500], Loss: 0.1569\n",
      "Epoch [138/500], Loss: 0.0172\n",
      "Epoch [138/500], Loss: 0.0182\n",
      "Epoch [139/500], Loss: 0.1499\n",
      "Epoch [139/500], Loss: 0.0371\n",
      "Epoch [139/500], Loss: 0.1578\n",
      "Epoch [139/500], Loss: 0.0164\n",
      "Epoch [139/500], Loss: 0.2674\n",
      "Epoch [139/500], Loss: 0.0203\n",
      "Epoch [139/500], Loss: 0.0174\n",
      "Epoch [139/500], Loss: 0.0251\n",
      "Epoch [140/500], Loss: 0.0156\n",
      "Epoch [140/500], Loss: 0.0237\n",
      "Epoch [140/500], Loss: 0.0152\n",
      "Epoch [140/500], Loss: 0.0492\n",
      "Epoch [140/500], Loss: 0.2689\n",
      "Epoch [140/500], Loss: 0.1494\n",
      "Epoch [140/500], Loss: 0.1542\n",
      "Epoch [140/500], Loss: 0.0155\n",
      "Epoch [141/500], Loss: 0.1390\n",
      "Epoch [141/500], Loss: 0.1164\n",
      "Epoch [141/500], Loss: 0.0160\n",
      "Epoch [141/500], Loss: 0.0189\n",
      "Epoch [141/500], Loss: 0.1607\n",
      "Epoch [141/500], Loss: 0.0204\n",
      "Epoch [141/500], Loss: 0.0318\n",
      "Epoch [141/500], Loss: 0.4350\n",
      "Epoch [142/500], Loss: 0.0177\n",
      "Epoch [142/500], Loss: 0.0157\n",
      "Epoch [142/500], Loss: 0.1449\n",
      "Epoch [142/500], Loss: 0.1435\n",
      "Epoch [142/500], Loss: 0.1539\n",
      "Epoch [142/500], Loss: 0.1524\n",
      "Epoch [142/500], Loss: 0.0219\n",
      "Epoch [142/500], Loss: 0.0246\n",
      "Epoch [143/500], Loss: 0.0255\n",
      "Epoch [143/500], Loss: 0.1477\n",
      "Epoch [143/500], Loss: 0.0229\n",
      "Epoch [143/500], Loss: 0.1405\n",
      "Epoch [143/500], Loss: 0.1426\n",
      "Epoch [143/500], Loss: 0.1397\n",
      "Epoch [143/500], Loss: 0.0519\n",
      "Epoch [143/500], Loss: 0.0223\n",
      "Epoch [144/500], Loss: 0.1475\n",
      "Epoch [144/500], Loss: 0.0186\n",
      "Epoch [144/500], Loss: 0.1457\n",
      "Epoch [144/500], Loss: 0.0242\n",
      "Epoch [144/500], Loss: 0.2566\n",
      "Epoch [144/500], Loss: 0.0175\n",
      "Epoch [144/500], Loss: 0.0154\n",
      "Epoch [144/500], Loss: 0.0135\n",
      "Epoch [145/500], Loss: 0.1617\n",
      "Epoch [145/500], Loss: 0.1409\n",
      "Epoch [145/500], Loss: 0.0206\n",
      "Epoch [145/500], Loss: 0.1334\n",
      "Epoch [145/500], Loss: 0.0088\n",
      "Epoch [145/500], Loss: 0.0095\n",
      "Epoch [145/500], Loss: 0.2570\n",
      "Epoch [145/500], Loss: 0.0140\n",
      "Epoch [146/500], Loss: 0.1387\n",
      "Epoch [146/500], Loss: 0.0284\n",
      "Epoch [146/500], Loss: 0.1325\n",
      "Epoch [146/500], Loss: 0.0462\n",
      "Epoch [146/500], Loss: 0.1489\n",
      "Epoch [146/500], Loss: 0.0310\n",
      "Epoch [146/500], Loss: 0.1446\n",
      "Epoch [146/500], Loss: 0.0248\n",
      "Epoch [147/500], Loss: 0.0224\n",
      "Epoch [147/500], Loss: 0.1423\n",
      "Epoch [147/500], Loss: 0.1287\n",
      "Epoch [147/500], Loss: 0.1481\n",
      "Epoch [147/500], Loss: 0.1440\n",
      "Epoch [147/500], Loss: 0.0194\n",
      "Epoch [147/500], Loss: 0.0616\n",
      "Epoch [147/500], Loss: 0.0181\n",
      "Epoch [148/500], Loss: 0.0171\n",
      "Epoch [148/500], Loss: 0.1447\n",
      "Epoch [148/500], Loss: 0.0214\n",
      "Epoch [148/500], Loss: 0.2374\n",
      "Epoch [148/500], Loss: 0.0418\n",
      "Epoch [148/500], Loss: 0.0295\n",
      "Epoch [148/500], Loss: 0.1553\n",
      "Epoch [148/500], Loss: 0.0185\n",
      "Epoch [149/500], Loss: 0.0372\n",
      "Epoch [149/500], Loss: 0.0258\n",
      "Epoch [149/500], Loss: 0.0118\n",
      "Epoch [149/500], Loss: 0.1620\n",
      "Epoch [149/500], Loss: 0.0076\n",
      "Epoch [149/500], Loss: 0.0756\n",
      "Epoch [149/500], Loss: 0.3216\n",
      "Epoch [149/500], Loss: 0.4630\n",
      "Epoch [150/500], Loss: 0.1520\n",
      "Epoch [150/500], Loss: 0.0133\n",
      "Epoch [150/500], Loss: 0.1442\n",
      "Epoch [150/500], Loss: 0.0498\n",
      "Epoch [150/500], Loss: 0.1232\n",
      "Epoch [150/500], Loss: 0.1414\n",
      "Epoch [150/500], Loss: 0.0468\n",
      "Epoch [150/500], Loss: 0.0235\n",
      "Epoch [151/500], Loss: 0.0418\n",
      "Epoch [151/500], Loss: 0.0451\n",
      "Epoch [151/500], Loss: 0.0321\n",
      "Epoch [151/500], Loss: 0.1343\n",
      "Epoch [151/500], Loss: 0.0287\n",
      "Epoch [151/500], Loss: 0.2693\n",
      "Epoch [151/500], Loss: 0.0221\n",
      "Epoch [151/500], Loss: 0.4102\n",
      "Epoch [152/500], Loss: 0.0197\n",
      "Epoch [152/500], Loss: 0.1407\n",
      "Epoch [152/500], Loss: 0.0418\n",
      "Epoch [152/500], Loss: 0.0217\n",
      "Epoch [152/500], Loss: 0.0213\n",
      "Epoch [152/500], Loss: 0.1405\n",
      "Epoch [152/500], Loss: 0.2661\n",
      "Epoch [152/500], Loss: 0.0176\n",
      "Epoch [153/500], Loss: 0.0192\n",
      "Epoch [153/500], Loss: 0.0177\n",
      "Epoch [153/500], Loss: 0.0322\n",
      "Epoch [153/500], Loss: 0.2676\n",
      "Epoch [153/500], Loss: 0.0416\n",
      "Epoch [153/500], Loss: 0.0182\n",
      "Epoch [153/500], Loss: 0.1473\n",
      "Epoch [153/500], Loss: 0.4208\n",
      "Epoch [154/500], Loss: 0.1713\n",
      "Epoch [154/500], Loss: 0.1465\n",
      "Epoch [154/500], Loss: 0.0369\n",
      "Epoch [154/500], Loss: 0.1485\n",
      "Epoch [154/500], Loss: 0.0355\n",
      "Epoch [154/500], Loss: 0.0490\n",
      "Epoch [154/500], Loss: 0.0324\n",
      "Epoch [154/500], Loss: 0.3717\n",
      "Epoch [155/500], Loss: 0.0263\n",
      "Epoch [155/500], Loss: 0.1435\n",
      "Epoch [155/500], Loss: 0.0310\n",
      "Epoch [155/500], Loss: 0.0239\n",
      "Epoch [155/500], Loss: 0.0243\n",
      "Epoch [155/500], Loss: 0.1405\n",
      "Epoch [155/500], Loss: 0.2541\n",
      "Epoch [155/500], Loss: 0.0200\n",
      "Epoch [156/500], Loss: 0.1376\n",
      "Epoch [156/500], Loss: 0.0291\n",
      "Epoch [156/500], Loss: 0.0198\n",
      "Epoch [156/500], Loss: 0.0221\n",
      "Epoch [156/500], Loss: 0.0146\n",
      "Epoch [156/500], Loss: 0.1426\n",
      "Epoch [156/500], Loss: 0.2732\n",
      "Epoch [156/500], Loss: 0.0171\n",
      "Epoch [157/500], Loss: 0.1446\n",
      "Epoch [157/500], Loss: 0.1530\n",
      "Epoch [157/500], Loss: 0.1379\n",
      "Epoch [157/500], Loss: 0.0168\n",
      "Epoch [157/500], Loss: 0.0239\n",
      "Epoch [157/500], Loss: 0.0180\n",
      "Epoch [157/500], Loss: 0.1434\n",
      "Epoch [157/500], Loss: 0.0163\n",
      "Epoch [158/500], Loss: 0.1413\n",
      "Epoch [158/500], Loss: 0.0189\n",
      "Epoch [158/500], Loss: 0.0554\n",
      "Epoch [158/500], Loss: 0.1349\n",
      "Epoch [158/500], Loss: 0.2641\n",
      "Epoch [158/500], Loss: 0.0228\n",
      "Epoch [158/500], Loss: 0.0230\n",
      "Epoch [158/500], Loss: 0.0187\n",
      "Epoch [159/500], Loss: 0.0248\n",
      "Epoch [159/500], Loss: 0.0174\n",
      "Epoch [159/500], Loss: 0.0126\n",
      "Epoch [159/500], Loss: 0.2733\n",
      "Epoch [159/500], Loss: 0.2774\n",
      "Epoch [159/500], Loss: 0.0178\n",
      "Epoch [159/500], Loss: 0.0172\n",
      "Epoch [159/500], Loss: 0.0197\n",
      "Epoch [160/500], Loss: 0.2649\n",
      "Epoch [160/500], Loss: 0.0188\n",
      "Epoch [160/500], Loss: 0.1359\n",
      "Epoch [160/500], Loss: 0.1480\n",
      "Epoch [160/500], Loss: 0.0218\n",
      "Epoch [160/500], Loss: 0.0228\n",
      "Epoch [160/500], Loss: 0.0263\n",
      "Epoch [160/500], Loss: 0.0158\n",
      "Epoch [161/500], Loss: 0.1391\n",
      "Epoch [161/500], Loss: 0.0196\n",
      "Epoch [161/500], Loss: 0.0287\n",
      "Epoch [161/500], Loss: 0.1107\n",
      "Epoch [161/500], Loss: 0.1372\n",
      "Epoch [161/500], Loss: 0.1489\n",
      "Epoch [161/500], Loss: 0.0188\n",
      "Epoch [161/500], Loss: 0.0184\n",
      "Epoch [162/500], Loss: 0.1528\n",
      "Epoch [162/500], Loss: 0.0559\n",
      "Epoch [162/500], Loss: 0.0167\n",
      "Epoch [162/500], Loss: 0.1512\n",
      "Epoch [162/500], Loss: 0.1481\n",
      "Epoch [162/500], Loss: 0.1447\n",
      "Epoch [162/500], Loss: 0.0097\n",
      "Epoch [162/500], Loss: 0.0277\n",
      "Epoch [163/500], Loss: 0.3172\n",
      "Epoch [163/500], Loss: 0.0142\n",
      "Epoch [163/500], Loss: 0.0098\n",
      "Epoch [163/500], Loss: 0.0157\n",
      "Epoch [163/500], Loss: 0.2926\n",
      "Epoch [163/500], Loss: 0.0156\n",
      "Epoch [163/500], Loss: 0.0202\n",
      "Epoch [163/500], Loss: 0.0202\n",
      "Epoch [164/500], Loss: 0.0216\n",
      "Epoch [164/500], Loss: 0.1301\n",
      "Epoch [164/500], Loss: 0.1451\n",
      "Epoch [164/500], Loss: 0.2472\n",
      "Epoch [164/500], Loss: 0.0229\n",
      "Epoch [164/500], Loss: 0.0306\n",
      "Epoch [164/500], Loss: 0.0183\n",
      "Epoch [164/500], Loss: 0.0138\n",
      "Epoch [165/500], Loss: 0.1413\n",
      "Epoch [165/500], Loss: 0.1083\n",
      "Epoch [165/500], Loss: 0.0226\n",
      "Epoch [165/500], Loss: 0.0213\n",
      "Epoch [165/500], Loss: 0.1458\n",
      "Epoch [165/500], Loss: 0.1465\n",
      "Epoch [165/500], Loss: 0.0179\n",
      "Epoch [165/500], Loss: 0.0278\n",
      "Epoch [166/500], Loss: 0.0150\n",
      "Epoch [166/500], Loss: 0.1543\n",
      "Epoch [166/500], Loss: 0.1465\n",
      "Epoch [166/500], Loss: 0.0098\n",
      "Epoch [166/500], Loss: 0.2995\n",
      "Epoch [166/500], Loss: 0.0312\n",
      "Epoch [166/500], Loss: 0.0125\n",
      "Epoch [166/500], Loss: 0.0123\n",
      "Epoch [167/500], Loss: 0.0106\n",
      "Epoch [167/500], Loss: 0.1484\n",
      "Epoch [167/500], Loss: 0.1500\n",
      "Epoch [167/500], Loss: 0.0224\n",
      "Epoch [167/500], Loss: 0.0109\n",
      "Epoch [167/500], Loss: 0.1441\n",
      "Epoch [167/500], Loss: 0.1407\n",
      "Epoch [167/500], Loss: 0.0100\n",
      "Epoch [168/500], Loss: 0.1079\n",
      "Epoch [168/500], Loss: 0.0226\n",
      "Epoch [168/500], Loss: 0.1749\n",
      "Epoch [168/500], Loss: 0.0105\n",
      "Epoch [168/500], Loss: 0.1552\n",
      "Epoch [168/500], Loss: 0.1125\n",
      "Epoch [168/500], Loss: 0.0142\n",
      "Epoch [168/500], Loss: 0.0133\n",
      "Epoch [169/500], Loss: 0.1322\n",
      "Epoch [169/500], Loss: 0.1607\n",
      "Epoch [169/500], Loss: 0.0157\n",
      "Epoch [169/500], Loss: 0.0372\n",
      "Epoch [169/500], Loss: 0.1178\n",
      "Epoch [169/500], Loss: 0.1194\n",
      "Epoch [169/500], Loss: 0.0235\n",
      "Epoch [169/500], Loss: 0.0206\n",
      "Epoch [170/500], Loss: 0.0210\n",
      "Epoch [170/500], Loss: 0.0132\n",
      "Epoch [170/500], Loss: 0.0259\n",
      "Epoch [170/500], Loss: 0.1303\n",
      "Epoch [170/500], Loss: 0.2388\n",
      "Epoch [170/500], Loss: 0.2415\n",
      "Epoch [170/500], Loss: 0.0222\n",
      "Epoch [170/500], Loss: 0.0242\n",
      "Epoch [171/500], Loss: 0.0445\n",
      "Epoch [171/500], Loss: 0.0284\n",
      "Epoch [171/500], Loss: 0.0673\n",
      "Epoch [171/500], Loss: 0.1296\n",
      "Epoch [171/500], Loss: 0.0214\n",
      "Epoch [171/500], Loss: 0.1359\n",
      "Epoch [171/500], Loss: 0.1322\n",
      "Epoch [171/500], Loss: 0.4289\n",
      "Epoch [172/500], Loss: 0.0136\n",
      "Epoch [172/500], Loss: 0.0155\n",
      "Epoch [172/500], Loss: 0.1313\n",
      "Epoch [172/500], Loss: 0.2624\n",
      "Epoch [172/500], Loss: 0.0182\n",
      "Epoch [172/500], Loss: 0.0368\n",
      "Epoch [172/500], Loss: 0.1368\n",
      "Epoch [172/500], Loss: 0.0233\n",
      "Epoch [173/500], Loss: 0.0267\n",
      "Epoch [173/500], Loss: 0.1285\n",
      "Epoch [173/500], Loss: 0.1345\n",
      "Epoch [173/500], Loss: 0.0141\n",
      "Epoch [173/500], Loss: 0.0278\n",
      "Epoch [173/500], Loss: 0.2474\n",
      "Epoch [173/500], Loss: 0.0129\n",
      "Epoch [173/500], Loss: 0.0205\n",
      "Epoch [174/500], Loss: 0.1163\n",
      "Epoch [174/500], Loss: 0.2299\n",
      "Epoch [174/500], Loss: 0.1120\n",
      "Epoch [174/500], Loss: 0.0209\n",
      "Epoch [174/500], Loss: 0.0445\n",
      "Epoch [174/500], Loss: 0.0378\n",
      "Epoch [174/500], Loss: 0.0198\n",
      "Epoch [174/500], Loss: 0.0282\n",
      "Epoch [175/500], Loss: 0.1335\n",
      "Epoch [175/500], Loss: 0.0240\n",
      "Epoch [175/500], Loss: 0.0111\n",
      "Epoch [175/500], Loss: 0.0116\n",
      "Epoch [175/500], Loss: 0.1651\n",
      "Epoch [175/500], Loss: 0.2085\n",
      "Epoch [175/500], Loss: 0.1083\n",
      "Epoch [175/500], Loss: 0.0124\n",
      "Epoch [176/500], Loss: 0.0206\n",
      "Epoch [176/500], Loss: 0.0207\n",
      "Epoch [176/500], Loss: 0.0271\n",
      "Epoch [176/500], Loss: 0.0148\n",
      "Epoch [176/500], Loss: 0.0166\n",
      "Epoch [176/500], Loss: 0.1411\n",
      "Epoch [176/500], Loss: 0.1391\n",
      "Epoch [176/500], Loss: 0.7592\n",
      "Epoch [177/500], Loss: 0.0205\n",
      "Epoch [177/500], Loss: 0.0287\n",
      "Epoch [177/500], Loss: 0.1416\n",
      "Epoch [177/500], Loss: 0.2578\n",
      "Epoch [177/500], Loss: 0.0510\n",
      "Epoch [177/500], Loss: 0.0666\n",
      "Epoch [177/500], Loss: 0.1331\n",
      "Epoch [177/500], Loss: 0.0463\n",
      "Epoch [178/500], Loss: 0.0774\n",
      "Epoch [178/500], Loss: 0.1305\n",
      "Epoch [178/500], Loss: 0.0273\n",
      "Epoch [178/500], Loss: 0.0250\n",
      "Epoch [178/500], Loss: 0.1354\n",
      "Epoch [178/500], Loss: 0.0152\n",
      "Epoch [178/500], Loss: 0.1647\n",
      "Epoch [178/500], Loss: 0.3889\n",
      "Epoch [179/500], Loss: 0.0141\n",
      "Epoch [179/500], Loss: 0.1427\n",
      "Epoch [179/500], Loss: 0.0116\n",
      "Epoch [179/500], Loss: 0.1450\n",
      "Epoch [179/500], Loss: 0.0835\n",
      "Epoch [179/500], Loss: 0.0198\n",
      "Epoch [179/500], Loss: 0.1427\n",
      "Epoch [179/500], Loss: 0.1166\n",
      "Epoch [180/500], Loss: 0.1491\n",
      "Epoch [180/500], Loss: 0.0201\n",
      "Epoch [180/500], Loss: 0.2622\n",
      "Epoch [180/500], Loss: 0.1476\n",
      "Epoch [180/500], Loss: 0.0158\n",
      "Epoch [180/500], Loss: 0.0613\n",
      "Epoch [180/500], Loss: 0.0197\n",
      "Epoch [180/500], Loss: 0.1160\n",
      "Epoch [181/500], Loss: 0.0225\n",
      "Epoch [181/500], Loss: 0.2473\n",
      "Epoch [181/500], Loss: 0.2243\n",
      "Epoch [181/500], Loss: 0.0405\n",
      "Epoch [181/500], Loss: 0.0294\n",
      "Epoch [181/500], Loss: 0.0988\n",
      "Epoch [181/500], Loss: 0.0415\n",
      "Epoch [181/500], Loss: 0.0488\n",
      "Epoch [182/500], Loss: 0.2191\n",
      "Epoch [182/500], Loss: 0.1473\n",
      "Epoch [182/500], Loss: 0.0305\n",
      "Epoch [182/500], Loss: 0.1002\n",
      "Epoch [182/500], Loss: 0.0313\n",
      "Epoch [182/500], Loss: 0.1356\n",
      "Epoch [182/500], Loss: 0.0169\n",
      "Epoch [182/500], Loss: 0.0671\n",
      "Epoch [183/500], Loss: 0.0177\n",
      "Epoch [183/500], Loss: 0.2538\n",
      "Epoch [183/500], Loss: 0.0143\n",
      "Epoch [183/500], Loss: 0.0148\n",
      "Epoch [183/500], Loss: 0.0184\n",
      "Epoch [183/500], Loss: 0.0371\n",
      "Epoch [183/500], Loss: 0.3248\n",
      "Epoch [183/500], Loss: 0.0057\n",
      "Epoch [184/500], Loss: 0.0124\n",
      "Epoch [184/500], Loss: 0.1675\n",
      "Epoch [184/500], Loss: 0.0060\n",
      "Epoch [184/500], Loss: 0.1024\n",
      "Epoch [184/500], Loss: 0.0095\n",
      "Epoch [184/500], Loss: 0.0064\n",
      "Epoch [184/500], Loss: 0.3425\n",
      "Epoch [184/500], Loss: 0.0051\n",
      "Epoch [185/500], Loss: 0.0283\n",
      "Epoch [185/500], Loss: 0.1394\n",
      "Epoch [185/500], Loss: 0.0240\n",
      "Epoch [185/500], Loss: 0.1370\n",
      "Epoch [185/500], Loss: 0.1462\n",
      "Epoch [185/500], Loss: 0.1009\n",
      "Epoch [185/500], Loss: 0.0223\n",
      "Epoch [185/500], Loss: 0.0146\n",
      "Epoch [186/500], Loss: 0.0188\n",
      "Epoch [186/500], Loss: 0.1385\n",
      "Epoch [186/500], Loss: 0.1219\n",
      "Epoch [186/500], Loss: 0.0181\n",
      "Epoch [186/500], Loss: 0.1545\n",
      "Epoch [186/500], Loss: 0.0320\n",
      "Epoch [186/500], Loss: 0.1352\n",
      "Epoch [186/500], Loss: 0.0158\n",
      "Epoch [187/500], Loss: 0.2586\n",
      "Epoch [187/500], Loss: 0.0615\n",
      "Epoch [187/500], Loss: 0.0130\n",
      "Epoch [187/500], Loss: 0.0116\n",
      "Epoch [187/500], Loss: 0.0095\n",
      "Epoch [187/500], Loss: 0.0457\n",
      "Epoch [187/500], Loss: 0.1949\n",
      "Epoch [187/500], Loss: 0.0591\n",
      "Epoch [188/500], Loss: 0.0277\n",
      "Epoch [188/500], Loss: 0.1357\n",
      "Epoch [188/500], Loss: 0.1199\n",
      "Epoch [188/500], Loss: 0.1002\n",
      "Epoch [188/500], Loss: 0.1335\n",
      "Epoch [188/500], Loss: 0.0643\n",
      "Epoch [188/500], Loss: 0.0377\n",
      "Epoch [188/500], Loss: 0.5355\n",
      "Epoch [189/500], Loss: 0.0141\n",
      "Epoch [189/500], Loss: 0.1206\n",
      "Epoch [189/500], Loss: 0.0591\n",
      "Epoch [189/500], Loss: 0.1308\n",
      "Epoch [189/500], Loss: 0.0406\n",
      "Epoch [189/500], Loss: 0.0114\n",
      "Epoch [189/500], Loss: 0.2806\n",
      "Epoch [189/500], Loss: 0.0150\n",
      "Epoch [190/500], Loss: 0.0268\n",
      "Epoch [190/500], Loss: 0.2373\n",
      "Epoch [190/500], Loss: 0.0563\n",
      "Epoch [190/500], Loss: 0.0803\n",
      "Epoch [190/500], Loss: 0.1186\n",
      "Epoch [190/500], Loss: 0.0503\n",
      "Epoch [190/500], Loss: 0.0251\n",
      "Epoch [190/500], Loss: 0.0277\n",
      "Epoch [191/500], Loss: 0.1156\n",
      "Epoch [191/500], Loss: 0.0249\n",
      "Epoch [191/500], Loss: 0.2675\n",
      "Epoch [191/500], Loss: 0.0286\n",
      "Epoch [191/500], Loss: 0.0127\n",
      "Epoch [191/500], Loss: 0.0111\n",
      "Epoch [191/500], Loss: 0.1597\n",
      "Epoch [191/500], Loss: 0.0270\n",
      "Epoch [192/500], Loss: 0.1670\n",
      "Epoch [192/500], Loss: 0.0202\n",
      "Epoch [192/500], Loss: 0.0560\n",
      "Epoch [192/500], Loss: 0.0404\n",
      "Epoch [192/500], Loss: 0.1170\n",
      "Epoch [192/500], Loss: 0.1394\n",
      "Epoch [192/500], Loss: 0.0162\n",
      "Epoch [192/500], Loss: 0.0291\n",
      "Epoch [193/500], Loss: 0.0210\n",
      "Epoch [193/500], Loss: 0.0982\n",
      "Epoch [193/500], Loss: 0.0165\n",
      "Epoch [193/500], Loss: 0.1439\n",
      "Epoch [193/500], Loss: 0.2619\n",
      "Epoch [193/500], Loss: 0.0116\n",
      "Epoch [193/500], Loss: 0.0118\n",
      "Epoch [193/500], Loss: 0.0071\n",
      "Epoch [194/500], Loss: 0.0124\n",
      "Epoch [194/500], Loss: 0.0707\n",
      "Epoch [194/500], Loss: 0.2909\n",
      "Epoch [194/500], Loss: 0.0407\n",
      "Epoch [194/500], Loss: 0.0090\n",
      "Epoch [194/500], Loss: 0.1366\n",
      "Epoch [194/500], Loss: 0.0144\n",
      "Epoch [194/500], Loss: 0.0148\n",
      "Epoch [195/500], Loss: 0.0088\n",
      "Epoch [195/500], Loss: 0.1470\n",
      "Epoch [195/500], Loss: 0.1097\n",
      "Epoch [195/500], Loss: 0.2353\n",
      "Epoch [195/500], Loss: 0.0187\n",
      "Epoch [195/500], Loss: 0.0167\n",
      "Epoch [195/500], Loss: 0.1109\n",
      "Epoch [195/500], Loss: 0.0177\n",
      "Epoch [196/500], Loss: 0.0273\n",
      "Epoch [196/500], Loss: 0.0227\n",
      "Epoch [196/500], Loss: 0.0213\n",
      "Epoch [196/500], Loss: 0.1224\n",
      "Epoch [196/500], Loss: 0.0154\n",
      "Epoch [196/500], Loss: 0.1438\n",
      "Epoch [196/500], Loss: 0.1326\n",
      "Epoch [196/500], Loss: 0.0173\n",
      "Epoch [197/500], Loss: 0.0244\n",
      "Epoch [197/500], Loss: 0.2032\n",
      "Epoch [197/500], Loss: 0.1592\n",
      "Epoch [197/500], Loss: 0.0898\n",
      "Epoch [197/500], Loss: 0.0136\n",
      "Epoch [197/500], Loss: 0.0139\n",
      "Epoch [197/500], Loss: 0.0104\n",
      "Epoch [197/500], Loss: 0.0073\n",
      "Epoch [198/500], Loss: 0.0120\n",
      "Epoch [198/500], Loss: 0.2507\n",
      "Epoch [198/500], Loss: 0.1260\n",
      "Epoch [198/500], Loss: 0.0099\n",
      "Epoch [198/500], Loss: 0.1002\n",
      "Epoch [198/500], Loss: 0.0202\n",
      "Epoch [198/500], Loss: 0.0135\n",
      "Epoch [198/500], Loss: 0.0064\n",
      "Epoch [199/500], Loss: 0.0202\n",
      "Epoch [199/500], Loss: 0.2124\n",
      "Epoch [199/500], Loss: 0.1140\n",
      "Epoch [199/500], Loss: 0.0135\n",
      "Epoch [199/500], Loss: 0.1522\n",
      "Epoch [199/500], Loss: 0.0142\n",
      "Epoch [199/500], Loss: 0.0329\n",
      "Epoch [199/500], Loss: 0.0098\n",
      "Epoch [200/500], Loss: 0.0137\n",
      "Epoch [200/500], Loss: 0.1920\n",
      "Epoch [200/500], Loss: 0.1588\n",
      "Epoch [200/500], Loss: 0.0171\n",
      "Epoch [200/500], Loss: 0.0256\n",
      "Epoch [200/500], Loss: 0.1395\n",
      "Epoch [200/500], Loss: 0.0169\n",
      "Epoch [200/500], Loss: 0.0260\n",
      "Epoch [201/500], Loss: 0.0125\n",
      "Epoch [201/500], Loss: 0.1362\n",
      "Epoch [201/500], Loss: 0.1303\n",
      "Epoch [201/500], Loss: 0.0121\n",
      "Epoch [201/500], Loss: 0.1206\n",
      "Epoch [201/500], Loss: 0.1176\n",
      "Epoch [201/500], Loss: 0.0196\n",
      "Epoch [201/500], Loss: 0.0090\n",
      "Epoch [202/500], Loss: 0.1393\n",
      "Epoch [202/500], Loss: 0.0101\n",
      "Epoch [202/500], Loss: 0.1498\n",
      "Epoch [202/500], Loss: 0.0117\n",
      "Epoch [202/500], Loss: 0.0134\n",
      "Epoch [202/500], Loss: 0.0865\n",
      "Epoch [202/500], Loss: 0.1127\n",
      "Epoch [202/500], Loss: 0.0035\n",
      "Epoch [203/500], Loss: 0.1104\n",
      "Epoch [203/500], Loss: 0.1926\n",
      "Epoch [203/500], Loss: 0.0233\n",
      "Epoch [203/500], Loss: 0.0324\n",
      "Epoch [203/500], Loss: 0.0291\n",
      "Epoch [203/500], Loss: 0.1097\n",
      "Epoch [203/500], Loss: 0.0195\n",
      "Epoch [203/500], Loss: 0.0216\n",
      "Epoch [204/500], Loss: 0.0125\n",
      "Epoch [204/500], Loss: 0.1601\n",
      "Epoch [204/500], Loss: 0.0091\n",
      "Epoch [204/500], Loss: 0.0178\n",
      "Epoch [204/500], Loss: 0.0067\n",
      "Epoch [204/500], Loss: 0.1094\n",
      "Epoch [204/500], Loss: 0.3175\n",
      "Epoch [204/500], Loss: 0.0340\n",
      "Epoch [205/500], Loss: 0.0201\n",
      "Epoch [205/500], Loss: 0.0239\n",
      "Epoch [205/500], Loss: 0.0368\n",
      "Epoch [205/500], Loss: 0.0554\n",
      "Epoch [205/500], Loss: 0.2110\n",
      "Epoch [205/500], Loss: 0.0122\n",
      "Epoch [205/500], Loss: 0.1160\n",
      "Epoch [205/500], Loss: 0.0164\n",
      "Epoch [206/500], Loss: 0.0277\n",
      "Epoch [206/500], Loss: 0.0162\n",
      "Epoch [206/500], Loss: 0.0124\n",
      "Epoch [206/500], Loss: 0.2397\n",
      "Epoch [206/500], Loss: 0.0167\n",
      "Epoch [206/500], Loss: 0.0119\n",
      "Epoch [206/500], Loss: 0.0114\n",
      "Epoch [206/500], Loss: 0.4335\n",
      "Epoch [207/500], Loss: 0.0093\n",
      "Epoch [207/500], Loss: 0.0189\n",
      "Epoch [207/500], Loss: 0.1368\n",
      "Epoch [207/500], Loss: 0.1589\n",
      "Epoch [207/500], Loss: 0.0184\n",
      "Epoch [207/500], Loss: 0.0195\n",
      "Epoch [207/500], Loss: 0.1000\n",
      "Epoch [207/500], Loss: 0.0086\n",
      "Epoch [208/500], Loss: 0.0213\n",
      "Epoch [208/500], Loss: 0.0086\n",
      "Epoch [208/500], Loss: 0.0060\n",
      "Epoch [208/500], Loss: 0.0149\n",
      "Epoch [208/500], Loss: 0.0134\n",
      "Epoch [208/500], Loss: 0.2169\n",
      "Epoch [208/500], Loss: 0.3219\n",
      "Epoch [208/500], Loss: 0.0681\n",
      "Epoch [209/500], Loss: 0.2097\n",
      "Epoch [209/500], Loss: 0.0482\n",
      "Epoch [209/500], Loss: 0.0978\n",
      "Epoch [209/500], Loss: 0.0641\n",
      "Epoch [209/500], Loss: 0.0557\n",
      "Epoch [209/500], Loss: 0.0158\n",
      "Epoch [209/500], Loss: 0.0199\n",
      "Epoch [209/500], Loss: 0.0151\n",
      "Epoch [210/500], Loss: 0.1553\n",
      "Epoch [210/500], Loss: 0.0080\n",
      "Epoch [210/500], Loss: 0.0060\n",
      "Epoch [210/500], Loss: 0.0159\n",
      "Epoch [210/500], Loss: 0.1879\n",
      "Epoch [210/500], Loss: 0.2039\n",
      "Epoch [210/500], Loss: 0.1305\n",
      "Epoch [210/500], Loss: 0.0143\n",
      "Epoch [211/500], Loss: 0.2182\n",
      "Epoch [211/500], Loss: 0.0170\n",
      "Epoch [211/500], Loss: 0.0303\n",
      "Epoch [211/500], Loss: 0.0247\n",
      "Epoch [211/500], Loss: 0.0313\n",
      "Epoch [211/500], Loss: 0.2029\n",
      "Epoch [211/500], Loss: 0.0549\n",
      "Epoch [211/500], Loss: 0.0291\n",
      "Epoch [212/500], Loss: 0.2166\n",
      "Epoch [212/500], Loss: 0.0184\n",
      "Epoch [212/500], Loss: 0.1620\n",
      "Epoch [212/500], Loss: 0.1482\n",
      "Epoch [212/500], Loss: 0.0177\n",
      "Epoch [212/500], Loss: 0.0213\n",
      "Epoch [212/500], Loss: 0.0212\n",
      "Epoch [212/500], Loss: 0.0183\n",
      "Epoch [213/500], Loss: 0.1283\n",
      "Epoch [213/500], Loss: 0.0208\n",
      "Epoch [213/500], Loss: 0.1262\n",
      "Epoch [213/500], Loss: 0.0103\n",
      "Epoch [213/500], Loss: 0.0088\n",
      "Epoch [213/500], Loss: 0.0140\n",
      "Epoch [213/500], Loss: 0.2651\n",
      "Epoch [213/500], Loss: 0.0091\n",
      "Epoch [214/500], Loss: 0.0094\n",
      "Epoch [214/500], Loss: 0.1388\n",
      "Epoch [214/500], Loss: 0.2866\n",
      "Epoch [214/500], Loss: 0.0104\n",
      "Epoch [214/500], Loss: 0.0611\n",
      "Epoch [214/500], Loss: 0.0183\n",
      "Epoch [214/500], Loss: 0.0390\n",
      "Epoch [214/500], Loss: 0.0217\n",
      "Epoch [215/500], Loss: 0.0674\n",
      "Epoch [215/500], Loss: 0.0257\n",
      "Epoch [215/500], Loss: 0.0276\n",
      "Epoch [215/500], Loss: 0.1274\n",
      "Epoch [215/500], Loss: 0.1411\n",
      "Epoch [215/500], Loss: 0.0174\n",
      "Epoch [215/500], Loss: 0.0999\n",
      "Epoch [215/500], Loss: 0.0083\n",
      "Epoch [216/500], Loss: 0.0268\n",
      "Epoch [216/500], Loss: 0.0079\n",
      "Epoch [216/500], Loss: 0.1548\n",
      "Epoch [216/500], Loss: 0.0078\n",
      "Epoch [216/500], Loss: 0.2646\n",
      "Epoch [216/500], Loss: 0.0361\n",
      "Epoch [216/500], Loss: 0.0050\n",
      "Epoch [216/500], Loss: 0.0040\n",
      "Epoch [217/500], Loss: 0.1430\n",
      "Epoch [217/500], Loss: 0.0047\n",
      "Epoch [217/500], Loss: 0.0615\n",
      "Epoch [217/500], Loss: 0.1243\n",
      "Epoch [217/500], Loss: 0.0109\n",
      "Epoch [217/500], Loss: 0.1185\n",
      "Epoch [217/500], Loss: 0.0276\n",
      "Epoch [217/500], Loss: 0.0069\n",
      "Epoch [218/500], Loss: 0.0239\n",
      "Epoch [218/500], Loss: 0.1944\n",
      "Epoch [218/500], Loss: 0.1421\n",
      "Epoch [218/500], Loss: 0.2091\n",
      "Epoch [218/500], Loss: 0.0319\n",
      "Epoch [218/500], Loss: 0.0282\n",
      "Epoch [218/500], Loss: 0.0345\n",
      "Epoch [218/500], Loss: 0.0105\n",
      "Epoch [219/500], Loss: 0.0323\n",
      "Epoch [219/500], Loss: 0.1019\n",
      "Epoch [219/500], Loss: 0.0957\n",
      "Epoch [219/500], Loss: 0.0155\n",
      "Epoch [219/500], Loss: 0.1003\n",
      "Epoch [219/500], Loss: 0.1345\n",
      "Epoch [219/500], Loss: 0.0191\n",
      "Epoch [219/500], Loss: 0.0049\n",
      "Epoch [220/500], Loss: 0.2684\n",
      "Epoch [220/500], Loss: 0.0427\n",
      "Epoch [220/500], Loss: 0.0156\n",
      "Epoch [220/500], Loss: 0.1021\n",
      "Epoch [220/500], Loss: 0.0146\n",
      "Epoch [220/500], Loss: 0.0896\n",
      "Epoch [220/500], Loss: 0.0081\n",
      "Epoch [220/500], Loss: 0.0270\n",
      "Epoch [221/500], Loss: 0.0160\n",
      "Epoch [221/500], Loss: 0.0108\n",
      "Epoch [221/500], Loss: 0.0143\n",
      "Epoch [221/500], Loss: 0.0073\n",
      "Epoch [221/500], Loss: 0.0039\n",
      "Epoch [221/500], Loss: 0.1370\n",
      "Epoch [221/500], Loss: 0.1819\n",
      "Epoch [221/500], Loss: 0.6069\n",
      "Epoch [222/500], Loss: 0.0094\n",
      "Epoch [222/500], Loss: 0.1132\n",
      "Epoch [222/500], Loss: 0.1671\n",
      "Epoch [222/500], Loss: 0.0527\n",
      "Epoch [222/500], Loss: 0.0615\n",
      "Epoch [222/500], Loss: 0.2062\n",
      "Epoch [222/500], Loss: 0.0675\n",
      "Epoch [222/500], Loss: 0.0511\n",
      "Epoch [223/500], Loss: 0.1306\n",
      "Epoch [223/500], Loss: 0.1326\n",
      "Epoch [223/500], Loss: 0.1205\n",
      "Epoch [223/500], Loss: 0.0430\n",
      "Epoch [223/500], Loss: 0.0350\n",
      "Epoch [223/500], Loss: 0.0282\n",
      "Epoch [223/500], Loss: 0.0292\n",
      "Epoch [223/500], Loss: 0.3657\n",
      "Epoch [224/500], Loss: 0.0161\n",
      "Epoch [224/500], Loss: 0.1329\n",
      "Epoch [224/500], Loss: 0.0139\n",
      "Epoch [224/500], Loss: 0.0192\n",
      "Epoch [224/500], Loss: 0.1001\n",
      "Epoch [224/500], Loss: 0.0049\n",
      "Epoch [224/500], Loss: 0.2869\n",
      "Epoch [224/500], Loss: 0.0044\n",
      "Epoch [225/500], Loss: 0.1499\n",
      "Epoch [225/500], Loss: 0.0075\n",
      "Epoch [225/500], Loss: 0.0053\n",
      "Epoch [225/500], Loss: 0.1917\n",
      "Epoch [225/500], Loss: 0.0229\n",
      "Epoch [225/500], Loss: 0.0111\n",
      "Epoch [225/500], Loss: 0.1344\n",
      "Epoch [225/500], Loss: 0.0132\n",
      "Epoch [226/500], Loss: 0.1487\n",
      "Epoch [226/500], Loss: 0.1244\n",
      "Epoch [226/500], Loss: 0.0106\n",
      "Epoch [226/500], Loss: 0.0554\n",
      "Epoch [226/500], Loss: 0.0220\n",
      "Epoch [226/500], Loss: 0.0197\n",
      "Epoch [226/500], Loss: 0.0155\n",
      "Epoch [226/500], Loss: 0.3261\n",
      "Epoch [227/500], Loss: 0.1690\n",
      "Epoch [227/500], Loss: 0.0131\n",
      "Epoch [227/500], Loss: 0.0147\n",
      "Epoch [227/500], Loss: 0.1275\n",
      "Epoch [227/500], Loss: 0.1313\n",
      "Epoch [227/500], Loss: 0.0382\n",
      "Epoch [227/500], Loss: 0.0709\n",
      "Epoch [227/500], Loss: 0.3952\n",
      "Epoch [228/500], Loss: 0.2319\n",
      "Epoch [228/500], Loss: 0.0563\n",
      "Epoch [228/500], Loss: 0.1369\n",
      "Epoch [228/500], Loss: 0.0514\n",
      "Epoch [228/500], Loss: 0.0571\n",
      "Epoch [228/500], Loss: 0.1308\n",
      "Epoch [228/500], Loss: 0.0645\n",
      "Epoch [228/500], Loss: 0.0322\n",
      "Epoch [229/500], Loss: 0.0469\n",
      "Epoch [229/500], Loss: 0.0290\n",
      "Epoch [229/500], Loss: 0.2327\n",
      "Epoch [229/500], Loss: 0.0400\n",
      "Epoch [229/500], Loss: 0.0175\n",
      "Epoch [229/500], Loss: 0.1276\n",
      "Epoch [229/500], Loss: 0.0112\n",
      "Epoch [229/500], Loss: 0.0139\n",
      "Epoch [230/500], Loss: 0.0096\n",
      "Epoch [230/500], Loss: 0.0047\n",
      "Epoch [230/500], Loss: 0.2821\n",
      "Epoch [230/500], Loss: 0.0060\n",
      "Epoch [230/500], Loss: 0.0114\n",
      "Epoch [230/500], Loss: 0.0072\n",
      "Epoch [230/500], Loss: 0.0739\n",
      "Epoch [230/500], Loss: 0.5937\n",
      "Epoch [231/500], Loss: 0.3003\n",
      "Epoch [231/500], Loss: 0.0081\n",
      "Epoch [231/500], Loss: 0.0168\n",
      "Epoch [231/500], Loss: 0.0453\n",
      "Epoch [231/500], Loss: 0.1212\n",
      "Epoch [231/500], Loss: 0.0643\n",
      "Epoch [231/500], Loss: 0.0365\n",
      "Epoch [231/500], Loss: 0.0324\n",
      "Epoch [232/500], Loss: 0.0327\n",
      "Epoch [232/500], Loss: 0.1248\n",
      "Epoch [232/500], Loss: 0.0244\n",
      "Epoch [232/500], Loss: 0.1469\n",
      "Epoch [232/500], Loss: 0.0147\n",
      "Epoch [232/500], Loss: 0.1002\n",
      "Epoch [232/500], Loss: 0.0134\n",
      "Epoch [232/500], Loss: 0.0209\n",
      "Epoch [233/500], Loss: 0.1633\n",
      "Epoch [233/500], Loss: 0.0952\n",
      "Epoch [233/500], Loss: 0.0053\n",
      "Epoch [233/500], Loss: 0.0118\n",
      "Epoch [233/500], Loss: 0.0081\n",
      "Epoch [233/500], Loss: 0.1189\n",
      "Epoch [233/500], Loss: 0.1324\n",
      "Epoch [233/500], Loss: 0.0298\n",
      "Epoch [234/500], Loss: 0.2406\n",
      "Epoch [234/500], Loss: 0.0212\n",
      "Epoch [234/500], Loss: 0.0107\n",
      "Epoch [234/500], Loss: 0.2218\n",
      "Epoch [234/500], Loss: 0.0698\n",
      "Epoch [234/500], Loss: 0.0282\n",
      "Epoch [234/500], Loss: 0.0465\n",
      "Epoch [234/500], Loss: 0.0240\n",
      "Epoch [235/500], Loss: 0.1268\n",
      "Epoch [235/500], Loss: 0.0237\n",
      "Epoch [235/500], Loss: 0.1134\n",
      "Epoch [235/500], Loss: 0.0438\n",
      "Epoch [235/500], Loss: 0.0261\n",
      "Epoch [235/500], Loss: 0.0174\n",
      "Epoch [235/500], Loss: 0.2397\n",
      "Epoch [235/500], Loss: 0.0162\n",
      "Epoch [236/500], Loss: 0.1019\n",
      "Epoch [236/500], Loss: 0.0117\n",
      "Epoch [236/500], Loss: 0.0093\n",
      "Epoch [236/500], Loss: 0.2527\n",
      "Epoch [236/500], Loss: 0.1334\n",
      "Epoch [236/500], Loss: 0.0067\n",
      "Epoch [236/500], Loss: 0.1067\n",
      "Epoch [236/500], Loss: 0.0097\n",
      "Epoch [237/500], Loss: 0.0163\n",
      "Epoch [237/500], Loss: 0.1041\n",
      "Epoch [237/500], Loss: 0.0265\n",
      "Epoch [237/500], Loss: 0.0142\n",
      "Epoch [237/500], Loss: 0.1200\n",
      "Epoch [237/500], Loss: 0.1543\n",
      "Epoch [237/500], Loss: 0.0337\n",
      "Epoch [237/500], Loss: 0.0097\n",
      "Epoch [238/500], Loss: 0.0101\n",
      "Epoch [238/500], Loss: 0.0534\n",
      "Epoch [238/500], Loss: 0.1004\n",
      "Epoch [238/500], Loss: 0.1053\n",
      "Epoch [238/500], Loss: 0.1028\n",
      "Epoch [238/500], Loss: 0.0118\n",
      "Epoch [238/500], Loss: 0.0197\n",
      "Epoch [238/500], Loss: 0.0159\n",
      "Epoch [239/500], Loss: 0.0101\n",
      "Epoch [239/500], Loss: 0.1254\n",
      "Epoch [239/500], Loss: 0.0693\n",
      "Epoch [239/500], Loss: 0.0140\n",
      "Epoch [239/500], Loss: 0.0095\n",
      "Epoch [239/500], Loss: 0.1012\n",
      "Epoch [239/500], Loss: 0.0094\n",
      "Epoch [239/500], Loss: 0.0023\n",
      "Epoch [240/500], Loss: 0.0139\n",
      "Epoch [240/500], Loss: 0.0005\n",
      "Epoch [240/500], Loss: 0.0069\n",
      "Epoch [240/500], Loss: 0.4387\n",
      "Epoch [240/500], Loss: 0.1605\n",
      "Epoch [240/500], Loss: 0.0127\n",
      "Epoch [240/500], Loss: 0.0122\n",
      "Epoch [240/500], Loss: 0.0098\n",
      "Epoch [241/500], Loss: 0.0223\n",
      "Epoch [241/500], Loss: 0.0228\n",
      "Epoch [241/500], Loss: 0.1152\n",
      "Epoch [241/500], Loss: 0.0360\n",
      "Epoch [241/500], Loss: 0.1329\n",
      "Epoch [241/500], Loss: 0.0316\n",
      "Epoch [241/500], Loss: 0.1974\n",
      "Epoch [241/500], Loss: 0.0330\n",
      "Epoch [242/500], Loss: 0.1955\n",
      "Epoch [242/500], Loss: 0.0380\n",
      "Epoch [242/500], Loss: 0.0308\n",
      "Epoch [242/500], Loss: 0.0212\n",
      "Epoch [242/500], Loss: 0.0182\n",
      "Epoch [242/500], Loss: 0.0168\n",
      "Epoch [242/500], Loss: 0.2965\n",
      "Epoch [242/500], Loss: 0.0119\n",
      "Epoch [243/500], Loss: 0.0073\n",
      "Epoch [243/500], Loss: 0.1241\n",
      "Epoch [243/500], Loss: 0.0097\n",
      "Epoch [243/500], Loss: 0.0887\n",
      "Epoch [243/500], Loss: 0.0177\n",
      "Epoch [243/500], Loss: 0.0072\n",
      "Epoch [243/500], Loss: 0.1793\n",
      "Epoch [243/500], Loss: 0.4533\n",
      "Epoch [244/500], Loss: 0.0101\n",
      "Epoch [244/500], Loss: 0.1659\n",
      "Epoch [244/500], Loss: 0.1186\n",
      "Epoch [244/500], Loss: 0.0274\n",
      "Epoch [244/500], Loss: 0.0223\n",
      "Epoch [244/500], Loss: 0.1303\n",
      "Epoch [244/500], Loss: 0.0243\n",
      "Epoch [244/500], Loss: 0.0247\n",
      "Epoch [245/500], Loss: 0.0989\n",
      "Epoch [245/500], Loss: 0.0182\n",
      "Epoch [245/500], Loss: 0.0156\n",
      "Epoch [245/500], Loss: 0.0195\n",
      "Epoch [245/500], Loss: 0.1372\n",
      "Epoch [245/500], Loss: 0.0104\n",
      "Epoch [245/500], Loss: 0.1982\n",
      "Epoch [245/500], Loss: 0.0084\n",
      "Epoch [246/500], Loss: 0.0281\n",
      "Epoch [246/500], Loss: 0.1505\n",
      "Epoch [246/500], Loss: 0.0154\n",
      "Epoch [246/500], Loss: 0.0950\n",
      "Epoch [246/500], Loss: 0.1206\n",
      "Epoch [246/500], Loss: 0.0221\n",
      "Epoch [246/500], Loss: 0.1436\n",
      "Epoch [246/500], Loss: 0.0294\n",
      "Epoch [247/500], Loss: 0.0325\n",
      "Epoch [247/500], Loss: 0.0219\n",
      "Epoch [247/500], Loss: 0.0292\n",
      "Epoch [247/500], Loss: 0.2584\n",
      "Epoch [247/500], Loss: 0.0855\n",
      "Epoch [247/500], Loss: 0.0146\n",
      "Epoch [247/500], Loss: 0.0320\n",
      "Epoch [247/500], Loss: 0.0311\n",
      "Epoch [248/500], Loss: 0.0401\n",
      "Epoch [248/500], Loss: 0.0225\n",
      "Epoch [248/500], Loss: 0.0071\n",
      "Epoch [248/500], Loss: 0.2075\n",
      "Epoch [248/500], Loss: 0.2629\n",
      "Epoch [248/500], Loss: 0.0073\n",
      "Epoch [248/500], Loss: 0.0096\n",
      "Epoch [248/500], Loss: 0.0039\n",
      "Epoch [249/500], Loss: 0.0082\n",
      "Epoch [249/500], Loss: 0.0081\n",
      "Epoch [249/500], Loss: 0.0059\n",
      "Epoch [249/500], Loss: 0.0753\n",
      "Epoch [249/500], Loss: 0.2735\n",
      "Epoch [249/500], Loss: 0.0974\n",
      "Epoch [249/500], Loss: 0.1480\n",
      "Epoch [249/500], Loss: 0.0203\n",
      "Epoch [250/500], Loss: 0.1116\n",
      "Epoch [250/500], Loss: 0.1181\n",
      "Epoch [250/500], Loss: 0.1132\n",
      "Epoch [250/500], Loss: 0.0422\n",
      "Epoch [250/500], Loss: 0.0547\n",
      "Epoch [250/500], Loss: 0.0413\n",
      "Epoch [250/500], Loss: 0.0390\n",
      "Epoch [250/500], Loss: 0.0498\n",
      "Epoch [251/500], Loss: 0.0303\n",
      "Epoch [251/500], Loss: 0.0370\n",
      "Epoch [251/500], Loss: 0.0129\n",
      "Epoch [251/500], Loss: 0.0110\n",
      "Epoch [251/500], Loss: 0.2727\n",
      "Epoch [251/500], Loss: 0.1300\n",
      "Epoch [251/500], Loss: 0.1187\n",
      "Epoch [251/500], Loss: 0.0090\n",
      "Epoch [252/500], Loss: 0.0051\n",
      "Epoch [252/500], Loss: 0.0044\n",
      "Epoch [252/500], Loss: 0.1704\n",
      "Epoch [252/500], Loss: 0.0080\n",
      "Epoch [252/500], Loss: 0.1799\n",
      "Epoch [252/500], Loss: 0.1323\n",
      "Epoch [252/500], Loss: 0.0049\n",
      "Epoch [252/500], Loss: 0.0055\n",
      "Epoch [253/500], Loss: 0.1478\n",
      "Epoch [253/500], Loss: 0.0274\n",
      "Epoch [253/500], Loss: 0.0107\n",
      "Epoch [253/500], Loss: 0.0080\n",
      "Epoch [253/500], Loss: 0.0179\n",
      "Epoch [253/500], Loss: 0.1102\n",
      "Epoch [253/500], Loss: 0.2710\n",
      "Epoch [253/500], Loss: 0.0162\n",
      "Epoch [254/500], Loss: 0.3328\n",
      "Epoch [254/500], Loss: 0.0311\n",
      "Epoch [254/500], Loss: 0.0199\n",
      "Epoch [254/500], Loss: 0.0314\n",
      "Epoch [254/500], Loss: 0.0269\n",
      "Epoch [254/500], Loss: 0.0176\n",
      "Epoch [254/500], Loss: 0.0156\n",
      "Epoch [254/500], Loss: 0.0334\n",
      "Epoch [255/500], Loss: 0.0160\n",
      "Epoch [255/500], Loss: 0.0159\n",
      "Epoch [255/500], Loss: 0.0174\n",
      "Epoch [255/500], Loss: 0.1810\n",
      "Epoch [255/500], Loss: 0.1302\n",
      "Epoch [255/500], Loss: 0.0093\n",
      "Epoch [255/500], Loss: 0.0089\n",
      "Epoch [255/500], Loss: 0.0107\n",
      "Epoch [256/500], Loss: 0.0109\n",
      "Epoch [256/500], Loss: 0.0049\n",
      "Epoch [256/500], Loss: 0.1583\n",
      "Epoch [256/500], Loss: 0.0055\n",
      "Epoch [256/500], Loss: 0.0059\n",
      "Epoch [256/500], Loss: 0.1690\n",
      "Epoch [256/500], Loss: 0.0098\n",
      "Epoch [256/500], Loss: 0.3643\n",
      "Epoch [257/500], Loss: 0.0105\n",
      "Epoch [257/500], Loss: 0.0073\n",
      "Epoch [257/500], Loss: 0.0093\n",
      "Epoch [257/500], Loss: 0.2076\n",
      "Epoch [257/500], Loss: 0.1166\n",
      "Epoch [257/500], Loss: 0.0165\n",
      "Epoch [257/500], Loss: 0.0244\n",
      "Epoch [257/500], Loss: 0.0172\n",
      "Epoch [258/500], Loss: 0.0297\n",
      "Epoch [258/500], Loss: 0.0021\n",
      "Epoch [258/500], Loss: 3.4367\n",
      "Epoch [258/500], Loss: 0.0002\n",
      "Epoch [258/500], Loss: 0.1811\n",
      "Epoch [258/500], Loss: 0.1576\n",
      "Epoch [258/500], Loss: 0.0606\n",
      "Epoch [258/500], Loss: 0.0933\n",
      "Epoch [259/500], Loss: 0.0307\n",
      "Epoch [259/500], Loss: 0.0258\n",
      "Epoch [259/500], Loss: 0.0240\n",
      "Epoch [259/500], Loss: 0.2341\n",
      "Epoch [259/500], Loss: 0.1046\n",
      "Epoch [259/500], Loss: 0.0223\n",
      "Epoch [259/500], Loss: 0.1325\n",
      "Epoch [259/500], Loss: 0.0148\n",
      "Epoch [260/500], Loss: 0.0181\n",
      "Epoch [260/500], Loss: 0.1482\n",
      "Epoch [260/500], Loss: 0.0985\n",
      "Epoch [260/500], Loss: 0.0202\n",
      "Epoch [260/500], Loss: 0.0092\n",
      "Epoch [260/500], Loss: 0.1107\n",
      "Epoch [260/500], Loss: 0.1152\n",
      "Epoch [260/500], Loss: 0.0106\n",
      "Epoch [261/500], Loss: 0.0123\n",
      "Epoch [261/500], Loss: 0.0088\n",
      "Epoch [261/500], Loss: 0.0157\n",
      "Epoch [261/500], Loss: 0.0074\n",
      "Epoch [261/500], Loss: 0.3059\n",
      "Epoch [261/500], Loss: 0.0906\n",
      "Epoch [261/500], Loss: 0.1142\n",
      "Epoch [261/500], Loss: 0.0245\n",
      "Epoch [262/500], Loss: 0.2175\n",
      "Epoch [262/500], Loss: 0.1402\n",
      "Epoch [262/500], Loss: 0.0305\n",
      "Epoch [262/500], Loss: 0.0202\n",
      "Epoch [262/500], Loss: 0.0166\n",
      "Epoch [262/500], Loss: 0.0095\n",
      "Epoch [262/500], Loss: 0.0077\n",
      "Epoch [262/500], Loss: 0.0085\n",
      "Epoch [263/500], Loss: 0.1707\n",
      "Epoch [263/500], Loss: 0.0049\n",
      "Epoch [263/500], Loss: 0.0124\n",
      "Epoch [263/500], Loss: 0.0109\n",
      "Epoch [263/500], Loss: 0.0046\n",
      "Epoch [263/500], Loss: 0.0050\n",
      "Epoch [263/500], Loss: 0.1969\n",
      "Epoch [263/500], Loss: 0.1102\n",
      "Epoch [264/500], Loss: 0.0043\n",
      "Epoch [264/500], Loss: 0.0176\n",
      "Epoch [264/500], Loss: 0.0898\n",
      "Epoch [264/500], Loss: 0.2090\n",
      "Epoch [264/500], Loss: 0.1298\n",
      "Epoch [264/500], Loss: 0.0083\n",
      "Epoch [264/500], Loss: 0.1204\n",
      "Epoch [264/500], Loss: 0.0133\n",
      "Epoch [265/500], Loss: 0.1370\n",
      "Epoch [265/500], Loss: 0.1533\n",
      "Epoch [265/500], Loss: 0.0066\n",
      "Epoch [265/500], Loss: 0.0925\n",
      "Epoch [265/500], Loss: 0.1231\n",
      "Epoch [265/500], Loss: 0.0260\n",
      "Epoch [265/500], Loss: 0.0456\n",
      "Epoch [265/500], Loss: 0.0180\n",
      "Epoch [266/500], Loss: 0.1606\n",
      "Epoch [266/500], Loss: 0.0461\n",
      "Epoch [266/500], Loss: 0.0226\n",
      "Epoch [266/500], Loss: 0.0217\n",
      "Epoch [266/500], Loss: 0.0164\n",
      "Epoch [266/500], Loss: 0.0835\n",
      "Epoch [266/500], Loss: 0.1874\n",
      "Epoch [266/500], Loss: 0.0170\n",
      "Epoch [267/500], Loss: 0.1811\n",
      "Epoch [267/500], Loss: 0.0146\n",
      "Epoch [267/500], Loss: 0.1323\n",
      "Epoch [267/500], Loss: 0.0184\n",
      "Epoch [267/500], Loss: 0.1637\n",
      "Epoch [267/500], Loss: 0.0242\n",
      "Epoch [267/500], Loss: 0.0229\n",
      "Epoch [267/500], Loss: 0.0756\n",
      "Epoch [268/500], Loss: 0.1611\n",
      "Epoch [268/500], Loss: 0.0142\n",
      "Epoch [268/500], Loss: 0.0121\n",
      "Epoch [268/500], Loss: 0.0129\n",
      "Epoch [268/500], Loss: 0.0171\n",
      "Epoch [268/500], Loss: 0.1089\n",
      "Epoch [268/500], Loss: 0.0984\n",
      "Epoch [268/500], Loss: 0.0053\n",
      "Epoch [269/500], Loss: 0.1708\n",
      "Epoch [269/500], Loss: 0.0948\n",
      "Epoch [269/500], Loss: 0.0057\n",
      "Epoch [269/500], Loss: 0.0139\n",
      "Epoch [269/500], Loss: 0.1087\n",
      "Epoch [269/500], Loss: 0.0163\n",
      "Epoch [269/500], Loss: 0.0249\n",
      "Epoch [269/500], Loss: 0.0078\n",
      "Epoch [270/500], Loss: 0.1185\n",
      "Epoch [270/500], Loss: 0.0164\n",
      "Epoch [270/500], Loss: 0.0206\n",
      "Epoch [270/500], Loss: 0.0950\n",
      "Epoch [270/500], Loss: 0.0139\n",
      "Epoch [270/500], Loss: 0.1304\n",
      "Epoch [270/500], Loss: 0.0122\n",
      "Epoch [270/500], Loss: 0.0105\n",
      "Epoch [271/500], Loss: 0.0069\n",
      "Epoch [271/500], Loss: 0.0150\n",
      "Epoch [271/500], Loss: 0.0074\n",
      "Epoch [271/500], Loss: 0.1412\n",
      "Epoch [271/500], Loss: 0.1201\n",
      "Epoch [271/500], Loss: 0.0581\n",
      "Epoch [271/500], Loss: 0.0084\n",
      "Epoch [271/500], Loss: 0.3660\n",
      "Epoch [272/500], Loss: 0.1127\n",
      "Epoch [272/500], Loss: 0.1486\n",
      "Epoch [272/500], Loss: 0.0153\n",
      "Epoch [272/500], Loss: 0.0205\n",
      "Epoch [272/500], Loss: 0.0791\n",
      "Epoch [272/500], Loss: 0.0442\n",
      "Epoch [272/500], Loss: 0.0227\n",
      "Epoch [272/500], Loss: 0.0290\n",
      "Epoch [273/500], Loss: 0.1214\n",
      "Epoch [273/500], Loss: 0.0201\n",
      "Epoch [273/500], Loss: 0.0217\n",
      "Epoch [273/500], Loss: 0.1012\n",
      "Epoch [273/500], Loss: 0.0166\n",
      "Epoch [273/500], Loss: 0.0933\n",
      "Epoch [273/500], Loss: 0.0211\n",
      "Epoch [273/500], Loss: 0.0062\n",
      "Epoch [274/500], Loss: 0.0120\n",
      "Epoch [274/500], Loss: 0.0069\n",
      "Epoch [274/500], Loss: 0.0034\n",
      "Epoch [274/500], Loss: 0.0619\n",
      "Epoch [274/500], Loss: 0.0025\n",
      "Epoch [274/500], Loss: 0.2418\n",
      "Epoch [274/500], Loss: 0.0025\n",
      "Epoch [274/500], Loss: 0.3438\n",
      "Epoch [275/500], Loss: 0.0050\n",
      "Epoch [275/500], Loss: 0.0851\n",
      "Epoch [275/500], Loss: 0.0190\n",
      "Epoch [275/500], Loss: 0.0088\n",
      "Epoch [275/500], Loss: 0.0097\n",
      "Epoch [275/500], Loss: 0.0233\n",
      "Epoch [275/500], Loss: 0.1583\n",
      "Epoch [275/500], Loss: 0.0077\n",
      "Epoch [276/500], Loss: 0.1066\n",
      "Epoch [276/500], Loss: 0.0317\n",
      "Epoch [276/500], Loss: 0.1212\n",
      "Epoch [276/500], Loss: 0.0193\n",
      "Epoch [276/500], Loss: 0.0204\n",
      "Epoch [276/500], Loss: 0.0475\n",
      "Epoch [276/500], Loss: 0.1417\n",
      "Epoch [276/500], Loss: 0.0101\n",
      "Epoch [277/500], Loss: 0.1166\n",
      "Epoch [277/500], Loss: 0.0553\n",
      "Epoch [277/500], Loss: 0.0333\n",
      "Epoch [277/500], Loss: 0.0901\n",
      "Epoch [277/500], Loss: 0.0481\n",
      "Epoch [277/500], Loss: 0.0906\n",
      "Epoch [277/500], Loss: 0.0181\n",
      "Epoch [277/500], Loss: 0.0063\n",
      "Epoch [278/500], Loss: 0.0127\n",
      "Epoch [278/500], Loss: 0.0159\n",
      "Epoch [278/500], Loss: 0.0080\n",
      "Epoch [278/500], Loss: 0.2488\n",
      "Epoch [278/500], Loss: 0.1151\n",
      "Epoch [278/500], Loss: 0.1126\n",
      "Epoch [278/500], Loss: 0.0078\n",
      "Epoch [278/500], Loss: 0.0284\n",
      "Epoch [279/500], Loss: 0.0193\n",
      "Epoch [279/500], Loss: 0.1052\n",
      "Epoch [279/500], Loss: 0.0917\n",
      "Epoch [279/500], Loss: 0.0183\n",
      "Epoch [279/500], Loss: 0.1029\n",
      "Epoch [279/500], Loss: 0.0833\n",
      "Epoch [279/500], Loss: 0.0179\n",
      "Epoch [279/500], Loss: 0.0078\n",
      "Epoch [280/500], Loss: 0.0131\n",
      "Epoch [280/500], Loss: 0.1141\n",
      "Epoch [280/500], Loss: 0.0118\n",
      "Epoch [280/500], Loss: 0.0022\n",
      "Epoch [280/500], Loss: 0.0053\n",
      "Epoch [280/500], Loss: 0.1670\n",
      "Epoch [280/500], Loss: 0.0068\n",
      "Epoch [280/500], Loss: 0.2734\n",
      "Epoch [281/500], Loss: 0.0113\n",
      "Epoch [281/500], Loss: 0.0869\n",
      "Epoch [281/500], Loss: 0.0181\n",
      "Epoch [281/500], Loss: 0.0723\n",
      "Epoch [281/500], Loss: 0.0204\n",
      "Epoch [281/500], Loss: 0.0411\n",
      "Epoch [281/500], Loss: 0.0843\n",
      "Epoch [281/500], Loss: 0.0233\n",
      "Epoch [282/500], Loss: 0.0881\n",
      "Epoch [282/500], Loss: 0.0159\n",
      "Epoch [282/500], Loss: 0.0171\n",
      "Epoch [282/500], Loss: 0.1882\n",
      "Epoch [282/500], Loss: 0.0082\n",
      "Epoch [282/500], Loss: 0.0076\n",
      "Epoch [282/500], Loss: 0.1201\n",
      "Epoch [282/500], Loss: 0.0084\n",
      "Epoch [283/500], Loss: 0.1918\n",
      "Epoch [283/500], Loss: 0.0073\n",
      "Epoch [283/500], Loss: 0.0210\n",
      "Epoch [283/500], Loss: 0.0089\n",
      "Epoch [283/500], Loss: 0.0099\n",
      "Epoch [283/500], Loss: 0.0991\n",
      "Epoch [283/500], Loss: 0.0180\n",
      "Epoch [283/500], Loss: 0.0090\n",
      "Epoch [284/500], Loss: 0.1978\n",
      "Epoch [284/500], Loss: 0.0821\n",
      "Epoch [284/500], Loss: 0.0082\n",
      "Epoch [284/500], Loss: 0.0117\n",
      "Epoch [284/500], Loss: 0.0109\n",
      "Epoch [284/500], Loss: 0.0128\n",
      "Epoch [284/500], Loss: 0.0220\n",
      "Epoch [284/500], Loss: 0.0046\n",
      "Epoch [285/500], Loss: 0.0152\n",
      "Epoch [285/500], Loss: 0.0873\n",
      "Epoch [285/500], Loss: 0.0075\n",
      "Epoch [285/500], Loss: 0.1934\n",
      "Epoch [285/500], Loss: 0.0026\n",
      "Epoch [285/500], Loss: 0.0089\n",
      "Epoch [285/500], Loss: 0.0117\n",
      "Epoch [285/500], Loss: 0.0159\n",
      "Epoch [286/500], Loss: 0.0269\n",
      "Epoch [286/500], Loss: 0.1229\n",
      "Epoch [286/500], Loss: 0.0054\n",
      "Epoch [286/500], Loss: 0.1942\n",
      "Epoch [286/500], Loss: 0.0064\n",
      "Epoch [286/500], Loss: 0.0271\n",
      "Epoch [286/500], Loss: 0.0382\n",
      "Epoch [286/500], Loss: 0.0092\n",
      "Epoch [287/500], Loss: 0.1115\n",
      "Epoch [287/500], Loss: 0.1261\n",
      "Epoch [287/500], Loss: 0.0957\n",
      "Epoch [287/500], Loss: 0.0170\n",
      "Epoch [287/500], Loss: 0.0057\n",
      "Epoch [287/500], Loss: 0.0079\n",
      "Epoch [287/500], Loss: 0.1241\n",
      "Epoch [287/500], Loss: 0.0177\n",
      "Epoch [288/500], Loss: 0.0063\n",
      "Epoch [288/500], Loss: 0.1310\n",
      "Epoch [288/500], Loss: 0.0085\n",
      "Epoch [288/500], Loss: 0.1714\n",
      "Epoch [288/500], Loss: 0.0161\n",
      "Epoch [288/500], Loss: 0.0877\n",
      "Epoch [288/500], Loss: 0.0133\n",
      "Epoch [288/500], Loss: 0.0156\n",
      "Epoch [289/500], Loss: 0.0518\n",
      "Epoch [289/500], Loss: 0.0227\n",
      "Epoch [289/500], Loss: 0.0104\n",
      "Epoch [289/500], Loss: 0.0097\n",
      "Epoch [289/500], Loss: 0.0132\n",
      "Epoch [289/500], Loss: 0.0127\n",
      "Epoch [289/500], Loss: 0.2436\n",
      "Epoch [289/500], Loss: 0.0085\n",
      "Epoch [290/500], Loss: 0.0115\n",
      "Epoch [290/500], Loss: 0.2445\n",
      "Epoch [290/500], Loss: 0.0259\n",
      "Epoch [290/500], Loss: 0.0627\n",
      "Epoch [290/500], Loss: 0.0056\n",
      "Epoch [290/500], Loss: 0.0032\n",
      "Epoch [290/500], Loss: 0.0083\n",
      "Epoch [290/500], Loss: 0.0095\n",
      "Epoch [291/500], Loss: 0.0067\n",
      "Epoch [291/500], Loss: 0.0199\n",
      "Epoch [291/500], Loss: 0.0067\n",
      "Epoch [291/500], Loss: 0.0918\n",
      "Epoch [291/500], Loss: 0.1745\n",
      "Epoch [291/500], Loss: 0.0098\n",
      "Epoch [291/500], Loss: 0.0146\n",
      "Epoch [291/500], Loss: 0.3235\n",
      "Epoch [292/500], Loss: 0.0928\n",
      "Epoch [292/500], Loss: 0.0353\n",
      "Epoch [292/500], Loss: 0.0434\n",
      "Epoch [292/500], Loss: 0.0512\n",
      "Epoch [292/500], Loss: 0.0274\n",
      "Epoch [292/500], Loss: 0.1176\n",
      "Epoch [292/500], Loss: 0.1303\n",
      "Epoch [292/500], Loss: 0.0186\n",
      "Epoch [293/500], Loss: 0.0372\n",
      "Epoch [293/500], Loss: 0.0141\n",
      "Epoch [293/500], Loss: 0.0108\n",
      "Epoch [293/500], Loss: 0.3141\n",
      "Epoch [293/500], Loss: 0.1077\n",
      "Epoch [293/500], Loss: 0.1097\n",
      "Epoch [293/500], Loss: 0.0332\n",
      "Epoch [293/500], Loss: 0.0034\n",
      "Epoch [294/500], Loss: 0.0000\n",
      "Epoch [294/500], Loss: 0.0926\n",
      "Epoch [294/500], Loss: 0.0000\n",
      "Epoch [294/500], Loss: 0.1310\n",
      "Epoch [294/500], Loss: 0.2852\n",
      "Epoch [294/500], Loss: 0.0085\n",
      "Epoch [294/500], Loss: 0.2822\n",
      "Epoch [294/500], Loss: 0.0163\n",
      "Epoch [295/500], Loss: 0.0132\n",
      "Epoch [295/500], Loss: 0.0177\n",
      "Epoch [295/500], Loss: 0.2278\n",
      "Epoch [295/500], Loss: 0.0834\n",
      "Epoch [295/500], Loss: 0.0675\n",
      "Epoch [295/500], Loss: 0.0268\n",
      "Epoch [295/500], Loss: 0.0649\n",
      "Epoch [295/500], Loss: 0.2710\n",
      "Epoch [296/500], Loss: 0.0950\n",
      "Epoch [296/500], Loss: 0.0337\n",
      "Epoch [296/500], Loss: 0.0773\n",
      "Epoch [296/500], Loss: 0.1085\n",
      "Epoch [296/500], Loss: 0.0843\n",
      "Epoch [296/500], Loss: 0.0288\n",
      "Epoch [296/500], Loss: 0.0168\n",
      "Epoch [296/500], Loss: 0.0151\n",
      "Epoch [297/500], Loss: 0.1249\n",
      "Epoch [297/500], Loss: 0.0113\n",
      "Epoch [297/500], Loss: 0.0105\n",
      "Epoch [297/500], Loss: 0.1634\n",
      "Epoch [297/500], Loss: 0.0068\n",
      "Epoch [297/500], Loss: 0.0076\n",
      "Epoch [297/500], Loss: 0.2490\n",
      "Epoch [297/500], Loss: 0.0111\n",
      "Epoch [298/500], Loss: 0.0970\n",
      "Epoch [298/500], Loss: 0.0854\n",
      "Epoch [298/500], Loss: 0.0112\n",
      "Epoch [298/500], Loss: 0.0917\n",
      "Epoch [298/500], Loss: 0.0417\n",
      "Epoch [298/500], Loss: 0.0335\n",
      "Epoch [298/500], Loss: 0.0206\n",
      "Epoch [298/500], Loss: 0.0180\n",
      "Epoch [299/500], Loss: 0.0054\n",
      "Epoch [299/500], Loss: 0.0042\n",
      "Epoch [299/500], Loss: 0.1161\n",
      "Epoch [299/500], Loss: 0.0099\n",
      "Epoch [299/500], Loss: 0.0035\n",
      "Epoch [299/500], Loss: 0.1019\n",
      "Epoch [299/500], Loss: 0.3932\n",
      "Epoch [299/500], Loss: 0.0140\n",
      "Epoch [300/500], Loss: 0.0078\n",
      "Epoch [300/500], Loss: 0.0118\n",
      "Epoch [300/500], Loss: 0.1532\n",
      "Epoch [300/500], Loss: 0.0095\n",
      "Epoch [300/500], Loss: 0.0742\n",
      "Epoch [300/500], Loss: 0.1260\n",
      "Epoch [300/500], Loss: 0.0449\n",
      "Epoch [300/500], Loss: 0.0140\n",
      "Epoch [301/500], Loss: 0.0973\n",
      "Epoch [301/500], Loss: 0.0080\n",
      "Epoch [301/500], Loss: 0.0097\n",
      "Epoch [301/500], Loss: 0.1891\n",
      "Epoch [301/500], Loss: 0.0115\n",
      "Epoch [301/500], Loss: 0.0240\n",
      "Epoch [301/500], Loss: 0.0352\n",
      "Epoch [301/500], Loss: 0.0049\n",
      "Epoch [302/500], Loss: 0.0123\n",
      "Epoch [302/500], Loss: 0.0069\n",
      "Epoch [302/500], Loss: 0.0012\n",
      "Epoch [302/500], Loss: 0.3240\n",
      "Epoch [302/500], Loss: 0.0040\n",
      "Epoch [302/500], Loss: 0.0626\n",
      "Epoch [302/500], Loss: 0.1862\n",
      "Epoch [302/500], Loss: 0.0040\n",
      "Epoch [303/500], Loss: 0.0855\n",
      "Epoch [303/500], Loss: 0.0599\n",
      "Epoch [303/500], Loss: 0.0164\n",
      "Epoch [303/500], Loss: 0.0990\n",
      "Epoch [303/500], Loss: 0.0364\n",
      "Epoch [303/500], Loss: 0.0455\n",
      "Epoch [303/500], Loss: 0.0635\n",
      "Epoch [303/500], Loss: 0.0220\n",
      "Epoch [304/500], Loss: 0.0247\n",
      "Epoch [304/500], Loss: 0.0374\n",
      "Epoch [304/500], Loss: 0.0184\n",
      "Epoch [304/500], Loss: 0.1419\n",
      "Epoch [304/500], Loss: 0.0041\n",
      "Epoch [304/500], Loss: 0.1045\n",
      "Epoch [304/500], Loss: 0.1656\n",
      "Epoch [304/500], Loss: 0.0069\n",
      "Epoch [305/500], Loss: 0.0168\n",
      "Epoch [305/500], Loss: 0.0096\n",
      "Epoch [305/500], Loss: 0.0094\n",
      "Epoch [305/500], Loss: 0.4045\n",
      "Epoch [305/500], Loss: 0.0910\n",
      "Epoch [305/500], Loss: 0.0105\n",
      "Epoch [305/500], Loss: 0.0349\n",
      "Epoch [305/500], Loss: 0.4371\n",
      "Epoch [306/500], Loss: 0.0238\n",
      "Epoch [306/500], Loss: 0.0700\n",
      "Epoch [306/500], Loss: 0.0862\n",
      "Epoch [306/500], Loss: 0.1473\n",
      "Epoch [306/500], Loss: 0.0868\n",
      "Epoch [306/500], Loss: 0.0435\n",
      "Epoch [306/500], Loss: 0.1180\n",
      "Epoch [306/500], Loss: 0.2643\n",
      "Epoch [307/500], Loss: 0.1038\n",
      "Epoch [307/500], Loss: 0.0159\n",
      "Epoch [307/500], Loss: 0.0281\n",
      "Epoch [307/500], Loss: 0.0156\n",
      "Epoch [307/500], Loss: 0.0122\n",
      "Epoch [307/500], Loss: 0.0869\n",
      "Epoch [307/500], Loss: 0.0937\n",
      "Epoch [307/500], Loss: 0.0074\n",
      "Epoch [308/500], Loss: 0.0819\n",
      "Epoch [308/500], Loss: 0.0059\n",
      "Epoch [308/500], Loss: 0.0933\n",
      "Epoch [308/500], Loss: 0.0033\n",
      "Epoch [308/500], Loss: 0.0941\n",
      "Epoch [308/500], Loss: 0.0028\n",
      "Epoch [308/500], Loss: 0.0044\n",
      "Epoch [308/500], Loss: 0.0026\n",
      "Epoch [309/500], Loss: 0.0071\n",
      "Epoch [309/500], Loss: 0.0074\n",
      "Epoch [309/500], Loss: 0.0024\n",
      "Epoch [309/500], Loss: 0.0061\n",
      "Epoch [309/500], Loss: 0.0963\n",
      "Epoch [309/500], Loss: 0.3440\n",
      "Epoch [309/500], Loss: 0.0027\n",
      "Epoch [309/500], Loss: 0.0016\n",
      "Epoch [310/500], Loss: 0.0304\n",
      "Epoch [310/500], Loss: 0.1499\n",
      "Epoch [310/500], Loss: 0.1065\n",
      "Epoch [310/500], Loss: 0.0340\n",
      "Epoch [310/500], Loss: 0.0143\n",
      "Epoch [310/500], Loss: 0.0309\n",
      "Epoch [310/500], Loss: 0.0190\n",
      "Epoch [310/500], Loss: 0.0100\n",
      "Epoch [311/500], Loss: 0.0143\n",
      "Epoch [311/500], Loss: 0.0177\n",
      "Epoch [311/500], Loss: 0.0159\n",
      "Epoch [311/500], Loss: 0.0444\n",
      "Epoch [311/500], Loss: 0.0917\n",
      "Epoch [311/500], Loss: 0.0101\n",
      "Epoch [311/500], Loss: 0.1114\n",
      "Epoch [311/500], Loss: 0.4177\n",
      "Epoch [312/500], Loss: 0.0100\n",
      "Epoch [312/500], Loss: 0.1048\n",
      "Epoch [312/500], Loss: 0.0210\n",
      "Epoch [312/500], Loss: 0.1510\n",
      "Epoch [312/500], Loss: 0.0292\n",
      "Epoch [312/500], Loss: 0.1008\n",
      "Epoch [312/500], Loss: 0.0246\n",
      "Epoch [312/500], Loss: 0.0288\n",
      "Epoch [313/500], Loss: 0.1320\n",
      "Epoch [313/500], Loss: 0.0536\n",
      "Epoch [313/500], Loss: 0.0376\n",
      "Epoch [313/500], Loss: 0.0690\n",
      "Epoch [313/500], Loss: 0.1455\n",
      "Epoch [313/500], Loss: 0.0215\n",
      "Epoch [313/500], Loss: 0.0163\n",
      "Epoch [313/500], Loss: 0.2870\n",
      "Epoch [314/500], Loss: 0.0817\n",
      "Epoch [314/500], Loss: 0.0073\n",
      "Epoch [314/500], Loss: 0.0966\n",
      "Epoch [314/500], Loss: 0.0737\n",
      "Epoch [314/500], Loss: 0.0119\n",
      "Epoch [314/500], Loss: 0.0174\n",
      "Epoch [314/500], Loss: 0.0039\n",
      "Epoch [314/500], Loss: 0.0335\n",
      "Epoch [315/500], Loss: 0.0099\n",
      "Epoch [315/500], Loss: 0.0055\n",
      "Epoch [315/500], Loss: 0.0004\n",
      "Epoch [315/500], Loss: 0.0001\n",
      "Epoch [315/500], Loss: 0.3289\n",
      "Epoch [315/500], Loss: 0.3454\n",
      "Epoch [315/500], Loss: 0.0037\n",
      "Epoch [315/500], Loss: 0.0115\n",
      "Epoch [316/500], Loss: 0.0604\n",
      "Epoch [316/500], Loss: 0.1357\n",
      "Epoch [316/500], Loss: 0.0551\n",
      "Epoch [316/500], Loss: 0.0497\n",
      "Epoch [316/500], Loss: 0.2391\n",
      "Epoch [316/500], Loss: 0.0494\n",
      "Epoch [316/500], Loss: 0.0277\n",
      "Epoch [316/500], Loss: 0.0127\n",
      "Epoch [317/500], Loss: 0.0072\n",
      "Epoch [317/500], Loss: 0.0002\n",
      "Epoch [317/500], Loss: 0.8142\n",
      "Epoch [317/500], Loss: 0.0028\n",
      "Epoch [317/500], Loss: 0.1774\n",
      "Epoch [317/500], Loss: 0.2406\n",
      "Epoch [317/500], Loss: 0.1151\n",
      "Epoch [317/500], Loss: 0.1155\n",
      "Epoch [318/500], Loss: 0.2127\n",
      "Epoch [318/500], Loss: 0.1106\n",
      "Epoch [318/500], Loss: 0.0671\n",
      "Epoch [318/500], Loss: 0.0656\n",
      "Epoch [318/500], Loss: 0.1167\n",
      "Epoch [318/500], Loss: 0.0250\n",
      "Epoch [318/500], Loss: 0.1284\n",
      "Epoch [318/500], Loss: 0.0048\n",
      "Epoch [319/500], Loss: 0.2632\n",
      "Epoch [319/500], Loss: 0.0620\n",
      "Epoch [319/500], Loss: 0.1182\n",
      "Epoch [319/500], Loss: 0.0139\n",
      "Epoch [319/500], Loss: 0.0226\n",
      "Epoch [319/500], Loss: 0.1185\n",
      "Epoch [319/500], Loss: 0.0090\n",
      "Epoch [319/500], Loss: 0.0163\n",
      "Epoch [320/500], Loss: 0.0141\n",
      "Epoch [320/500], Loss: 0.0072\n",
      "Epoch [320/500], Loss: 0.0072\n",
      "Epoch [320/500], Loss: 0.2219\n",
      "Epoch [320/500], Loss: 0.1432\n",
      "Epoch [320/500], Loss: 0.0142\n",
      "Epoch [320/500], Loss: 0.1115\n",
      "Epoch [320/500], Loss: 0.0062\n",
      "Epoch [321/500], Loss: 0.0097\n",
      "Epoch [321/500], Loss: 0.0192\n",
      "Epoch [321/500], Loss: 0.1712\n",
      "Epoch [321/500], Loss: 0.1232\n",
      "Epoch [321/500], Loss: 0.0109\n",
      "Epoch [321/500], Loss: 0.1324\n",
      "Epoch [321/500], Loss: 0.0155\n",
      "Epoch [321/500], Loss: 0.0084\n",
      "Epoch [322/500], Loss: 0.0121\n",
      "Epoch [322/500], Loss: 0.0111\n",
      "Epoch [322/500], Loss: 0.1719\n",
      "Epoch [322/500], Loss: 0.0929\n",
      "Epoch [322/500], Loss: 0.0116\n",
      "Epoch [322/500], Loss: 0.0352\n",
      "Epoch [322/500], Loss: 0.0117\n",
      "Epoch [322/500], Loss: 0.0037\n",
      "Epoch [323/500], Loss: 0.0101\n",
      "Epoch [323/500], Loss: 0.1003\n",
      "Epoch [323/500], Loss: 0.0049\n",
      "Epoch [323/500], Loss: 0.0147\n",
      "Epoch [323/500], Loss: 0.2479\n",
      "Epoch [323/500], Loss: 0.0041\n",
      "Epoch [323/500], Loss: 0.2386\n",
      "Epoch [323/500], Loss: 0.0030\n",
      "Epoch [324/500], Loss: 0.0092\n",
      "Epoch [324/500], Loss: 0.1227\n",
      "Epoch [324/500], Loss: 0.0188\n",
      "Epoch [324/500], Loss: 0.0200\n",
      "Epoch [324/500], Loss: 0.0535\n",
      "Epoch [324/500], Loss: 0.1167\n",
      "Epoch [324/500], Loss: 0.1196\n",
      "Epoch [324/500], Loss: 0.0230\n",
      "Epoch [325/500], Loss: 0.0924\n",
      "Epoch [325/500], Loss: 0.0245\n",
      "Epoch [325/500], Loss: 0.0185\n",
      "Epoch [325/500], Loss: 0.1182\n",
      "Epoch [325/500], Loss: 0.0167\n",
      "Epoch [325/500], Loss: 0.1397\n",
      "Epoch [325/500], Loss: 0.0419\n",
      "Epoch [325/500], Loss: 0.0147\n",
      "Epoch [326/500], Loss: 0.0474\n",
      "Epoch [326/500], Loss: 0.0073\n",
      "Epoch [326/500], Loss: 0.1034\n",
      "Epoch [326/500], Loss: 0.1674\n",
      "Epoch [326/500], Loss: 0.0148\n",
      "Epoch [326/500], Loss: 0.1140\n",
      "Epoch [326/500], Loss: 0.0095\n",
      "Epoch [326/500], Loss: 0.0072\n",
      "Epoch [327/500], Loss: 0.0082\n",
      "Epoch [327/500], Loss: 0.0044\n",
      "Epoch [327/500], Loss: 0.0896\n",
      "Epoch [327/500], Loss: 0.0788\n",
      "Epoch [327/500], Loss: 0.0846\n",
      "Epoch [327/500], Loss: 0.1073\n",
      "Epoch [327/500], Loss: 0.0041\n",
      "Epoch [327/500], Loss: 0.0077\n",
      "Epoch [328/500], Loss: 0.0067\n",
      "Epoch [328/500], Loss: 0.0094\n",
      "Epoch [328/500], Loss: 0.0086\n",
      "Epoch [328/500], Loss: 0.1941\n",
      "Epoch [328/500], Loss: 0.0118\n",
      "Epoch [328/500], Loss: 0.0934\n",
      "Epoch [328/500], Loss: 0.0108\n",
      "Epoch [328/500], Loss: 0.0117\n",
      "Epoch [329/500], Loss: 0.0167\n",
      "Epoch [329/500], Loss: 0.2240\n",
      "Epoch [329/500], Loss: 0.0113\n",
      "Epoch [329/500], Loss: 0.0121\n",
      "Epoch [329/500], Loss: 0.0181\n",
      "Epoch [329/500], Loss: 0.1072\n",
      "Epoch [329/500], Loss: 0.0136\n",
      "Epoch [329/500], Loss: 0.0089\n",
      "Epoch [330/500], Loss: 0.0064\n",
      "Epoch [330/500], Loss: 0.1030\n",
      "Epoch [330/500], Loss: 0.0092\n",
      "Epoch [330/500], Loss: 0.0764\n",
      "Epoch [330/500], Loss: 0.1680\n",
      "Epoch [330/500], Loss: 0.0076\n",
      "Epoch [330/500], Loss: 0.0059\n",
      "Epoch [330/500], Loss: 0.0070\n",
      "Epoch [331/500], Loss: 0.0091\n",
      "Epoch [331/500], Loss: 0.0287\n",
      "Epoch [331/500], Loss: 0.2297\n",
      "Epoch [331/500], Loss: 0.0112\n",
      "Epoch [331/500], Loss: 0.0362\n",
      "Epoch [331/500], Loss: 0.0159\n",
      "Epoch [331/500], Loss: 0.1153\n",
      "Epoch [331/500], Loss: 0.0079\n",
      "Epoch [332/500], Loss: 0.0970\n",
      "Epoch [332/500], Loss: 0.0169\n",
      "Epoch [332/500], Loss: 0.0260\n",
      "Epoch [332/500], Loss: 0.0525\n",
      "Epoch [332/500], Loss: 0.0113\n",
      "Epoch [332/500], Loss: 0.0903\n",
      "Epoch [332/500], Loss: 0.0048\n",
      "Epoch [332/500], Loss: 0.0047\n",
      "Epoch [333/500], Loss: 0.0119\n",
      "Epoch [333/500], Loss: 0.0820\n",
      "Epoch [333/500], Loss: 0.0080\n",
      "Epoch [333/500], Loss: 0.1464\n",
      "Epoch [333/500], Loss: 0.0630\n",
      "Epoch [333/500], Loss: 0.0073\n",
      "Epoch [333/500], Loss: 0.0129\n",
      "Epoch [333/500], Loss: 0.0020\n",
      "Epoch [334/500], Loss: 0.1379\n",
      "Epoch [334/500], Loss: 0.0056\n",
      "Epoch [334/500], Loss: 0.1029\n",
      "Epoch [334/500], Loss: 0.0061\n",
      "Epoch [334/500], Loss: 0.1049\n",
      "Epoch [334/500], Loss: 0.0151\n",
      "Epoch [334/500], Loss: 0.0057\n",
      "Epoch [334/500], Loss: 0.0205\n",
      "Epoch [335/500], Loss: 0.0204\n",
      "Epoch [335/500], Loss: 0.1593\n",
      "Epoch [335/500], Loss: 0.0538\n",
      "Epoch [335/500], Loss: 0.0053\n",
      "Epoch [335/500], Loss: 0.0128\n",
      "Epoch [335/500], Loss: 0.0773\n",
      "Epoch [335/500], Loss: 0.0197\n",
      "Epoch [335/500], Loss: 0.0068\n",
      "Epoch [336/500], Loss: 0.0081\n",
      "Epoch [336/500], Loss: 0.0072\n",
      "Epoch [336/500], Loss: 0.0240\n",
      "Epoch [336/500], Loss: 0.0642\n",
      "Epoch [336/500], Loss: 0.0623\n",
      "Epoch [336/500], Loss: 0.1136\n",
      "Epoch [336/500], Loss: 0.0044\n",
      "Epoch [336/500], Loss: 0.0056\n",
      "Epoch [337/500], Loss: 0.0022\n",
      "Epoch [337/500], Loss: 0.0187\n",
      "Epoch [337/500], Loss: 0.0047\n",
      "Epoch [337/500], Loss: 0.0027\n",
      "Epoch [337/500], Loss: 0.1473\n",
      "Epoch [337/500], Loss: 0.0015\n",
      "Epoch [337/500], Loss: 0.0026\n",
      "Epoch [337/500], Loss: 0.5687\n",
      "Epoch [338/500], Loss: 0.0077\n",
      "Epoch [338/500], Loss: 0.0058\n",
      "Epoch [338/500], Loss: 0.1545\n",
      "Epoch [338/500], Loss: 0.0247\n",
      "Epoch [338/500], Loss: 0.1142\n",
      "Epoch [338/500], Loss: 0.0564\n",
      "Epoch [338/500], Loss: 0.0593\n",
      "Epoch [338/500], Loss: 0.1003\n",
      "Epoch [339/500], Loss: 0.1059\n",
      "Epoch [339/500], Loss: 0.0396\n",
      "Epoch [339/500], Loss: 0.0270\n",
      "Epoch [339/500], Loss: 0.0388\n",
      "Epoch [339/500], Loss: 0.0769\n",
      "Epoch [339/500], Loss: 0.0124\n",
      "Epoch [339/500], Loss: 0.2150\n",
      "Epoch [339/500], Loss: 0.0023\n",
      "Epoch [340/500], Loss: 0.0054\n",
      "Epoch [340/500], Loss: 0.0072\n",
      "Epoch [340/500], Loss: 0.0150\n",
      "Epoch [340/500], Loss: 0.1521\n",
      "Epoch [340/500], Loss: 0.1079\n",
      "Epoch [340/500], Loss: 0.1164\n",
      "Epoch [340/500], Loss: 0.0128\n",
      "Epoch [340/500], Loss: 0.0023\n",
      "Epoch [341/500], Loss: 0.2639\n",
      "Epoch [341/500], Loss: 0.0239\n",
      "Epoch [341/500], Loss: 0.0265\n",
      "Epoch [341/500], Loss: 0.0299\n",
      "Epoch [341/500], Loss: 0.0209\n",
      "Epoch [341/500], Loss: 0.1131\n",
      "Epoch [341/500], Loss: 0.0178\n",
      "Epoch [341/500], Loss: 0.0500\n",
      "Epoch [342/500], Loss: 0.0166\n",
      "Epoch [342/500], Loss: 0.0222\n",
      "Epoch [342/500], Loss: 0.0161\n",
      "Epoch [342/500], Loss: 0.0241\n",
      "Epoch [342/500], Loss: 0.2458\n",
      "Epoch [342/500], Loss: 0.1611\n",
      "Epoch [342/500], Loss: 0.0939\n",
      "Epoch [342/500], Loss: 0.0061\n",
      "Epoch [343/500], Loss: 0.1699\n",
      "Epoch [343/500], Loss: 0.0174\n",
      "Epoch [343/500], Loss: 0.0187\n",
      "Epoch [343/500], Loss: 0.0268\n",
      "Epoch [343/500], Loss: 0.0959\n",
      "Epoch [343/500], Loss: 0.0123\n",
      "Epoch [343/500], Loss: 0.0073\n",
      "Epoch [343/500], Loss: 0.0132\n",
      "Epoch [344/500], Loss: 0.2155\n",
      "Epoch [344/500], Loss: 0.0181\n",
      "Epoch [344/500], Loss: 0.0049\n",
      "Epoch [344/500], Loss: 0.0081\n",
      "Epoch [344/500], Loss: 0.0094\n",
      "Epoch [344/500], Loss: 0.0106\n",
      "Epoch [344/500], Loss: 0.0088\n",
      "Epoch [344/500], Loss: 0.2982\n",
      "Epoch [345/500], Loss: 0.0872\n",
      "Epoch [345/500], Loss: 0.1499\n",
      "Epoch [345/500], Loss: 0.0084\n",
      "Epoch [345/500], Loss: 0.0110\n",
      "Epoch [345/500], Loss: 0.1347\n",
      "Epoch [345/500], Loss: 0.0140\n",
      "Epoch [345/500], Loss: 0.0234\n",
      "Epoch [345/500], Loss: 0.0499\n",
      "Epoch [346/500], Loss: 0.1363\n",
      "Epoch [346/500], Loss: 0.0185\n",
      "Epoch [346/500], Loss: 0.0209\n",
      "Epoch [346/500], Loss: 0.1406\n",
      "Epoch [346/500], Loss: 0.0123\n",
      "Epoch [346/500], Loss: 0.0156\n",
      "Epoch [346/500], Loss: 0.1094\n",
      "Epoch [346/500], Loss: 0.0125\n",
      "Epoch [347/500], Loss: 0.0846\n",
      "Epoch [347/500], Loss: 0.0866\n",
      "Epoch [347/500], Loss: 0.0122\n",
      "Epoch [347/500], Loss: 0.0167\n",
      "Epoch [347/500], Loss: 0.0179\n",
      "Epoch [347/500], Loss: 0.0670\n",
      "Epoch [347/500], Loss: 0.0071\n",
      "Epoch [347/500], Loss: 0.0058\n",
      "Epoch [348/500], Loss: 0.0086\n",
      "Epoch [348/500], Loss: 0.0247\n",
      "Epoch [348/500], Loss: 0.0022\n",
      "Epoch [348/500], Loss: 0.0028\n",
      "Epoch [348/500], Loss: 0.0036\n",
      "Epoch [348/500], Loss: 0.3380\n",
      "Epoch [348/500], Loss: 0.3159\n",
      "Epoch [348/500], Loss: 0.0007\n",
      "Epoch [349/500], Loss: 0.0514\n",
      "Epoch [349/500], Loss: 0.0492\n",
      "Epoch [349/500], Loss: 0.0113\n",
      "Epoch [349/500], Loss: 0.0354\n",
      "Epoch [349/500], Loss: 0.0805\n",
      "Epoch [349/500], Loss: 0.0211\n",
      "Epoch [349/500], Loss: 0.1323\n",
      "Epoch [349/500], Loss: 0.0538\n",
      "Epoch [350/500], Loss: 0.0235\n",
      "Epoch [350/500], Loss: 0.1314\n",
      "Epoch [350/500], Loss: 0.1240\n",
      "Epoch [350/500], Loss: 0.0197\n",
      "Epoch [350/500], Loss: 0.1392\n",
      "Epoch [350/500], Loss: 0.0184\n",
      "Epoch [350/500], Loss: 0.0168\n",
      "Epoch [350/500], Loss: 0.0186\n",
      "Epoch [351/500], Loss: 0.0908\n",
      "Epoch [351/500], Loss: 0.0133\n",
      "Epoch [351/500], Loss: 0.1007\n",
      "Epoch [351/500], Loss: 0.0079\n",
      "Epoch [351/500], Loss: 0.2144\n",
      "Epoch [351/500], Loss: 0.0065\n",
      "Epoch [351/500], Loss: 0.0062\n",
      "Epoch [351/500], Loss: 0.0127\n",
      "Epoch [352/500], Loss: 0.0093\n",
      "Epoch [352/500], Loss: 0.0071\n",
      "Epoch [352/500], Loss: 0.0077\n",
      "Epoch [352/500], Loss: 0.0679\n",
      "Epoch [352/500], Loss: 0.0865\n",
      "Epoch [352/500], Loss: 0.0030\n",
      "Epoch [352/500], Loss: 0.1670\n",
      "Epoch [352/500], Loss: 0.0084\n",
      "Epoch [353/500], Loss: 0.1467\n",
      "Epoch [353/500], Loss: 0.0044\n",
      "Epoch [353/500], Loss: 0.0056\n",
      "Epoch [353/500], Loss: 0.0059\n",
      "Epoch [353/500], Loss: 0.0781\n",
      "Epoch [353/500], Loss: 0.0163\n",
      "Epoch [353/500], Loss: 0.0851\n",
      "Epoch [353/500], Loss: 0.0089\n",
      "Epoch [354/500], Loss: 0.0095\n",
      "Epoch [354/500], Loss: 0.0156\n",
      "Epoch [354/500], Loss: 0.0066\n",
      "Epoch [354/500], Loss: 0.1360\n",
      "Epoch [354/500], Loss: 0.0890\n",
      "Epoch [354/500], Loss: 0.0549\n",
      "Epoch [354/500], Loss: 0.0806\n",
      "Epoch [354/500], Loss: 0.0142\n",
      "Epoch [355/500], Loss: 0.1016\n",
      "Epoch [355/500], Loss: 0.0179\n",
      "Epoch [355/500], Loss: 0.0222\n",
      "Epoch [355/500], Loss: 0.1476\n",
      "Epoch [355/500], Loss: 0.0288\n",
      "Epoch [355/500], Loss: 0.0261\n",
      "Epoch [355/500], Loss: 0.0218\n",
      "Epoch [355/500], Loss: 0.0031\n",
      "Epoch [356/500], Loss: 0.0046\n",
      "Epoch [356/500], Loss: 0.1002\n",
      "Epoch [356/500], Loss: 0.0064\n",
      "Epoch [356/500], Loss: 0.0062\n",
      "Epoch [356/500], Loss: 0.0035\n",
      "Epoch [356/500], Loss: 0.0131\n",
      "Epoch [356/500], Loss: 0.1213\n",
      "Epoch [356/500], Loss: 0.1644\n",
      "Epoch [357/500], Loss: 0.0091\n",
      "Epoch [357/500], Loss: 0.1280\n",
      "Epoch [357/500], Loss: 0.0086\n",
      "Epoch [357/500], Loss: 0.0058\n",
      "Epoch [357/500], Loss: 0.1104\n",
      "Epoch [357/500], Loss: 0.1856\n",
      "Epoch [357/500], Loss: 0.0123\n",
      "Epoch [357/500], Loss: 0.2546\n",
      "Epoch [358/500], Loss: 0.0797\n",
      "Epoch [358/500], Loss: 0.0207\n",
      "Epoch [358/500], Loss: 0.1589\n",
      "Epoch [358/500], Loss: 0.0796\n",
      "Epoch [358/500], Loss: 0.0752\n",
      "Epoch [358/500], Loss: 0.0306\n",
      "Epoch [358/500], Loss: 0.0201\n",
      "Epoch [358/500], Loss: 0.0304\n",
      "Epoch [359/500], Loss: 0.0280\n",
      "Epoch [359/500], Loss: 0.0172\n",
      "Epoch [359/500], Loss: 0.0059\n",
      "Epoch [359/500], Loss: 0.1836\n",
      "Epoch [359/500], Loss: 0.0116\n",
      "Epoch [359/500], Loss: 0.0044\n",
      "Epoch [359/500], Loss: 0.0041\n",
      "Epoch [359/500], Loss: 0.8588\n",
      "Epoch [360/500], Loss: 0.0925\n",
      "Epoch [360/500], Loss: 0.0223\n",
      "Epoch [360/500], Loss: 0.0219\n",
      "Epoch [360/500], Loss: 0.0487\n",
      "Epoch [360/500], Loss: 0.0873\n",
      "Epoch [360/500], Loss: 0.1318\n",
      "Epoch [360/500], Loss: 0.0979\n",
      "Epoch [360/500], Loss: 0.1157\n",
      "Epoch [361/500], Loss: 0.1566\n",
      "Epoch [361/500], Loss: 0.0517\n",
      "Epoch [361/500], Loss: 0.0647\n",
      "Epoch [361/500], Loss: 0.0411\n",
      "Epoch [361/500], Loss: 0.0230\n",
      "Epoch [361/500], Loss: 0.0515\n",
      "Epoch [361/500], Loss: 0.1711\n",
      "Epoch [361/500], Loss: 0.2867\n",
      "Epoch [362/500], Loss: 0.0084\n",
      "Epoch [362/500], Loss: 0.0543\n",
      "Epoch [362/500], Loss: 0.0116\n",
      "Epoch [362/500], Loss: 0.0171\n",
      "Epoch [362/500], Loss: 0.0096\n",
      "Epoch [362/500], Loss: 0.0089\n",
      "Epoch [362/500], Loss: 0.2634\n",
      "Epoch [362/500], Loss: 0.0085\n",
      "Epoch [363/500], Loss: 0.2313\n",
      "Epoch [363/500], Loss: 0.0044\n",
      "Epoch [363/500], Loss: 0.1182\n",
      "Epoch [363/500], Loss: 0.0070\n",
      "Epoch [363/500], Loss: 0.1122\n",
      "Epoch [363/500], Loss: 0.0343\n",
      "Epoch [363/500], Loss: 0.1007\n",
      "Epoch [363/500], Loss: 0.0525\n",
      "Epoch [364/500], Loss: 0.0297\n",
      "Epoch [364/500], Loss: 0.1165\n",
      "Epoch [364/500], Loss: 0.0549\n",
      "Epoch [364/500], Loss: 0.0343\n",
      "Epoch [364/500], Loss: 0.0662\n",
      "Epoch [364/500], Loss: 0.0561\n",
      "Epoch [364/500], Loss: 0.0814\n",
      "Epoch [364/500], Loss: 0.1960\n",
      "Epoch [365/500], Loss: 0.0103\n",
      "Epoch [365/500], Loss: 0.0066\n",
      "Epoch [365/500], Loss: 0.0305\n",
      "Epoch [365/500], Loss: 0.1480\n",
      "Epoch [365/500], Loss: 0.1189\n",
      "Epoch [365/500], Loss: 0.0733\n",
      "Epoch [365/500], Loss: 0.0072\n",
      "Epoch [365/500], Loss: 0.0019\n",
      "Epoch [366/500], Loss: 0.0121\n",
      "Epoch [366/500], Loss: 0.0037\n",
      "Epoch [366/500], Loss: 0.1486\n",
      "Epoch [366/500], Loss: 0.0252\n",
      "Epoch [366/500], Loss: 0.0087\n",
      "Epoch [366/500], Loss: 0.0648\n",
      "Epoch [366/500], Loss: 0.1300\n",
      "Epoch [366/500], Loss: 0.0045\n",
      "Epoch [367/500], Loss: 0.0112\n",
      "Epoch [367/500], Loss: 0.0144\n",
      "Epoch [367/500], Loss: 0.0342\n",
      "Epoch [367/500], Loss: 0.0858\n",
      "Epoch [367/500], Loss: 0.1615\n",
      "Epoch [367/500], Loss: 0.0628\n",
      "Epoch [367/500], Loss: 0.0136\n",
      "Epoch [367/500], Loss: 0.0042\n",
      "Epoch [368/500], Loss: 0.0956\n",
      "Epoch [368/500], Loss: 0.0112\n",
      "Epoch [368/500], Loss: 0.0685\n",
      "Epoch [368/500], Loss: 0.0513\n",
      "Epoch [368/500], Loss: 0.0080\n",
      "Epoch [368/500], Loss: 0.0135\n",
      "Epoch [368/500], Loss: 0.0061\n",
      "Epoch [368/500], Loss: 0.0013\n",
      "Epoch [369/500], Loss: 0.0039\n",
      "Epoch [369/500], Loss: 0.0105\n",
      "Epoch [369/500], Loss: 0.1233\n",
      "Epoch [369/500], Loss: 0.1675\n",
      "Epoch [369/500], Loss: 0.0867\n",
      "Epoch [369/500], Loss: 0.0942\n",
      "Epoch [369/500], Loss: 0.0095\n",
      "Epoch [369/500], Loss: 0.0246\n",
      "Epoch [370/500], Loss: 0.0379\n",
      "Epoch [370/500], Loss: 0.0651\n",
      "Epoch [370/500], Loss: 0.0129\n",
      "Epoch [370/500], Loss: 0.0191\n",
      "Epoch [370/500], Loss: 0.0229\n",
      "Epoch [370/500], Loss: 0.0219\n",
      "Epoch [370/500], Loss: 0.0989\n",
      "Epoch [370/500], Loss: 0.2049\n",
      "Epoch [371/500], Loss: 0.0053\n",
      "Epoch [371/500], Loss: 0.0189\n",
      "Epoch [371/500], Loss: 0.0288\n",
      "Epoch [371/500], Loss: 0.1129\n",
      "Epoch [371/500], Loss: 0.0340\n",
      "Epoch [371/500], Loss: 0.0134\n",
      "Epoch [371/500], Loss: 0.1132\n",
      "Epoch [371/500], Loss: 0.0192\n",
      "Epoch [372/500], Loss: 0.1419\n",
      "Epoch [372/500], Loss: 0.0072\n",
      "Epoch [372/500], Loss: 0.0124\n",
      "Epoch [372/500], Loss: 0.1168\n",
      "Epoch [372/500], Loss: 0.0054\n",
      "Epoch [372/500], Loss: 0.0063\n",
      "Epoch [372/500], Loss: 0.0769\n",
      "Epoch [372/500], Loss: 0.0024\n",
      "Epoch [373/500], Loss: 0.0046\n",
      "Epoch [373/500], Loss: 0.0105\n",
      "Epoch [373/500], Loss: 0.0081\n",
      "Epoch [373/500], Loss: 0.0436\n",
      "Epoch [373/500], Loss: 0.0399\n",
      "Epoch [373/500], Loss: 0.0009\n",
      "Epoch [373/500], Loss: 0.0074\n",
      "Epoch [373/500], Loss: 0.6296\n",
      "Epoch [374/500], Loss: 0.1046\n",
      "Epoch [374/500], Loss: 0.0038\n",
      "Epoch [374/500], Loss: 0.0523\n",
      "Epoch [374/500], Loss: 0.0873\n",
      "Epoch [374/500], Loss: 0.0734\n",
      "Epoch [374/500], Loss: 0.0151\n",
      "Epoch [374/500], Loss: 0.0298\n",
      "Epoch [374/500], Loss: 0.0083\n",
      "Epoch [375/500], Loss: 0.1882\n",
      "Epoch [375/500], Loss: 0.0126\n",
      "Epoch [375/500], Loss: 0.0333\n",
      "Epoch [375/500], Loss: 0.0722\n",
      "Epoch [375/500], Loss: 0.0379\n",
      "Epoch [375/500], Loss: 0.0214\n",
      "Epoch [375/500], Loss: 0.0176\n",
      "Epoch [375/500], Loss: 0.0134\n",
      "Epoch [376/500], Loss: 0.0112\n",
      "Epoch [376/500], Loss: 0.0059\n",
      "Epoch [376/500], Loss: 0.0015\n",
      "Epoch [376/500], Loss: 0.1544\n",
      "Epoch [376/500], Loss: 0.0122\n",
      "Epoch [376/500], Loss: 0.1494\n",
      "Epoch [376/500], Loss: 0.1586\n",
      "Epoch [376/500], Loss: 0.0020\n",
      "Epoch [377/500], Loss: 0.0894\n",
      "Epoch [377/500], Loss: 0.0143\n",
      "Epoch [377/500], Loss: 0.0147\n",
      "Epoch [377/500], Loss: 0.0708\n",
      "Epoch [377/500], Loss: 0.0165\n",
      "Epoch [377/500], Loss: 0.0268\n",
      "Epoch [377/500], Loss: 0.1261\n",
      "Epoch [377/500], Loss: 0.0060\n",
      "Epoch [378/500], Loss: 0.0119\n",
      "Epoch [378/500], Loss: 0.0946\n",
      "Epoch [378/500], Loss: 0.0131\n",
      "Epoch [378/500], Loss: 0.0284\n",
      "Epoch [378/500], Loss: 0.0130\n",
      "Epoch [378/500], Loss: 0.0598\n",
      "Epoch [378/500], Loss: 0.0905\n",
      "Epoch [378/500], Loss: 0.0140\n",
      "Epoch [379/500], Loss: 0.1463\n",
      "Epoch [379/500], Loss: 0.0904\n",
      "Epoch [379/500], Loss: 0.0454\n",
      "Epoch [379/500], Loss: 0.0125\n",
      "Epoch [379/500], Loss: 0.0107\n",
      "Epoch [379/500], Loss: 0.0071\n",
      "Epoch [379/500], Loss: 0.0811\n",
      "Epoch [379/500], Loss: 0.0582\n",
      "Epoch [380/500], Loss: 0.0065\n",
      "Epoch [380/500], Loss: 0.0868\n",
      "Epoch [380/500], Loss: 0.1513\n",
      "Epoch [380/500], Loss: 0.0126\n",
      "Epoch [380/500], Loss: 0.0076\n",
      "Epoch [380/500], Loss: 0.0073\n",
      "Epoch [380/500], Loss: 0.0063\n",
      "Epoch [380/500], Loss: 0.0183\n",
      "Epoch [381/500], Loss: 0.0043\n",
      "Epoch [381/500], Loss: 0.2488\n",
      "Epoch [381/500], Loss: 0.1093\n",
      "Epoch [381/500], Loss: 0.0135\n",
      "Epoch [381/500], Loss: 0.0139\n",
      "Epoch [381/500], Loss: 0.0090\n",
      "Epoch [381/500], Loss: 0.0041\n",
      "Epoch [381/500], Loss: 0.0043\n",
      "Epoch [382/500], Loss: 0.0113\n",
      "Epoch [382/500], Loss: 0.1152\n",
      "Epoch [382/500], Loss: 0.0138\n",
      "Epoch [382/500], Loss: 0.1489\n",
      "Epoch [382/500], Loss: 0.1107\n",
      "Epoch [382/500], Loss: 0.0078\n",
      "Epoch [382/500], Loss: 0.0109\n",
      "Epoch [382/500], Loss: 0.0059\n",
      "Epoch [383/500], Loss: 0.0150\n",
      "Epoch [383/500], Loss: 0.0108\n",
      "Epoch [383/500], Loss: 0.1144\n",
      "Epoch [383/500], Loss: 0.0144\n",
      "Epoch [383/500], Loss: 0.1661\n",
      "Epoch [383/500], Loss: 0.0063\n",
      "Epoch [383/500], Loss: 0.0118\n",
      "Epoch [383/500], Loss: 0.1838\n",
      "Epoch [384/500], Loss: 0.0070\n",
      "Epoch [384/500], Loss: 0.1079\n",
      "Epoch [384/500], Loss: 0.1906\n",
      "Epoch [384/500], Loss: 0.0113\n",
      "Epoch [384/500], Loss: 0.0575\n",
      "Epoch [384/500], Loss: 0.0206\n",
      "Epoch [384/500], Loss: 0.0503\n",
      "Epoch [384/500], Loss: 0.2718\n",
      "Epoch [385/500], Loss: 0.2034\n",
      "Epoch [385/500], Loss: 0.0156\n",
      "Epoch [385/500], Loss: 0.0139\n",
      "Epoch [385/500], Loss: 0.0236\n",
      "Epoch [385/500], Loss: 0.0106\n",
      "Epoch [385/500], Loss: 0.0149\n",
      "Epoch [385/500], Loss: 0.0469\n",
      "Epoch [385/500], Loss: 0.0113\n",
      "Epoch [386/500], Loss: 0.0246\n",
      "Epoch [386/500], Loss: 0.0089\n",
      "Epoch [386/500], Loss: 0.0402\n",
      "Epoch [386/500], Loss: 0.0195\n",
      "Epoch [386/500], Loss: 0.0765\n",
      "Epoch [386/500], Loss: 0.0189\n",
      "Epoch [386/500], Loss: 0.1400\n",
      "Epoch [386/500], Loss: 0.0048\n",
      "Epoch [387/500], Loss: 0.1069\n",
      "Epoch [387/500], Loss: 0.1451\n",
      "Epoch [387/500], Loss: 0.0045\n",
      "Epoch [387/500], Loss: 0.0129\n",
      "Epoch [387/500], Loss: 0.0064\n",
      "Epoch [387/500], Loss: 0.0036\n",
      "Epoch [387/500], Loss: 0.0149\n",
      "Epoch [387/500], Loss: 0.0106\n",
      "Epoch [388/500], Loss: 0.0218\n",
      "Epoch [388/500], Loss: 0.1338\n",
      "Epoch [388/500], Loss: 0.0109\n",
      "Epoch [388/500], Loss: 0.0062\n",
      "Epoch [388/500], Loss: 0.0551\n",
      "Epoch [388/500], Loss: 0.0156\n",
      "Epoch [388/500], Loss: 0.0416\n",
      "Epoch [388/500], Loss: 0.0088\n",
      "Epoch [389/500], Loss: 0.0566\n",
      "Epoch [389/500], Loss: 0.0044\n",
      "Epoch [389/500], Loss: 0.0524\n",
      "Epoch [389/500], Loss: 0.0106\n",
      "Epoch [389/500], Loss: 0.0099\n",
      "Epoch [389/500], Loss: 0.1156\n",
      "Epoch [389/500], Loss: 0.0516\n",
      "Epoch [389/500], Loss: 0.0034\n",
      "Epoch [390/500], Loss: 0.0287\n",
      "Epoch [390/500], Loss: 0.0052\n",
      "Epoch [390/500], Loss: 0.0087\n",
      "Epoch [390/500], Loss: 0.1500\n",
      "Epoch [390/500], Loss: 0.1254\n",
      "Epoch [390/500], Loss: 0.0045\n",
      "Epoch [390/500], Loss: 0.1560\n",
      "Epoch [390/500], Loss: 0.0014\n",
      "Epoch [391/500], Loss: 0.0139\n",
      "Epoch [391/500], Loss: 0.0141\n",
      "Epoch [391/500], Loss: 0.0133\n",
      "Epoch [391/500], Loss: 0.0288\n",
      "Epoch [391/500], Loss: 0.0136\n",
      "Epoch [391/500], Loss: 0.1666\n",
      "Epoch [391/500], Loss: 0.1524\n",
      "Epoch [391/500], Loss: 0.0055\n",
      "Epoch [392/500], Loss: 0.0207\n",
      "Epoch [392/500], Loss: 0.0483\n",
      "Epoch [392/500], Loss: 0.0295\n",
      "Epoch [392/500], Loss: 0.0492\n",
      "Epoch [392/500], Loss: 0.0141\n",
      "Epoch [392/500], Loss: 0.0469\n",
      "Epoch [392/500], Loss: 0.0656\n",
      "Epoch [392/500], Loss: 0.0018\n",
      "Epoch [393/500], Loss: 0.0065\n",
      "Epoch [393/500], Loss: 0.0118\n",
      "Epoch [393/500], Loss: 0.0120\n",
      "Epoch [393/500], Loss: 0.0023\n",
      "Epoch [393/500], Loss: 0.0774\n",
      "Epoch [393/500], Loss: 0.0088\n",
      "Epoch [393/500], Loss: 0.1083\n",
      "Epoch [393/500], Loss: 0.0037\n",
      "Epoch [394/500], Loss: 0.0026\n",
      "Epoch [394/500], Loss: 0.1454\n",
      "Epoch [394/500], Loss: 0.0051\n",
      "Epoch [394/500], Loss: 0.1131\n",
      "Epoch [394/500], Loss: 0.0041\n",
      "Epoch [394/500], Loss: 0.0396\n",
      "Epoch [394/500], Loss: 0.0108\n",
      "Epoch [394/500], Loss: 0.0139\n",
      "Epoch [395/500], Loss: 0.0065\n",
      "Epoch [395/500], Loss: 0.0151\n",
      "Epoch [395/500], Loss: 0.0073\n",
      "Epoch [395/500], Loss: 0.0404\n",
      "Epoch [395/500], Loss: 0.0360\n",
      "Epoch [395/500], Loss: 0.0047\n",
      "Epoch [395/500], Loss: 0.1099\n",
      "Epoch [395/500], Loss: 0.0069\n",
      "Epoch [396/500], Loss: 0.0110\n",
      "Epoch [396/500], Loss: 0.0046\n",
      "Epoch [396/500], Loss: 0.0699\n",
      "Epoch [396/500], Loss: 0.0341\n",
      "Epoch [396/500], Loss: 0.0058\n",
      "Epoch [396/500], Loss: 0.0160\n",
      "Epoch [396/500], Loss: 0.0038\n",
      "Epoch [396/500], Loss: 0.3262\n",
      "Epoch [397/500], Loss: 0.0033\n",
      "Epoch [397/500], Loss: 0.0270\n",
      "Epoch [397/500], Loss: 0.0648\n",
      "Epoch [397/500], Loss: 0.0524\n",
      "Epoch [397/500], Loss: 0.0140\n",
      "Epoch [397/500], Loss: 0.0854\n",
      "Epoch [397/500], Loss: 0.0336\n",
      "Epoch [397/500], Loss: 0.0102\n",
      "Epoch [398/500], Loss: 0.0232\n",
      "Epoch [398/500], Loss: 0.0318\n",
      "Epoch [398/500], Loss: 0.0186\n",
      "Epoch [398/500], Loss: 0.0028\n",
      "Epoch [398/500], Loss: 0.0039\n",
      "Epoch [398/500], Loss: 0.1057\n",
      "Epoch [398/500], Loss: 0.1097\n",
      "Epoch [398/500], Loss: 0.0066\n",
      "Epoch [399/500], Loss: 0.0039\n",
      "Epoch [399/500], Loss: 0.0106\n",
      "Epoch [399/500], Loss: 0.0029\n",
      "Epoch [399/500], Loss: 0.0758\n",
      "Epoch [399/500], Loss: 0.0038\n",
      "Epoch [399/500], Loss: 0.1061\n",
      "Epoch [399/500], Loss: 0.2189\n",
      "Epoch [399/500], Loss: 0.0318\n",
      "Epoch [400/500], Loss: 0.0175\n",
      "Epoch [400/500], Loss: 0.0362\n",
      "Epoch [400/500], Loss: 0.0108\n",
      "Epoch [400/500], Loss: 0.0559\n",
      "Epoch [400/500], Loss: 0.0237\n",
      "Epoch [400/500], Loss: 0.0394\n",
      "Epoch [400/500], Loss: 0.0848\n",
      "Epoch [400/500], Loss: 0.0293\n",
      "Epoch [401/500], Loss: 0.0791\n",
      "Epoch [401/500], Loss: 0.0304\n",
      "Epoch [401/500], Loss: 0.0227\n",
      "Epoch [401/500], Loss: 0.0108\n",
      "Epoch [401/500], Loss: 0.0077\n",
      "Epoch [401/500], Loss: 0.0236\n",
      "Epoch [401/500], Loss: 0.0032\n",
      "Epoch [401/500], Loss: 0.3437\n",
      "Epoch [402/500], Loss: 0.0259\n",
      "Epoch [402/500], Loss: 0.0551\n",
      "Epoch [402/500], Loss: 0.1327\n",
      "Epoch [402/500], Loss: 0.0145\n",
      "Epoch [402/500], Loss: 0.0215\n",
      "Epoch [402/500], Loss: 0.0086\n",
      "Epoch [402/500], Loss: 0.0129\n",
      "Epoch [402/500], Loss: 0.0179\n",
      "Epoch [403/500], Loss: 0.0428\n",
      "Epoch [403/500], Loss: 0.0057\n",
      "Epoch [403/500], Loss: 0.0035\n",
      "Epoch [403/500], Loss: 0.0320\n",
      "Epoch [403/500], Loss: 0.0620\n",
      "Epoch [403/500], Loss: 0.0588\n",
      "Epoch [403/500], Loss: 0.1715\n",
      "Epoch [403/500], Loss: 0.0009\n",
      "Epoch [404/500], Loss: 0.0416\n",
      "Epoch [404/500], Loss: 0.0191\n",
      "Epoch [404/500], Loss: 0.0044\n",
      "Epoch [404/500], Loss: 0.0049\n",
      "Epoch [404/500], Loss: 0.0548\n",
      "Epoch [404/500], Loss: 0.0324\n",
      "Epoch [404/500], Loss: 0.0358\n",
      "Epoch [404/500], Loss: 0.0796\n",
      "Epoch [405/500], Loss: 0.0190\n",
      "Epoch [405/500], Loss: 0.0836\n",
      "Epoch [405/500], Loss: 0.0680\n",
      "Epoch [405/500], Loss: 0.0339\n",
      "Epoch [405/500], Loss: 0.0266\n",
      "Epoch [405/500], Loss: 0.0172\n",
      "Epoch [405/500], Loss: 0.0159\n",
      "Epoch [405/500], Loss: 0.0069\n",
      "Epoch [406/500], Loss: 0.0764\n",
      "Epoch [406/500], Loss: 0.0114\n",
      "Epoch [406/500], Loss: 0.0573\n",
      "Epoch [406/500], Loss: 0.0082\n",
      "Epoch [406/500], Loss: 0.0229\n",
      "Epoch [406/500], Loss: 0.0079\n",
      "Epoch [406/500], Loss: 0.0630\n",
      "Epoch [406/500], Loss: 0.0007\n",
      "Epoch [407/500], Loss: 0.0028\n",
      "Epoch [407/500], Loss: 0.0144\n",
      "Epoch [407/500], Loss: 0.1259\n",
      "Epoch [407/500], Loss: 0.0703\n",
      "Epoch [407/500], Loss: 0.0021\n",
      "Epoch [407/500], Loss: 0.1750\n",
      "Epoch [407/500], Loss: 0.0908\n",
      "Epoch [407/500], Loss: 0.0060\n",
      "Epoch [408/500], Loss: 0.1064\n",
      "Epoch [408/500], Loss: 0.1471\n",
      "Epoch [408/500], Loss: 0.0624\n",
      "Epoch [408/500], Loss: 0.0508\n",
      "Epoch [408/500], Loss: 0.0589\n",
      "Epoch [408/500], Loss: 0.0176\n",
      "Epoch [408/500], Loss: 0.0878\n",
      "Epoch [408/500], Loss: 0.0067\n",
      "Epoch [409/500], Loss: 0.0173\n",
      "Epoch [409/500], Loss: 0.0076\n",
      "Epoch [409/500], Loss: 0.0185\n",
      "Epoch [409/500], Loss: 0.0841\n",
      "Epoch [409/500], Loss: 0.1161\n",
      "Epoch [409/500], Loss: 0.1178\n",
      "Epoch [409/500], Loss: 0.0044\n",
      "Epoch [409/500], Loss: 0.0007\n",
      "Epoch [410/500], Loss: 0.0852\n",
      "Epoch [410/500], Loss: 0.0988\n",
      "Epoch [410/500], Loss: 0.0094\n",
      "Epoch [410/500], Loss: 0.0095\n",
      "Epoch [410/500], Loss: 0.1285\n",
      "Epoch [410/500], Loss: 0.0026\n",
      "Epoch [410/500], Loss: 0.0147\n",
      "Epoch [410/500], Loss: 0.0247\n",
      "Epoch [411/500], Loss: 0.1363\n",
      "Epoch [411/500], Loss: 0.0105\n",
      "Epoch [411/500], Loss: 0.0105\n",
      "Epoch [411/500], Loss: 0.1657\n",
      "Epoch [411/500], Loss: 0.0188\n",
      "Epoch [411/500], Loss: 0.0069\n",
      "Epoch [411/500], Loss: 0.0200\n",
      "Epoch [411/500], Loss: 0.0188\n",
      "Epoch [412/500], Loss: 0.2337\n",
      "Epoch [412/500], Loss: 0.0092\n",
      "Epoch [412/500], Loss: 0.0195\n",
      "Epoch [412/500], Loss: 0.0105\n",
      "Epoch [412/500], Loss: 0.0420\n",
      "Epoch [412/500], Loss: 0.0197\n",
      "Epoch [412/500], Loss: 0.0057\n",
      "Epoch [412/500], Loss: 0.0099\n",
      "Epoch [413/500], Loss: 0.2649\n",
      "Epoch [413/500], Loss: 0.0621\n",
      "Epoch [413/500], Loss: 0.0197\n",
      "Epoch [413/500], Loss: 0.1291\n",
      "Epoch [413/500], Loss: 0.0085\n",
      "Epoch [413/500], Loss: 0.0537\n",
      "Epoch [413/500], Loss: 0.0115\n",
      "Epoch [413/500], Loss: 0.0036\n",
      "Epoch [414/500], Loss: 0.1165\n",
      "Epoch [414/500], Loss: 0.0773\n",
      "Epoch [414/500], Loss: 0.0068\n",
      "Epoch [414/500], Loss: 0.0646\n",
      "Epoch [414/500], Loss: 0.0170\n",
      "Epoch [414/500], Loss: 0.0203\n",
      "Epoch [414/500], Loss: 0.1282\n",
      "Epoch [414/500], Loss: 0.0067\n",
      "Epoch [415/500], Loss: 0.0081\n",
      "Epoch [415/500], Loss: 0.0132\n",
      "Epoch [415/500], Loss: 0.0153\n",
      "Epoch [415/500], Loss: 0.0140\n",
      "Epoch [415/500], Loss: 0.1297\n",
      "Epoch [415/500], Loss: 0.0545\n",
      "Epoch [415/500], Loss: 0.0169\n",
      "Epoch [415/500], Loss: 0.1379\n",
      "Epoch [416/500], Loss: 0.0125\n",
      "Epoch [416/500], Loss: 0.0272\n",
      "Epoch [416/500], Loss: 0.0167\n",
      "Epoch [416/500], Loss: 0.1147\n",
      "Epoch [416/500], Loss: 0.0225\n",
      "Epoch [416/500], Loss: 0.1098\n",
      "Epoch [416/500], Loss: 0.0778\n",
      "Epoch [416/500], Loss: 0.0018\n",
      "Epoch [417/500], Loss: 0.0961\n",
      "Epoch [417/500], Loss: 0.0129\n",
      "Epoch [417/500], Loss: 0.0543\n",
      "Epoch [417/500], Loss: 0.0276\n",
      "Epoch [417/500], Loss: 0.0074\n",
      "Epoch [417/500], Loss: 0.0109\n",
      "Epoch [417/500], Loss: 0.0015\n",
      "Epoch [417/500], Loss: 0.0001\n",
      "Epoch [418/500], Loss: 0.0053\n",
      "Epoch [418/500], Loss: 0.1609\n",
      "Epoch [418/500], Loss: 0.0033\n",
      "Epoch [418/500], Loss: 0.0046\n",
      "Epoch [418/500], Loss: 0.1713\n",
      "Epoch [418/500], Loss: 0.0120\n",
      "Epoch [418/500], Loss: 0.0091\n",
      "Epoch [418/500], Loss: 0.0029\n",
      "Epoch [419/500], Loss: 0.0926\n",
      "Epoch [419/500], Loss: 0.0059\n",
      "Epoch [419/500], Loss: 0.0093\n",
      "Epoch [419/500], Loss: 0.0790\n",
      "Epoch [419/500], Loss: 0.0052\n",
      "Epoch [419/500], Loss: 0.0640\n",
      "Epoch [419/500], Loss: 0.0225\n",
      "Epoch [419/500], Loss: 0.0054\n",
      "Epoch [420/500], Loss: 0.0097\n",
      "Epoch [420/500], Loss: 0.0156\n",
      "Epoch [420/500], Loss: 0.0416\n",
      "Epoch [420/500], Loss: 0.0086\n",
      "Epoch [420/500], Loss: 0.0105\n",
      "Epoch [420/500], Loss: 0.0152\n",
      "Epoch [420/500], Loss: 0.2150\n",
      "Epoch [420/500], Loss: 0.0025\n",
      "Epoch [421/500], Loss: 0.0081\n",
      "Epoch [421/500], Loss: 0.0189\n",
      "Epoch [421/500], Loss: 0.0014\n",
      "Epoch [421/500], Loss: 0.0073\n",
      "Epoch [421/500], Loss: 0.0386\n",
      "Epoch [421/500], Loss: 0.0021\n",
      "Epoch [421/500], Loss: 0.0931\n",
      "Epoch [421/500], Loss: 0.0077\n",
      "Epoch [422/500], Loss: 0.0016\n",
      "Epoch [422/500], Loss: 0.0022\n",
      "Epoch [422/500], Loss: 0.0432\n",
      "Epoch [422/500], Loss: 0.0028\n",
      "Epoch [422/500], Loss: 0.0910\n",
      "Epoch [422/500], Loss: 0.0038\n",
      "Epoch [422/500], Loss: 0.0105\n",
      "Epoch [422/500], Loss: 0.2117\n",
      "Epoch [423/500], Loss: 0.0418\n",
      "Epoch [423/500], Loss: 0.0770\n",
      "Epoch [423/500], Loss: 0.0092\n",
      "Epoch [423/500], Loss: 0.0087\n",
      "Epoch [423/500], Loss: 0.0526\n",
      "Epoch [423/500], Loss: 0.0084\n",
      "Epoch [423/500], Loss: 0.0041\n",
      "Epoch [423/500], Loss: 0.3828\n",
      "Epoch [424/500], Loss: 0.0052\n",
      "Epoch [424/500], Loss: 0.0416\n",
      "Epoch [424/500], Loss: 0.1232\n",
      "Epoch [424/500], Loss: 0.0229\n",
      "Epoch [424/500], Loss: 0.0149\n",
      "Epoch [424/500], Loss: 0.0282\n",
      "Epoch [424/500], Loss: 0.0635\n",
      "Epoch [424/500], Loss: 0.0241\n",
      "Epoch [425/500], Loss: 0.0472\n",
      "Epoch [425/500], Loss: 0.0622\n",
      "Epoch [425/500], Loss: 0.0204\n",
      "Epoch [425/500], Loss: 0.0074\n",
      "Epoch [425/500], Loss: 0.2229\n",
      "Epoch [425/500], Loss: 0.0011\n",
      "Epoch [425/500], Loss: 0.1100\n",
      "Epoch [425/500], Loss: 0.0104\n",
      "Epoch [426/500], Loss: 0.0144\n",
      "Epoch [426/500], Loss: 0.0021\n",
      "Epoch [426/500], Loss: 0.0016\n",
      "Epoch [426/500], Loss: 0.0008\n",
      "Epoch [426/500], Loss: 0.0913\n",
      "Epoch [426/500], Loss: 0.0788\n",
      "Epoch [426/500], Loss: 0.0044\n",
      "Epoch [426/500], Loss: 0.0003\n",
      "Epoch [427/500], Loss: 0.0037\n",
      "Epoch [427/500], Loss: 0.0024\n",
      "Epoch [427/500], Loss: 0.0343\n",
      "Epoch [427/500], Loss: 0.1168\n",
      "Epoch [427/500], Loss: 0.0231\n",
      "Epoch [427/500], Loss: 0.0966\n",
      "Epoch [427/500], Loss: 0.0170\n",
      "Epoch [427/500], Loss: 0.0959\n",
      "Epoch [428/500], Loss: 0.0183\n",
      "Epoch [428/500], Loss: 0.0932\n",
      "Epoch [428/500], Loss: 0.0336\n",
      "Epoch [428/500], Loss: 0.1335\n",
      "Epoch [428/500], Loss: 0.0121\n",
      "Epoch [428/500], Loss: 0.0177\n",
      "Epoch [428/500], Loss: 0.0675\n",
      "Epoch [428/500], Loss: 0.0554\n",
      "Epoch [429/500], Loss: 0.0033\n",
      "Epoch [429/500], Loss: 0.2819\n",
      "Epoch [429/500], Loss: 0.0025\n",
      "Epoch [429/500], Loss: 0.1927\n",
      "Epoch [429/500], Loss: 0.0090\n",
      "Epoch [429/500], Loss: 0.0191\n",
      "Epoch [429/500], Loss: 0.0822\n",
      "Epoch [429/500], Loss: 0.0263\n",
      "Epoch [430/500], Loss: 0.0396\n",
      "Epoch [430/500], Loss: 0.0054\n",
      "Epoch [430/500], Loss: 0.0526\n",
      "Epoch [430/500], Loss: 0.0489\n",
      "Epoch [430/500], Loss: 0.0867\n",
      "Epoch [430/500], Loss: 0.0901\n",
      "Epoch [430/500], Loss: 0.0187\n",
      "Epoch [430/500], Loss: 0.0204\n",
      "Epoch [431/500], Loss: 0.0309\n",
      "Epoch [431/500], Loss: 0.0368\n",
      "Epoch [431/500], Loss: 0.0472\n",
      "Epoch [431/500], Loss: 0.0223\n",
      "Epoch [431/500], Loss: 0.0284\n",
      "Epoch [431/500], Loss: 0.0243\n",
      "Epoch [431/500], Loss: 0.0667\n",
      "Epoch [431/500], Loss: 0.0012\n",
      "Epoch [432/500], Loss: 0.1741\n",
      "Epoch [432/500], Loss: 0.1920\n",
      "Epoch [432/500], Loss: 0.1320\n",
      "Epoch [432/500], Loss: 0.0044\n",
      "Epoch [432/500], Loss: 0.0156\n",
      "Epoch [432/500], Loss: 0.0050\n",
      "Epoch [432/500], Loss: 0.0111\n",
      "Epoch [432/500], Loss: 0.0963\n",
      "Epoch [433/500], Loss: 0.0065\n",
      "Epoch [433/500], Loss: 0.0244\n",
      "Epoch [433/500], Loss: 0.0259\n",
      "Epoch [433/500], Loss: 0.0771\n",
      "Epoch [433/500], Loss: 0.0083\n",
      "Epoch [433/500], Loss: 0.0236\n",
      "Epoch [433/500], Loss: 0.0184\n",
      "Epoch [433/500], Loss: 0.1619\n",
      "Epoch [434/500], Loss: 0.0131\n",
      "Epoch [434/500], Loss: 0.0046\n",
      "Epoch [434/500], Loss: 0.0834\n",
      "Epoch [434/500], Loss: 0.0311\n",
      "Epoch [434/500], Loss: 0.0031\n",
      "Epoch [434/500], Loss: 0.0491\n",
      "Epoch [434/500], Loss: 0.0839\n",
      "Epoch [434/500], Loss: 0.4768\n",
      "Epoch [435/500], Loss: 0.0779\n",
      "Epoch [435/500], Loss: 0.1427\n",
      "Epoch [435/500], Loss: 0.0201\n",
      "Epoch [435/500], Loss: 0.0439\n",
      "Epoch [435/500], Loss: 0.0647\n",
      "Epoch [435/500], Loss: 0.0297\n",
      "Epoch [435/500], Loss: 0.1368\n",
      "Epoch [435/500], Loss: 0.0121\n",
      "Epoch [436/500], Loss: 0.0405\n",
      "Epoch [436/500], Loss: 0.1351\n",
      "Epoch [436/500], Loss: 0.0482\n",
      "Epoch [436/500], Loss: 0.0101\n",
      "Epoch [436/500], Loss: 0.0101\n",
      "Epoch [436/500], Loss: 0.0043\n",
      "Epoch [436/500], Loss: 0.0114\n",
      "Epoch [436/500], Loss: 0.0002\n",
      "Epoch [437/500], Loss: 0.2125\n",
      "Epoch [437/500], Loss: 0.0016\n",
      "Epoch [437/500], Loss: 0.0019\n",
      "Epoch [437/500], Loss: 0.0025\n",
      "Epoch [437/500], Loss: 0.1236\n",
      "Epoch [437/500], Loss: 0.0024\n",
      "Epoch [437/500], Loss: 0.0018\n",
      "Epoch [437/500], Loss: 0.0064\n",
      "Epoch [438/500], Loss: 0.0133\n",
      "Epoch [438/500], Loss: 0.1506\n",
      "Epoch [438/500], Loss: 0.0442\n",
      "Epoch [438/500], Loss: 0.0134\n",
      "Epoch [438/500], Loss: 0.0220\n",
      "Epoch [438/500], Loss: 0.0062\n",
      "Epoch [438/500], Loss: 0.0768\n",
      "Epoch [438/500], Loss: 0.0019\n",
      "Epoch [439/500], Loss: 0.0259\n",
      "Epoch [439/500], Loss: 0.0112\n",
      "Epoch [439/500], Loss: 0.0517\n",
      "Epoch [439/500], Loss: 0.0018\n",
      "Epoch [439/500], Loss: 0.0050\n",
      "Epoch [439/500], Loss: 0.2127\n",
      "Epoch [439/500], Loss: 0.0182\n",
      "Epoch [439/500], Loss: 0.0025\n",
      "Epoch [440/500], Loss: 0.0968\n",
      "Epoch [440/500], Loss: 0.0115\n",
      "Epoch [440/500], Loss: 0.0781\n",
      "Epoch [440/500], Loss: 0.0104\n",
      "Epoch [440/500], Loss: 0.1308\n",
      "Epoch [440/500], Loss: 0.0149\n",
      "Epoch [440/500], Loss: 0.0168\n",
      "Epoch [440/500], Loss: 0.0062\n",
      "Epoch [441/500], Loss: 0.0134\n",
      "Epoch [441/500], Loss: 0.1181\n",
      "Epoch [441/500], Loss: 0.1078\n",
      "Epoch [441/500], Loss: 0.0166\n",
      "Epoch [441/500], Loss: 0.0142\n",
      "Epoch [441/500], Loss: 0.0298\n",
      "Epoch [441/500], Loss: 0.0366\n",
      "Epoch [441/500], Loss: 0.0124\n",
      "Epoch [442/500], Loss: 0.0073\n",
      "Epoch [442/500], Loss: 0.0142\n",
      "Epoch [442/500], Loss: 0.0810\n",
      "Epoch [442/500], Loss: 0.0052\n",
      "Epoch [442/500], Loss: 0.0204\n",
      "Epoch [442/500], Loss: 0.1162\n",
      "Epoch [442/500], Loss: 0.0023\n",
      "Epoch [442/500], Loss: 0.0007\n",
      "Epoch [443/500], Loss: 0.0013\n",
      "Epoch [443/500], Loss: 0.0053\n",
      "Epoch [443/500], Loss: 0.0037\n",
      "Epoch [443/500], Loss: 0.0031\n",
      "Epoch [443/500], Loss: 0.0634\n",
      "Epoch [443/500], Loss: 0.0295\n",
      "Epoch [443/500], Loss: 0.2142\n",
      "Epoch [443/500], Loss: 0.0010\n",
      "Epoch [444/500], Loss: 0.0033\n",
      "Epoch [444/500], Loss: 0.0454\n",
      "Epoch [444/500], Loss: 0.0047\n",
      "Epoch [444/500], Loss: 0.0227\n",
      "Epoch [444/500], Loss: 0.1909\n",
      "Epoch [444/500], Loss: 0.0915\n",
      "Epoch [444/500], Loss: 0.0215\n",
      "Epoch [444/500], Loss: 0.0114\n",
      "Epoch [445/500], Loss: 0.0285\n",
      "Epoch [445/500], Loss: 0.0146\n",
      "Epoch [445/500], Loss: 0.0054\n",
      "Epoch [445/500], Loss: 0.1923\n",
      "Epoch [445/500], Loss: 0.0292\n",
      "Epoch [445/500], Loss: 0.0040\n",
      "Epoch [445/500], Loss: 0.0072\n",
      "Epoch [445/500], Loss: 0.0123\n",
      "Epoch [446/500], Loss: 0.0019\n",
      "Epoch [446/500], Loss: 0.0090\n",
      "Epoch [446/500], Loss: 0.0642\n",
      "Epoch [446/500], Loss: 0.0015\n",
      "Epoch [446/500], Loss: 0.3607\n",
      "Epoch [446/500], Loss: 0.0385\n",
      "Epoch [446/500], Loss: 0.0081\n",
      "Epoch [446/500], Loss: 0.0147\n",
      "Epoch [447/500], Loss: 0.0356\n",
      "Epoch [447/500], Loss: 0.0578\n",
      "Epoch [447/500], Loss: 0.0910\n",
      "Epoch [447/500], Loss: 0.1258\n",
      "Epoch [447/500], Loss: 0.0728\n",
      "Epoch [447/500], Loss: 0.0534\n",
      "Epoch [447/500], Loss: 0.0578\n",
      "Epoch [447/500], Loss: 0.0550\n",
      "Epoch [448/500], Loss: 0.0348\n",
      "Epoch [448/500], Loss: 0.0600\n",
      "Epoch [448/500], Loss: 0.0218\n",
      "Epoch [448/500], Loss: 0.0049\n",
      "Epoch [448/500], Loss: 0.0017\n",
      "Epoch [448/500], Loss: 0.0777\n",
      "Epoch [448/500], Loss: 0.0090\n",
      "Epoch [448/500], Loss: 0.0000\n",
      "Epoch [449/500], Loss: 0.0021\n",
      "Epoch [449/500], Loss: 0.5385\n",
      "Epoch [449/500], Loss: 0.0011\n",
      "Epoch [449/500], Loss: 0.0008\n",
      "Epoch [449/500], Loss: 0.0590\n",
      "Epoch [449/500], Loss: 0.0073\n",
      "Epoch [449/500], Loss: 0.0048\n",
      "Epoch [449/500], Loss: 0.0975\n",
      "Epoch [450/500], Loss: 0.1536\n",
      "Epoch [450/500], Loss: 0.0566\n",
      "Epoch [450/500], Loss: 0.0610\n",
      "Epoch [450/500], Loss: 0.0104\n",
      "Epoch [450/500], Loss: 0.0094\n",
      "Epoch [450/500], Loss: 0.0192\n",
      "Epoch [450/500], Loss: 0.2352\n",
      "Epoch [450/500], Loss: 0.0023\n",
      "Epoch [451/500], Loss: 0.0135\n",
      "Epoch [451/500], Loss: 0.0492\n",
      "Epoch [451/500], Loss: 0.0668\n",
      "Epoch [451/500], Loss: 0.0082\n",
      "Epoch [451/500], Loss: 0.1414\n",
      "Epoch [451/500], Loss: 0.0176\n",
      "Epoch [451/500], Loss: 0.0405\n",
      "Epoch [451/500], Loss: 0.0030\n",
      "Epoch [452/500], Loss: 0.1699\n",
      "Epoch [452/500], Loss: 0.0176\n",
      "Epoch [452/500], Loss: 0.0057\n",
      "Epoch [452/500], Loss: 0.0162\n",
      "Epoch [452/500], Loss: 0.0255\n",
      "Epoch [452/500], Loss: 0.0064\n",
      "Epoch [452/500], Loss: 0.2229\n",
      "Epoch [452/500], Loss: 0.3808\n",
      "Epoch [453/500], Loss: 0.0100\n",
      "Epoch [453/500], Loss: 0.2397\n",
      "Epoch [453/500], Loss: 0.0320\n",
      "Epoch [453/500], Loss: 0.0320\n",
      "Epoch [453/500], Loss: 0.0247\n",
      "Epoch [453/500], Loss: 0.0248\n",
      "Epoch [453/500], Loss: 0.1427\n",
      "Epoch [453/500], Loss: 0.0207\n",
      "Epoch [454/500], Loss: 0.1576\n",
      "Epoch [454/500], Loss: 0.0902\n",
      "Epoch [454/500], Loss: 0.0060\n",
      "Epoch [454/500], Loss: 0.1259\n",
      "Epoch [454/500], Loss: 0.0151\n",
      "Epoch [454/500], Loss: 0.0535\n",
      "Epoch [454/500], Loss: 0.0910\n",
      "Epoch [454/500], Loss: 0.0120\n",
      "Epoch [455/500], Loss: 0.0704\n",
      "Epoch [455/500], Loss: 0.0035\n",
      "Epoch [455/500], Loss: 0.0303\n",
      "Epoch [455/500], Loss: 0.0302\n",
      "Epoch [455/500], Loss: 0.0107\n",
      "Epoch [455/500], Loss: 0.1121\n",
      "Epoch [455/500], Loss: 0.0031\n",
      "Epoch [455/500], Loss: 0.0004\n",
      "Epoch [456/500], Loss: 0.0120\n",
      "Epoch [456/500], Loss: 0.0566\n",
      "Epoch [456/500], Loss: 0.0566\n",
      "Epoch [456/500], Loss: 0.0062\n",
      "Epoch [456/500], Loss: 0.0032\n",
      "Epoch [456/500], Loss: 0.1020\n",
      "Epoch [456/500], Loss: 0.0250\n",
      "Epoch [456/500], Loss: 0.0131\n",
      "Epoch [457/500], Loss: 0.0806\n",
      "Epoch [457/500], Loss: 0.0146\n",
      "Epoch [457/500], Loss: 0.0968\n",
      "Epoch [457/500], Loss: 0.0196\n",
      "Epoch [457/500], Loss: 0.0074\n",
      "Epoch [457/500], Loss: 0.0081\n",
      "Epoch [457/500], Loss: 0.0159\n",
      "Epoch [457/500], Loss: 0.0017\n",
      "Epoch [458/500], Loss: 0.0077\n",
      "Epoch [458/500], Loss: 0.0031\n",
      "Epoch [458/500], Loss: 0.0860\n",
      "Epoch [458/500], Loss: 0.0042\n",
      "Epoch [458/500], Loss: 0.1419\n",
      "Epoch [458/500], Loss: 0.0056\n",
      "Epoch [458/500], Loss: 0.0109\n",
      "Epoch [458/500], Loss: 0.0068\n",
      "Epoch [459/500], Loss: 0.0087\n",
      "Epoch [459/500], Loss: 0.0397\n",
      "Epoch [459/500], Loss: 0.0076\n",
      "Epoch [459/500], Loss: 0.0541\n",
      "Epoch [459/500], Loss: 0.0062\n",
      "Epoch [459/500], Loss: 0.0070\n",
      "Epoch [459/500], Loss: 0.2076\n",
      "Epoch [459/500], Loss: 0.0038\n",
      "Epoch [460/500], Loss: 0.0384\n",
      "Epoch [460/500], Loss: 0.1461\n",
      "Epoch [460/500], Loss: 0.0190\n",
      "Epoch [460/500], Loss: 0.0231\n",
      "Epoch [460/500], Loss: 0.0270\n",
      "Epoch [460/500], Loss: 0.0402\n",
      "Epoch [460/500], Loss: 0.0470\n",
      "Epoch [460/500], Loss: 0.0045\n",
      "Epoch [461/500], Loss: 0.0910\n",
      "Epoch [461/500], Loss: 0.0045\n",
      "Epoch [461/500], Loss: 0.0054\n",
      "Epoch [461/500], Loss: 0.0049\n",
      "Epoch [461/500], Loss: 0.0075\n",
      "Epoch [461/500], Loss: 0.0702\n",
      "Epoch [461/500], Loss: 0.0061\n",
      "Epoch [461/500], Loss: 0.0017\n",
      "Epoch [462/500], Loss: 0.0013\n",
      "Epoch [462/500], Loss: 0.0023\n",
      "Epoch [462/500], Loss: 0.2546\n",
      "Epoch [462/500], Loss: 0.1148\n",
      "Epoch [462/500], Loss: 0.0013\n",
      "Epoch [462/500], Loss: 0.0045\n",
      "Epoch [462/500], Loss: 0.0103\n",
      "Epoch [462/500], Loss: 0.0107\n",
      "Epoch [463/500], Loss: 0.0331\n",
      "Epoch [463/500], Loss: 0.0409\n",
      "Epoch [463/500], Loss: 0.0102\n",
      "Epoch [463/500], Loss: 0.1147\n",
      "Epoch [463/500], Loss: 0.0110\n",
      "Epoch [463/500], Loss: 0.0120\n",
      "Epoch [463/500], Loss: 0.0349\n",
      "Epoch [463/500], Loss: 0.0149\n",
      "Epoch [464/500], Loss: 0.0090\n",
      "Epoch [464/500], Loss: 0.1098\n",
      "Epoch [464/500], Loss: 0.0502\n",
      "Epoch [464/500], Loss: 0.0116\n",
      "Epoch [464/500], Loss: 0.0384\n",
      "Epoch [464/500], Loss: 0.0041\n",
      "Epoch [464/500], Loss: 0.0164\n",
      "Epoch [464/500], Loss: 0.0026\n",
      "Epoch [465/500], Loss: 0.1094\n",
      "Epoch [465/500], Loss: 0.0068\n",
      "Epoch [465/500], Loss: 0.0017\n",
      "Epoch [465/500], Loss: 0.0041\n",
      "Epoch [465/500], Loss: 0.0846\n",
      "Epoch [465/500], Loss: 0.0239\n",
      "Epoch [465/500], Loss: 0.0443\n",
      "Epoch [465/500], Loss: 0.0096\n",
      "Epoch [466/500], Loss: 0.0037\n",
      "Epoch [466/500], Loss: 0.0219\n",
      "Epoch [466/500], Loss: 0.0069\n",
      "Epoch [466/500], Loss: 0.0044\n",
      "Epoch [466/500], Loss: 0.0715\n",
      "Epoch [466/500], Loss: 0.0066\n",
      "Epoch [466/500], Loss: 0.0049\n",
      "Epoch [466/500], Loss: 0.0727\n",
      "Epoch [467/500], Loss: 0.0036\n",
      "Epoch [467/500], Loss: 0.0259\n",
      "Epoch [467/500], Loss: 0.0176\n",
      "Epoch [467/500], Loss: 0.0492\n",
      "Epoch [467/500], Loss: 0.2399\n",
      "Epoch [467/500], Loss: 0.0134\n",
      "Epoch [467/500], Loss: 0.0122\n",
      "Epoch [467/500], Loss: 0.1717\n",
      "Epoch [468/500], Loss: 0.0303\n",
      "Epoch [468/500], Loss: 0.0311\n",
      "Epoch [468/500], Loss: 0.0881\n",
      "Epoch [468/500], Loss: 0.0163\n",
      "Epoch [468/500], Loss: 0.0175\n",
      "Epoch [468/500], Loss: 0.0191\n",
      "Epoch [468/500], Loss: 0.0800\n",
      "Epoch [468/500], Loss: 0.0484\n",
      "Epoch [469/500], Loss: 0.1082\n",
      "Epoch [469/500], Loss: 0.0165\n",
      "Epoch [469/500], Loss: 0.0022\n",
      "Epoch [469/500], Loss: 0.0069\n",
      "Epoch [469/500], Loss: 0.0355\n",
      "Epoch [469/500], Loss: 0.0025\n",
      "Epoch [469/500], Loss: 0.0150\n",
      "Epoch [469/500], Loss: 0.0075\n",
      "Epoch [470/500], Loss: 0.0011\n",
      "Epoch [470/500], Loss: 0.0240\n",
      "Epoch [470/500], Loss: 0.0004\n",
      "Epoch [470/500], Loss: 0.0222\n",
      "Epoch [470/500], Loss: 0.0003\n",
      "Epoch [470/500], Loss: 0.0921\n",
      "Epoch [470/500], Loss: 0.0008\n",
      "Epoch [470/500], Loss: 0.0002\n",
      "Epoch [471/500], Loss: 0.0055\n",
      "Epoch [471/500], Loss: 0.1152\n",
      "Epoch [471/500], Loss: 0.0089\n",
      "Epoch [471/500], Loss: 0.0683\n",
      "Epoch [471/500], Loss: 0.0092\n",
      "Epoch [471/500], Loss: 0.0016\n",
      "Epoch [471/500], Loss: 0.0447\n",
      "Epoch [471/500], Loss: 0.0030\n",
      "Epoch [472/500], Loss: 0.0350\n",
      "Epoch [472/500], Loss: 0.0930\n",
      "Epoch [472/500], Loss: 0.0092\n",
      "Epoch [472/500], Loss: 0.0338\n",
      "Epoch [472/500], Loss: 0.0134\n",
      "Epoch [472/500], Loss: 0.0144\n",
      "Epoch [472/500], Loss: 0.0310\n",
      "Epoch [472/500], Loss: 0.0017\n",
      "Epoch [473/500], Loss: 0.1237\n",
      "Epoch [473/500], Loss: 0.0567\n",
      "Epoch [473/500], Loss: 0.0017\n",
      "Epoch [473/500], Loss: 0.0104\n",
      "Epoch [473/500], Loss: 0.0129\n",
      "Epoch [473/500], Loss: 0.0237\n",
      "Epoch [473/500], Loss: 0.0175\n",
      "Epoch [473/500], Loss: 0.0014\n",
      "Epoch [474/500], Loss: 0.0150\n",
      "Epoch [474/500], Loss: 0.0407\n",
      "Epoch [474/500], Loss: 0.0019\n",
      "Epoch [474/500], Loss: 0.0051\n",
      "Epoch [474/500], Loss: 0.0381\n",
      "Epoch [474/500], Loss: 0.1549\n",
      "Epoch [474/500], Loss: 0.0807\n",
      "Epoch [474/500], Loss: 0.0010\n",
      "Epoch [475/500], Loss: 0.1067\n",
      "Epoch [475/500], Loss: 0.0421\n",
      "Epoch [475/500], Loss: 0.0252\n",
      "Epoch [475/500], Loss: 0.0432\n",
      "Epoch [475/500], Loss: 0.0201\n",
      "Epoch [475/500], Loss: 0.0213\n",
      "Epoch [475/500], Loss: 0.0132\n",
      "Epoch [475/500], Loss: 0.0099\n",
      "Epoch [476/500], Loss: 0.0088\n",
      "Epoch [476/500], Loss: 0.0244\n",
      "Epoch [476/500], Loss: 0.0273\n",
      "Epoch [476/500], Loss: 0.0010\n",
      "Epoch [476/500], Loss: 0.0011\n",
      "Epoch [476/500], Loss: 0.0361\n",
      "Epoch [476/500], Loss: 0.0739\n",
      "Epoch [476/500], Loss: 0.0001\n",
      "Epoch [477/500], Loss: 0.0044\n",
      "Epoch [477/500], Loss: 0.0008\n",
      "Epoch [477/500], Loss: 0.1171\n",
      "Epoch [477/500], Loss: 0.0011\n",
      "Epoch [477/500], Loss: 0.0207\n",
      "Epoch [477/500], Loss: 0.0031\n",
      "Epoch [477/500], Loss: 0.0417\n",
      "Epoch [477/500], Loss: 0.0009\n",
      "Epoch [478/500], Loss: 0.0077\n",
      "Epoch [478/500], Loss: 0.0263\n",
      "Epoch [478/500], Loss: 0.0501\n",
      "Epoch [478/500], Loss: 0.0033\n",
      "Epoch [478/500], Loss: 0.0006\n",
      "Epoch [478/500], Loss: 0.0279\n",
      "Epoch [478/500], Loss: 0.0027\n",
      "Epoch [478/500], Loss: 0.0054\n",
      "Epoch [479/500], Loss: 0.0026\n",
      "Epoch [479/500], Loss: 0.0815\n",
      "Epoch [479/500], Loss: 0.0202\n",
      "Epoch [479/500], Loss: 0.0023\n",
      "Epoch [479/500], Loss: 0.0188\n",
      "Epoch [479/500], Loss: 0.0026\n",
      "Epoch [479/500], Loss: 0.0203\n",
      "Epoch [479/500], Loss: 0.0073\n",
      "Epoch [480/500], Loss: 0.0401\n",
      "Epoch [480/500], Loss: 0.0044\n",
      "Epoch [480/500], Loss: 0.0129\n",
      "Epoch [480/500], Loss: 0.0324\n",
      "Epoch [480/500], Loss: 0.0022\n",
      "Epoch [480/500], Loss: 0.0282\n",
      "Epoch [480/500], Loss: 0.0112\n",
      "Epoch [480/500], Loss: 0.0035\n",
      "Epoch [481/500], Loss: 0.0015\n",
      "Epoch [481/500], Loss: 0.0016\n",
      "Epoch [481/500], Loss: 0.0808\n",
      "Epoch [481/500], Loss: 0.0019\n",
      "Epoch [481/500], Loss: 0.0562\n",
      "Epoch [481/500], Loss: 0.0076\n",
      "Epoch [481/500], Loss: 0.0036\n",
      "Epoch [481/500], Loss: 0.0147\n",
      "Epoch [482/500], Loss: 0.1340\n",
      "Epoch [482/500], Loss: 0.0181\n",
      "Epoch [482/500], Loss: 0.0101\n",
      "Epoch [482/500], Loss: 0.0054\n",
      "Epoch [482/500], Loss: 0.0058\n",
      "Epoch [482/500], Loss: 0.0926\n",
      "Epoch [482/500], Loss: 0.0071\n",
      "Epoch [482/500], Loss: 0.0009\n",
      "Epoch [483/500], Loss: 0.0085\n",
      "Epoch [483/500], Loss: 0.0203\n",
      "Epoch [483/500], Loss: 0.0040\n",
      "Epoch [483/500], Loss: 0.0600\n",
      "Epoch [483/500], Loss: 0.0055\n",
      "Epoch [483/500], Loss: 0.0051\n",
      "Epoch [483/500], Loss: 0.0050\n",
      "Epoch [483/500], Loss: 0.0135\n",
      "Epoch [484/500], Loss: 0.0325\n",
      "Epoch [484/500], Loss: 0.0245\n",
      "Epoch [484/500], Loss: 0.0004\n",
      "Epoch [484/500], Loss: 0.0031\n",
      "Epoch [484/500], Loss: 0.0004\n",
      "Epoch [484/500], Loss: 0.2093\n",
      "Epoch [484/500], Loss: 0.0006\n",
      "Epoch [484/500], Loss: 0.0001\n",
      "Epoch [485/500], Loss: 0.0005\n",
      "Epoch [485/500], Loss: 0.0305\n",
      "Epoch [485/500], Loss: 0.0106\n",
      "Epoch [485/500], Loss: 0.0009\n",
      "Epoch [485/500], Loss: 0.0173\n",
      "Epoch [485/500], Loss: 0.0023\n",
      "Epoch [485/500], Loss: 0.0232\n",
      "Epoch [485/500], Loss: 0.0011\n",
      "Epoch [486/500], Loss: 0.0271\n",
      "Epoch [486/500], Loss: 0.0167\n",
      "Epoch [486/500], Loss: 0.0150\n",
      "Epoch [486/500], Loss: 0.0036\n",
      "Epoch [486/500], Loss: 0.0006\n",
      "Epoch [486/500], Loss: 0.0017\n",
      "Epoch [486/500], Loss: 0.2528\n",
      "Epoch [486/500], Loss: 0.0005\n",
      "Epoch [487/500], Loss: 0.0037\n",
      "Epoch [487/500], Loss: 0.0009\n",
      "Epoch [487/500], Loss: 0.0018\n",
      "Epoch [487/500], Loss: 0.3375\n",
      "Epoch [487/500], Loss: 0.0457\n",
      "Epoch [487/500], Loss: 0.0084\n",
      "Epoch [487/500], Loss: 0.0261\n",
      "Epoch [487/500], Loss: 0.0326\n",
      "Epoch [488/500], Loss: 0.1111\n",
      "Epoch [488/500], Loss: 0.0676\n",
      "Epoch [488/500], Loss: 0.0079\n",
      "Epoch [488/500], Loss: 0.0150\n",
      "Epoch [488/500], Loss: 0.0029\n",
      "Epoch [488/500], Loss: 0.1033\n",
      "Epoch [488/500], Loss: 0.0296\n",
      "Epoch [488/500], Loss: 0.0033\n",
      "Epoch [489/500], Loss: 0.0044\n",
      "Epoch [489/500], Loss: 0.0016\n",
      "Epoch [489/500], Loss: 0.0008\n",
      "Epoch [489/500], Loss: 0.0012\n",
      "Epoch [489/500], Loss: 0.0332\n",
      "Epoch [489/500], Loss: 0.1718\n",
      "Epoch [489/500], Loss: 0.0037\n",
      "Epoch [489/500], Loss: 0.0013\n",
      "Epoch [490/500], Loss: 0.2029\n",
      "Epoch [490/500], Loss: 0.2452\n",
      "Epoch [490/500], Loss: 0.0081\n",
      "Epoch [490/500], Loss: 0.0202\n",
      "Epoch [490/500], Loss: 0.0190\n",
      "Epoch [490/500], Loss: 0.0148\n",
      "Epoch [490/500], Loss: 0.0475\n",
      "Epoch [490/500], Loss: 0.0613\n",
      "Epoch [491/500], Loss: 0.0963\n",
      "Epoch [491/500], Loss: 0.0354\n",
      "Epoch [491/500], Loss: 0.0487\n",
      "Epoch [491/500], Loss: 0.0063\n",
      "Epoch [491/500], Loss: 0.0027\n",
      "Epoch [491/500], Loss: 0.0034\n",
      "Epoch [491/500], Loss: 0.0033\n",
      "Epoch [491/500], Loss: 0.0002\n",
      "Epoch [492/500], Loss: 0.0016\n",
      "Epoch [492/500], Loss: 0.0674\n",
      "Epoch [492/500], Loss: 0.2231\n",
      "Epoch [492/500], Loss: 0.0432\n",
      "Epoch [492/500], Loss: 0.0037\n",
      "Epoch [492/500], Loss: 0.0055\n",
      "Epoch [492/500], Loss: 0.0689\n",
      "Epoch [492/500], Loss: 0.0007\n",
      "Epoch [493/500], Loss: 0.0094\n",
      "Epoch [493/500], Loss: 0.0037\n",
      "Epoch [493/500], Loss: 0.0471\n",
      "Epoch [493/500], Loss: 0.0391\n",
      "Epoch [493/500], Loss: 0.0253\n",
      "Epoch [493/500], Loss: 0.0668\n",
      "Epoch [493/500], Loss: 0.0055\n",
      "Epoch [493/500], Loss: 0.0014\n",
      "Epoch [494/500], Loss: 0.0099\n",
      "Epoch [494/500], Loss: 0.0171\n",
      "Epoch [494/500], Loss: 0.0314\n",
      "Epoch [494/500], Loss: 0.0848\n",
      "Epoch [494/500], Loss: 0.0218\n",
      "Epoch [494/500], Loss: 0.1172\n",
      "Epoch [494/500], Loss: 0.0361\n",
      "Epoch [494/500], Loss: 0.0012\n",
      "Epoch [495/500], Loss: 0.1044\n",
      "Epoch [495/500], Loss: 0.0500\n",
      "Epoch [495/500], Loss: 0.0663\n",
      "Epoch [495/500], Loss: 0.0106\n",
      "Epoch [495/500], Loss: 0.0034\n",
      "Epoch [495/500], Loss: 0.0619\n",
      "Epoch [495/500], Loss: 0.0062\n",
      "Epoch [495/500], Loss: 0.0067\n",
      "Epoch [496/500], Loss: 0.0801\n",
      "Epoch [496/500], Loss: 0.0482\n",
      "Epoch [496/500], Loss: 0.0287\n",
      "Epoch [496/500], Loss: 0.0730\n",
      "Epoch [496/500], Loss: 0.0159\n",
      "Epoch [496/500], Loss: 0.0498\n",
      "Epoch [496/500], Loss: 0.0113\n",
      "Epoch [496/500], Loss: 0.0256\n",
      "Epoch [497/500], Loss: 0.0115\n",
      "Epoch [497/500], Loss: 0.0038\n",
      "Epoch [497/500], Loss: 0.0285\n",
      "Epoch [497/500], Loss: 0.1328\n",
      "Epoch [497/500], Loss: 0.0038\n",
      "Epoch [497/500], Loss: 0.0128\n",
      "Epoch [497/500], Loss: 0.0439\n",
      "Epoch [497/500], Loss: 0.0027\n",
      "Epoch [498/500], Loss: 0.0451\n",
      "Epoch [498/500], Loss: 0.0006\n",
      "Epoch [498/500], Loss: 0.0342\n",
      "Epoch [498/500], Loss: 0.0737\n",
      "Epoch [498/500], Loss: 0.0715\n",
      "Epoch [498/500], Loss: 0.0079\n",
      "Epoch [498/500], Loss: 0.0016\n",
      "Epoch [498/500], Loss: 0.0011\n",
      "Epoch [499/500], Loss: 0.0061\n",
      "Epoch [499/500], Loss: 0.0603\n",
      "Epoch [499/500], Loss: 0.0282\n",
      "Epoch [499/500], Loss: 0.0046\n",
      "Epoch [499/500], Loss: 0.0089\n",
      "Epoch [499/500], Loss: 0.0408\n",
      "Epoch [499/500], Loss: 0.0073\n",
      "Epoch [499/500], Loss: 0.0205\n",
      "Epoch [500/500], Loss: 0.0952\n",
      "Epoch [500/500], Loss: 0.0450\n",
      "Epoch [500/500], Loss: 0.0093\n",
      "Epoch [500/500], Loss: 0.0191\n",
      "Epoch [500/500], Loss: 0.0099\n",
      "Epoch [500/500], Loss: 0.0063\n",
      "Epoch [500/500], Loss: 0.0023\n",
      "Epoch [500/500], Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 500\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for images, targets in train_loader:\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        targets = targets.unsqueeze(1).to(device)\n",
    "        \n",
    "        # Convert targets to float\n",
    "        targets = targets.float()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print training loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:32:37.818552Z",
     "iopub.status.busy": "2023-06-17T21:32:37.818097Z",
     "iopub.status.idle": "2023-06-17T21:32:38.505566Z",
     "shell.execute_reply": "2023-06-17T21:32:38.503789Z",
     "shell.execute_reply.started": "2023-06-17T21:32:37.818510Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    for images, targets in test_loader:\n",
    "        \n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        targets = targets.unsqueeze(1).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        predictions = torch.round(outputs)\n",
    "\n",
    "        # Collect predictions and targets\n",
    "        all_predictions.extend(predictions.cpu().numpy().flatten().tolist())\n",
    "        all_targets.extend(targets.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    # Calculate metrics\n",
    "    correct = sum(all_predictions[i] == all_targets[i] for i in range(len(all_predictions)))\n",
    "    accuracy = correct / len(all_predictions)\n",
    "    tp = sum(all_predictions[i] == 1 and all_targets[i] == 1 for i in range(len(all_predictions)))\n",
    "    tn = sum(all_predictions[i] == 0 and all_targets[i] == 0 for i in range(len(all_predictions)))\n",
    "    fp = sum(all_predictions[i] == 1 and all_targets[i] == 0 for i in range(len(all_predictions)))\n",
    "    fn = sum(all_predictions[i] == 0 and all_targets[i] == 1 for i in range(len(all_predictions)))\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    auc = roc_auc_score(all_targets, all_predictions)\n",
    "    p_value = ttest_ind(all_predictions, all_targets).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T21:32:43.265417Z",
     "iopub.status.busy": "2023-06-17T21:32:43.265060Z",
     "iopub.status.idle": "2023-06-17T21:32:43.271947Z",
     "shell.execute_reply": "2023-06-17T21:32:43.270907Z",
     "shell.execute_reply.started": "2023-06-17T21:32:43.265388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6214285714285714\n",
      "Sensitivity: 1.0\n",
      "Specificity: 0.35365853658536583\n",
      "AUC: 0.6768292682926829\n",
      "p-value: 1.9545323079916612e-11\n"
     ]
    }
   ],
   "source": [
    "# Print the evaluation results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Sensitivity:\", sensitivity)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"AUC:\", auc)\n",
    "print(\"p-value:\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
